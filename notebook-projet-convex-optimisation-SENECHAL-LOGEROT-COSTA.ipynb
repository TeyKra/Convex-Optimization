{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Convex Optimisation Project\n","\n","## Subject: Understanding and trying different optimizers in Neural Network to find the right one for our data analysis\n","\n","## Data : Kaggle Competition House Prices - Advanced Regression Techniques\n","\n","### Team : \n","\n","- SENECHAL Morgan\n","- Logerot Jules\n","- Costa Thomas\n","\n","# Dataset Description\n","\n","## File descriptions\n","\n","- train.csv - the training set\n","- test.csv - the test set\n","- data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n","- sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n","Data fields\n","\n","## Data description file.\n","\n","- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n","- MSSubClass: The building class\n","- MSZoning: The general zoning classification\n","- LotFrontage: Linear feet of street connected to property\n","- LotArea: Lot size in square feet\n","- Street: Type of road access\n","- Alley: Type of alley access\n","- LotShape: General shape of property\n","- LandContour: Flatness of the property\n","- Utilities: Type of utilities available\n","- LotConfig: Lot configuration\n","- LandSlope: Slope of property\n","- Neighborhood: Physical locations within Ames city limits\n","- Condition1: Proximity to main road or railroad\n","- Condition2: Proximity to main road or railroad (if a second is present)\n","- BldgType: Type of dwelling\n","- HouseStyle: Style of dwelling\n","- OverallQual: Overall material and finish quality\n","- OverallCond: Overall condition rating\n","- YearBuilt: Original construction date\n","- YearRemodAdd: Remodel date\n","- RoofStyle: Type of roof\n","- RoofMatl: Roof material\n","- Exterior1st: Exterior covering on house\n","- Exterior2nd: Exterior covering on house (if more than one material)\n","- MasVnrType: Masonry veneer type\n","- MasVnrArea: Masonry veneer area in square feet\n","- ExterQual: Exterior material quality\n","- ExterCond: Present condition of the material on the exterior\n","- Foundation: Type of foundation\n","- BsmtQual: Height of the basement\n","- BsmtCond: General condition of the basement\n","- BsmtExposure: Walkout or garden level basement walls\n","- BsmtFinType1: Quality of basement finished area\n","- BsmtFinSF1: Type 1 finished square feet\n","- BsmtFinType2: Quality of second finished area (if present)\n","- BsmtFinSF2: Type 2 finished square feet\n","- BsmtUnfSF: Unfinished square feet of basement area\n","- TotalBsmtSF: Total square feet of basement area\n","- Heating: Type of heating\n","- HeatingQC: Heating quality and condition\n","- CentralAir: Central air conditioning\n","- Electrical: Electrical system\n","- 1stFlrSF: First Floor square feet\n","- 2ndFlrSF: Second floor square feet\n","- LowQualFinSF: Low quality finished square feet (all floors)\n","- GrLivArea: Above grade (ground) living area square feet\n","- BsmtFullBath: Basement full bathrooms\n","- BsmtHalfBath: Basement half bathrooms\n","- FullBath: Full bathrooms above grade\n","- HalfBath: Half baths above grade\n","- Bedroom: Number of bedrooms above basement level\n","- Kitchen: Number of kitchens\n","- KitchenQual: Kitchen quality\n","- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n","- Functional: Home functionality rating\n","- Fireplaces: Number of fireplaces\n","- FireplaceQu: Fireplace quality\n","- GarageType: Garage location\n","- GarageYrBlt: Year garage was built\n","- GarageFinish: Interior finish of the garage\n","- GarageCars: Size of garage in car capacity\n","- GarageArea: Size of garage in square feet\n","- GarageQual: Garage quality\n","- GarageCond: Garage condition\n","- PavedDrive: Paved driveway\n","- WoodDeckSF: Wood deck area in square feet\n","- OpenPorchSF: Open porch area in square feet\n","- EnclosedPorch: Enclosed porch area in square feet\n","- 3SsnPorch: Three season porch area in square feet\n","- ScreenPorch: Screen porch area in square feet\n","- PoolArea: Pool area in square feet\n","- PoolQC: Pool quality\n","- Fence: Fence quality\n","- MiscFeature: Miscellaneous feature not covered in other categories\n","- MiscVal: Value of miscellaneous feature\n","- MoSold: Month Sold\n","- YrSold: Year Sold\n","- SaleType: Type of sale\n","- SaleCondition: Condition of sale"]},{"cell_type":"markdown","metadata":{},"source":["# Library"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-02T12:43:28.342890Z","iopub.status.busy":"2024-06-02T12:43:28.342539Z","iopub.status.idle":"2024-06-02T12:43:29.676198Z","shell.execute_reply":"2024-06-02T12:43:29.675148Z","shell.execute_reply.started":"2024-06-02T12:43:28.342862Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Importation"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:29.678457Z","iopub.status.busy":"2024-06-02T12:43:29.677908Z","iopub.status.idle":"2024-06-02T12:43:29.780489Z","shell.execute_reply":"2024-06-02T12:43:29.779317Z","shell.execute_reply.started":"2024-06-02T12:43:29.678409Z"},"trusted":true},"outputs":[],"source":["#importing the data and overview \n","df_train= pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n","df_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Exploration"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:30.639687Z","iopub.status.busy":"2024-06-02T12:43:30.639096Z","iopub.status.idle":"2024-06-02T12:43:30.698496Z","shell.execute_reply":"2024-06-02T12:43:30.697586Z","shell.execute_reply.started":"2024-06-02T12:43:30.639654Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>MSSubClass</th>\n","      <th>MSZoning</th>\n","      <th>LotFrontage</th>\n","      <th>LotArea</th>\n","      <th>Street</th>\n","      <th>Alley</th>\n","      <th>LotShape</th>\n","      <th>LandContour</th>\n","      <th>Utilities</th>\n","      <th>...</th>\n","      <th>PoolArea</th>\n","      <th>PoolQC</th>\n","      <th>Fence</th>\n","      <th>MiscFeature</th>\n","      <th>MiscVal</th>\n","      <th>MoSold</th>\n","      <th>YrSold</th>\n","      <th>SaleType</th>\n","      <th>SaleCondition</th>\n","      <th>SalePrice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>65.0</td>\n","      <td>8450</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>208500</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>80.0</td>\n","      <td>9600</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2007</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>181500</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>68.0</td>\n","      <td>11250</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>223500</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>70</td>\n","      <td>RL</td>\n","      <td>60.0</td>\n","      <td>9550</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Abnorml</td>\n","      <td>140000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>84.0</td>\n","      <td>14260</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>250000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 81 columns</p>\n","</div>"],"text/plain":["   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n","0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n","1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n","2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n","3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n","4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n","\n","  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n","0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n","1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n","2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n","3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n","4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n","\n","  YrSold  SaleType  SaleCondition  SalePrice  \n","0   2008        WD         Normal     208500  \n","1   2007        WD         Normal     181500  \n","2   2008        WD         Normal     223500  \n","3   2006        WD        Abnorml     140000  \n","4   2008        WD         Normal     250000  \n","\n","[5 rows x 81 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T15:58:36.691386Z","iopub.status.busy":"2024-06-02T15:58:36.690941Z","iopub.status.idle":"2024-06-02T15:58:36.719323Z","shell.execute_reply":"2024-06-02T15:58:36.718303Z","shell.execute_reply.started":"2024-06-02T15:58:36.691352Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LotArea</th>\n","      <th>OverallQual</th>\n","      <th>OverallCond</th>\n","      <th>YearBuilt</th>\n","      <th>YearRemodAdd</th>\n","      <th>MasVnrArea</th>\n","      <th>BsmtFinSF1</th>\n","      <th>BsmtFinSF2</th>\n","      <th>BsmtUnfSF</th>\n","      <th>TotalBsmtSF</th>\n","      <th>...</th>\n","      <th>SaleType_ConLI</th>\n","      <th>SaleType_ConLw</th>\n","      <th>SaleType_New</th>\n","      <th>SaleType_Oth</th>\n","      <th>SaleType_WD</th>\n","      <th>SaleCondition_AdjLand</th>\n","      <th>SaleCondition_Alloca</th>\n","      <th>SaleCondition_Family</th>\n","      <th>SaleCondition_Normal</th>\n","      <th>SaleCondition_Partial</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>11622</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>1961</td>\n","      <td>1961</td>\n","      <td>0.0</td>\n","      <td>468.0</td>\n","      <td>144.0</td>\n","      <td>270.0</td>\n","      <td>882.0</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>14267</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>1958</td>\n","      <td>1958</td>\n","      <td>108.0</td>\n","      <td>923.0</td>\n","      <td>0.0</td>\n","      <td>406.0</td>\n","      <td>1329.0</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13830</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1997</td>\n","      <td>1998</td>\n","      <td>0.0</td>\n","      <td>791.0</td>\n","      <td>0.0</td>\n","      <td>137.0</td>\n","      <td>928.0</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9978</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>1998</td>\n","      <td>1998</td>\n","      <td>20.0</td>\n","      <td>602.0</td>\n","      <td>0.0</td>\n","      <td>324.0</td>\n","      <td>926.0</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5005</td>\n","      <td>8</td>\n","      <td>5</td>\n","      <td>1992</td>\n","      <td>1992</td>\n","      <td>0.0</td>\n","      <td>263.0</td>\n","      <td>0.0</td>\n","      <td>1017.0</td>\n","      <td>1280.0</td>\n","      <td>...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 267 columns</p>\n","</div>"],"text/plain":["   LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  MasVnrArea  \\\n","0    11622            5            6       1961          1961         0.0   \n","1    14267            6            6       1958          1958       108.0   \n","2    13830            5            5       1997          1998         0.0   \n","3     9978            6            6       1998          1998        20.0   \n","4     5005            8            5       1992          1992         0.0   \n","\n","   BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  ...  SaleType_ConLI  \\\n","0       468.0       144.0      270.0        882.0  ...           False   \n","1       923.0         0.0      406.0       1329.0  ...           False   \n","2       791.0         0.0      137.0        928.0  ...           False   \n","3       602.0         0.0      324.0        926.0  ...           False   \n","4       263.0         0.0     1017.0       1280.0  ...           False   \n","\n","   SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n","0           False         False         False         True   \n","1           False         False         False         True   \n","2           False         False         False         True   \n","3           False         False         False         True   \n","4           False         False         False         True   \n","\n","   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n","0                  False                 False                 False   \n","1                  False                 False                 False   \n","2                  False                 False                 False   \n","3                  False                 False                 False   \n","4                  False                 False                 False   \n","\n","   SaleCondition_Normal  SaleCondition_Partial  \n","0                  True                  False  \n","1                  True                  False  \n","2                  True                  False  \n","3                  True                  False  \n","4                  True                  False  \n","\n","[5 rows x 267 columns]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["df_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["We decicded to drop the id column because this column will be not usefull in our analyse : "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:30.934615Z","iopub.status.busy":"2024-06-02T12:43:30.933886Z","iopub.status.idle":"2024-06-02T12:43:30.942963Z","shell.execute_reply":"2024-06-02T12:43:30.941845Z","shell.execute_reply.started":"2024-06-02T12:43:30.934583Z"},"trusted":true},"outputs":[],"source":["# Dropping 'Id' column\n","df_train.drop('Id', axis=1, inplace=True)\n","df_test_ids = df_test['Id']\n","df_test.drop('Id', axis=1, inplace=True)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:31.070497Z","iopub.status.busy":"2024-06-02T12:43:31.069411Z","iopub.status.idle":"2024-06-02T12:43:31.094975Z","shell.execute_reply":"2024-06-02T12:43:31.094037Z","shell.execute_reply.started":"2024-06-02T12:43:31.070458Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MSSubClass</th>\n","      <th>MSZoning</th>\n","      <th>LotFrontage</th>\n","      <th>LotArea</th>\n","      <th>Street</th>\n","      <th>Alley</th>\n","      <th>LotShape</th>\n","      <th>LandContour</th>\n","      <th>Utilities</th>\n","      <th>LotConfig</th>\n","      <th>...</th>\n","      <th>PoolArea</th>\n","      <th>PoolQC</th>\n","      <th>Fence</th>\n","      <th>MiscFeature</th>\n","      <th>MiscVal</th>\n","      <th>MoSold</th>\n","      <th>YrSold</th>\n","      <th>SaleType</th>\n","      <th>SaleCondition</th>\n","      <th>SalePrice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>65.0</td>\n","      <td>8450</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>208500</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>80.0</td>\n","      <td>9600</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>FR2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2007</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>181500</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>68.0</td>\n","      <td>11250</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>223500</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>70</td>\n","      <td>RL</td>\n","      <td>60.0</td>\n","      <td>9550</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Corner</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2006</td>\n","      <td>WD</td>\n","      <td>Abnorml</td>\n","      <td>140000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>84.0</td>\n","      <td>14260</td>\n","      <td>Pave</td>\n","      <td>NaN</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>FR2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>2008</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","      <td>250000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 80 columns</p>\n","</div>"],"text/plain":["   MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n","0          60       RL         65.0     8450   Pave   NaN      Reg   \n","1          20       RL         80.0     9600   Pave   NaN      Reg   \n","2          60       RL         68.0    11250   Pave   NaN      IR1   \n","3          70       RL         60.0     9550   Pave   NaN      IR1   \n","4          60       RL         84.0    14260   Pave   NaN      IR1   \n","\n","  LandContour Utilities LotConfig  ... PoolArea PoolQC Fence MiscFeature  \\\n","0         Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n","1         Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n","2         Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n","3         Lvl    AllPub    Corner  ...        0    NaN   NaN         NaN   \n","4         Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n","\n","  MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n","0       0      2    2008        WD         Normal     208500  \n","1       0      5    2007        WD         Normal     181500  \n","2       0      9    2008        WD         Normal     223500  \n","3       0      2    2006        WD        Abnorml     140000  \n","4       0     12    2008        WD         Normal     250000  \n","\n","[5 rows x 80 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:31.200604Z","iopub.status.busy":"2024-06-02T12:43:31.200265Z","iopub.status.idle":"2024-06-02T12:43:31.239822Z","shell.execute_reply":"2024-06-02T12:43:31.238870Z","shell.execute_reply.started":"2024-06-02T12:43:31.200577Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1460 entries, 0 to 1459\n","Data columns (total 80 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   MSSubClass     1460 non-null   int64  \n"," 1   MSZoning       1460 non-null   object \n"," 2   LotFrontage    1201 non-null   float64\n"," 3   LotArea        1460 non-null   int64  \n"," 4   Street         1460 non-null   object \n"," 5   Alley          91 non-null     object \n"," 6   LotShape       1460 non-null   object \n"," 7   LandContour    1460 non-null   object \n"," 8   Utilities      1460 non-null   object \n"," 9   LotConfig      1460 non-null   object \n"," 10  LandSlope      1460 non-null   object \n"," 11  Neighborhood   1460 non-null   object \n"," 12  Condition1     1460 non-null   object \n"," 13  Condition2     1460 non-null   object \n"," 14  BldgType       1460 non-null   object \n"," 15  HouseStyle     1460 non-null   object \n"," 16  OverallQual    1460 non-null   int64  \n"," 17  OverallCond    1460 non-null   int64  \n"," 18  YearBuilt      1460 non-null   int64  \n"," 19  YearRemodAdd   1460 non-null   int64  \n"," 20  RoofStyle      1460 non-null   object \n"," 21  RoofMatl       1460 non-null   object \n"," 22  Exterior1st    1460 non-null   object \n"," 23  Exterior2nd    1460 non-null   object \n"," 24  MasVnrType     588 non-null    object \n"," 25  MasVnrArea     1452 non-null   float64\n"," 26  ExterQual      1460 non-null   object \n"," 27  ExterCond      1460 non-null   object \n"," 28  Foundation     1460 non-null   object \n"," 29  BsmtQual       1423 non-null   object \n"," 30  BsmtCond       1423 non-null   object \n"," 31  BsmtExposure   1422 non-null   object \n"," 32  BsmtFinType1   1423 non-null   object \n"," 33  BsmtFinSF1     1460 non-null   int64  \n"," 34  BsmtFinType2   1422 non-null   object \n"," 35  BsmtFinSF2     1460 non-null   int64  \n"," 36  BsmtUnfSF      1460 non-null   int64  \n"," 37  TotalBsmtSF    1460 non-null   int64  \n"," 38  Heating        1460 non-null   object \n"," 39  HeatingQC      1460 non-null   object \n"," 40  CentralAir     1460 non-null   object \n"," 41  Electrical     1459 non-null   object \n"," 42  1stFlrSF       1460 non-null   int64  \n"," 43  2ndFlrSF       1460 non-null   int64  \n"," 44  LowQualFinSF   1460 non-null   int64  \n"," 45  GrLivArea      1460 non-null   int64  \n"," 46  BsmtFullBath   1460 non-null   int64  \n"," 47  BsmtHalfBath   1460 non-null   int64  \n"," 48  FullBath       1460 non-null   int64  \n"," 49  HalfBath       1460 non-null   int64  \n"," 50  BedroomAbvGr   1460 non-null   int64  \n"," 51  KitchenAbvGr   1460 non-null   int64  \n"," 52  KitchenQual    1460 non-null   object \n"," 53  TotRmsAbvGrd   1460 non-null   int64  \n"," 54  Functional     1460 non-null   object \n"," 55  Fireplaces     1460 non-null   int64  \n"," 56  FireplaceQu    770 non-null    object \n"," 57  GarageType     1379 non-null   object \n"," 58  GarageYrBlt    1379 non-null   float64\n"," 59  GarageFinish   1379 non-null   object \n"," 60  GarageCars     1460 non-null   int64  \n"," 61  GarageArea     1460 non-null   int64  \n"," 62  GarageQual     1379 non-null   object \n"," 63  GarageCond     1379 non-null   object \n"," 64  PavedDrive     1460 non-null   object \n"," 65  WoodDeckSF     1460 non-null   int64  \n"," 66  OpenPorchSF    1460 non-null   int64  \n"," 67  EnclosedPorch  1460 non-null   int64  \n"," 68  3SsnPorch      1460 non-null   int64  \n"," 69  ScreenPorch    1460 non-null   int64  \n"," 70  PoolArea       1460 non-null   int64  \n"," 71  PoolQC         7 non-null      object \n"," 72  Fence          281 non-null    object \n"," 73  MiscFeature    54 non-null     object \n"," 74  MiscVal        1460 non-null   int64  \n"," 75  MoSold         1460 non-null   int64  \n"," 76  YrSold         1460 non-null   int64  \n"," 77  SaleType       1460 non-null   object \n"," 78  SaleCondition  1460 non-null   object \n"," 79  SalePrice      1460 non-null   int64  \n","dtypes: float64(3), int64(34), object(43)\n","memory usage: 912.6+ KB\n"]}],"source":["df_train.info()"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T16:58:49.436687Z","iopub.status.busy":"2024-06-02T16:58:49.435970Z","iopub.status.idle":"2024-06-02T16:58:49.449839Z","shell.execute_reply":"2024-06-02T16:58:49.448736Z","shell.execute_reply.started":"2024-06-02T16:58:49.436651Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1459 entries, 0 to 1458\n","Columns: 267 entries, LotArea to SaleCondition_Partial\n","dtypes: bool(216), float64(10), int64(41)\n","memory usage: 889.2 KB\n"]}],"source":["df_test.info()"]},{"cell_type":"markdown","metadata":{},"source":["Here we want to see the correlation between the SalePrice column and the other numerical columns : "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:31.474275Z","iopub.status.busy":"2024-06-02T12:43:31.473939Z","iopub.status.idle":"2024-06-02T12:43:31.490196Z","shell.execute_reply":"2024-06-02T12:43:31.489196Z","shell.execute_reply.started":"2024-06-02T12:43:31.474248Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["KitchenAbvGr    -0.135907\n","EnclosedPorch   -0.128578\n","MSSubClass      -0.084284\n","OverallCond     -0.077856\n","YrSold          -0.028923\n","LowQualFinSF    -0.025606\n","MiscVal         -0.021190\n","BsmtHalfBath    -0.016844\n","BsmtFinSF2      -0.011378\n","3SsnPorch        0.044584\n","MoSold           0.046432\n","PoolArea         0.092404\n","ScreenPorch      0.111447\n","BedroomAbvGr     0.168213\n","BsmtUnfSF        0.214479\n","BsmtFullBath     0.227122\n","LotArea          0.263843\n","HalfBath         0.284108\n","OpenPorchSF      0.315856\n","2ndFlrSF         0.319334\n","WoodDeckSF       0.324413\n","LotFrontage      0.351799\n","BsmtFinSF1       0.386420\n","Fireplaces       0.466929\n","MasVnrArea       0.477493\n","GarageYrBlt      0.486362\n","YearRemodAdd     0.507101\n","YearBuilt        0.522897\n","TotRmsAbvGrd     0.533723\n","FullBath         0.560664\n","1stFlrSF         0.605852\n","TotalBsmtSF      0.613581\n","GarageArea       0.623431\n","GarageCars       0.640409\n","GrLivArea        0.708624\n","OverallQual      0.790982\n","SalePrice        1.000000\n","Name: SalePrice, dtype: float64\n"]}],"source":["# Sélection des colonnes numériques uniquement\n","numeric_df = df_train.select_dtypes(include=[float, int])\n","\n","# Calcul de la matrice de corrélation et tri des valeurs de corrélation avec 'SalePrice'\n","correlation_with_saleprice = numeric_df.corr()['SalePrice'].sort_values()\n","\n","# Affichage des résultats\n","print(correlation_with_saleprice)"]},{"cell_type":"markdown","metadata":{},"source":["Here we want to see the missing value in our columns :"]},{"cell_type":"markdown","metadata":{},"source":["Pourcentage of missing value in our columns : "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:31.913833Z","iopub.status.busy":"2024-06-02T12:43:31.913153Z","iopub.status.idle":"2024-06-02T12:43:31.929948Z","shell.execute_reply":"2024-06-02T12:43:31.928885Z","shell.execute_reply.started":"2024-06-02T12:43:31.913796Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Electrical       0.068493\n","MasVnrArea       0.547945\n","BsmtCond         2.534247\n","BsmtFinType1     2.534247\n","BsmtQual         2.534247\n","BsmtExposure     2.602740\n","BsmtFinType2     2.602740\n","GarageType       5.547945\n","GarageQual       5.547945\n","GarageFinish     5.547945\n","GarageCond       5.547945\n","GarageYrBlt      5.547945\n","LotFrontage     17.739726\n","FireplaceQu     47.260274\n","MasVnrType      59.726027\n","Fence           80.753425\n","Alley           93.767123\n","MiscFeature     96.301370\n","PoolQC          99.520548\n","dtype: float64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["def missing_percent(df_train):\n","    nan_percent= 100*(df_train.isnull().sum()/len(df_train))\n","    nan_percent= nan_percent[nan_percent>0].sort_values()\n","    return nan_percent\n","\n","nan_percent= missing_percent(df_train)\n","nan_percent\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:32.045112Z","iopub.status.busy":"2024-06-02T12:43:32.044266Z","iopub.status.idle":"2024-06-02T12:43:32.049888Z","shell.execute_reply":"2024-06-02T12:43:32.048810Z","shell.execute_reply.started":"2024-06-02T12:43:32.045079Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Nombre total de colonnes avec des valeurs manquantes : 19\n"]}],"source":["# Calcul du nombre total de colonnes avec des valeurs manquantes\n","num_columns_with_missing_values = len(nan_percent)\n","\n","print(f\"Nombre total de colonnes avec des valeurs manquantes : {num_columns_with_missing_values}\")"]},{"cell_type":"markdown","metadata":{},"source":["Number of missing value in our columns : "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:32.491956Z","iopub.status.busy":"2024-06-02T12:43:32.491073Z","iopub.status.idle":"2024-06-02T12:43:32.507782Z","shell.execute_reply":"2024-06-02T12:43:32.506851Z","shell.execute_reply.started":"2024-06-02T12:43:32.491919Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PoolQC          1453\n","MiscFeature     1406\n","Alley           1369\n","Fence           1179\n","MasVnrType       872\n","FireplaceQu      690\n","LotFrontage      259\n","GarageType        81\n","GarageYrBlt       81\n","GarageFinish      81\n","GarageQual        81\n","GarageCond        81\n","BsmtExposure      38\n","BsmtFinType2      38\n","BsmtCond          37\n","BsmtFinType1      37\n","BsmtQual          37\n","MasVnrArea         8\n","Electrical         1\n","dtype: int64"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["missing=df_train.isnull().sum().sort_values(ascending=False)\n","missing=missing.drop(missing[missing==0].index)\n","missing"]},{"cell_type":"markdown","metadata":{},"source":["# Data Processing "]},{"cell_type":"markdown","metadata":{},"source":["Filling the columns that have missing value with logical values:\n","\n","- 'no' value: Used for categorical columns where a missing value means the absence of the characteristic.\n","\n","- Value 0: Used for numeric columns where a missing value means the absence of the characteristic or a zero value."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:32.940691Z","iopub.status.busy":"2024-06-02T12:43:32.939694Z","iopub.status.idle":"2024-06-02T12:43:32.972510Z","shell.execute_reply":"2024-06-02T12:43:32.971725Z","shell.execute_reply.started":"2024-06-02T12:43:32.940648Z"},"trusted":true},"outputs":[],"source":["# Handling missing values\n","for col in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageQual', 'GarageFinish', 'BsmtExposure', 'BsmtCond', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'MasVnrType']:\n","    df_train[col] = df_train[col].fillna('no')\n","    df_test[col] = df_test[col].fillna('no')\n","\n","for col in ['GarageYrBlt', 'GarageType', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'MasVnrArea', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'TotalBsmtSF']:\n","    df_train[col] = df_train[col].fillna(0)\n","    df_test[col] = df_test[col].fillna(0)"]},{"cell_type":"markdown","metadata":{},"source":["Here we calcule again the number of missing value only in columns that still have missing value."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:33.248238Z","iopub.status.busy":"2024-06-02T12:43:33.247599Z","iopub.status.idle":"2024-06-02T12:43:33.265974Z","shell.execute_reply":"2024-06-02T12:43:33.264964Z","shell.execute_reply.started":"2024-06-02T12:43:33.248205Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LotFrontage    259\n","Electrical       1\n","dtype: int64"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["missing=df_train.isnull().sum().sort_values(ascending=False)\n","missing=missing.drop(missing[missing==0].index)\n","missing"]},{"cell_type":"markdown","metadata":{},"source":["Here we cant assigne a value for the LotFrontage and Electrical columns because the value of this columns represent : \n","\n","- LotFrontage represents the linear length in feet of the street that is connected to the property. In other words, it is the measurement of the width of the land that is in direct contact with the street.\n","\n","- Electrical indicates the type of electrical system installed in the property. Electrical systems in homes can vary in safety, efficiency and capacity.\n","\n","So our decision is to drop this 2 columns because :\n","\n","- The LotFrontage column has a missing value rate of 17.74%, which is relatively high. This means that almost a fifth of observations are missing this information. Deleting this column can be justified by the difficulty of filling in these values precisely without introducing bias.\n","\n","- The Electrical column has only one missing value (0.068%). However, its potential impact on selling price prediction may be limited. Variations in electrical systems may not have a significant influence on the overall price of a home compared to other variables like living space, overall quality, etc."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:33.554303Z","iopub.status.busy":"2024-06-02T12:43:33.553703Z","iopub.status.idle":"2024-06-02T12:43:33.565932Z","shell.execute_reply":"2024-06-02T12:43:33.564998Z","shell.execute_reply.started":"2024-06-02T12:43:33.554271Z"},"trusted":true},"outputs":[],"source":["# Dropping columns with remaining missing values\n","df_train.drop(['LotFrontage', 'Electrical'], axis=1, inplace=True)\n","df_test.drop(['LotFrontage', 'Electrical'], axis=1, inplace=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["Now we can verify that our missing values : "]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:33.854701Z","iopub.status.busy":"2024-06-02T12:43:33.853998Z","iopub.status.idle":"2024-06-02T12:43:33.870562Z","shell.execute_reply":"2024-06-02T12:43:33.869561Z","shell.execute_reply.started":"2024-06-02T12:43:33.854671Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Series([], dtype: int64)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["missing=df_train.isnull().sum().sort_values(ascending=False)\n","missing=missing.drop(missing[missing==0].index)\n","missing"]},{"cell_type":"markdown","metadata":{},"source":["Now we need to convert the MSSUbClass columns type int to type string because they do not represent quantities but categories. For example, the values can indicate different building types, such as 20 for a 1946 and later 1-story single-family home, 30 for a pre-1945 1-story single-family home, and so on. Moreover when we process categorical data in a machine learning model, it is often necessary to encode it correctly. By converting these values to strings, we are indicating that these values are distinct categories. Later, we'll use techniques like one-hot encoding to transform these categories into a form that models can use effectively."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:34.138139Z","iopub.status.busy":"2024-06-02T12:43:34.137313Z","iopub.status.idle":"2024-06-02T12:43:34.144983Z","shell.execute_reply":"2024-06-02T12:43:34.144226Z","shell.execute_reply.started":"2024-06-02T12:43:34.138108Z"},"trusted":true},"outputs":[],"source":["# Converting MSSubClass to string\n","df_train['MSSubClass'] = df_train['MSSubClass'].astype(str)\n","df_test['MSSubClass'] = df_test['MSSubClass'].astype(str)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:34.298796Z","iopub.status.busy":"2024-06-02T12:43:34.298263Z","iopub.status.idle":"2024-06-02T12:43:34.326982Z","shell.execute_reply":"2024-06-02T12:43:34.326147Z","shell.execute_reply.started":"2024-06-02T12:43:34.298768Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MSSubClass</th>\n","      <th>MSZoning</th>\n","      <th>Street</th>\n","      <th>Alley</th>\n","      <th>LotShape</th>\n","      <th>LandContour</th>\n","      <th>Utilities</th>\n","      <th>LotConfig</th>\n","      <th>LandSlope</th>\n","      <th>Neighborhood</th>\n","      <th>...</th>\n","      <th>GarageType</th>\n","      <th>GarageFinish</th>\n","      <th>GarageQual</th>\n","      <th>GarageCond</th>\n","      <th>PavedDrive</th>\n","      <th>PoolQC</th>\n","      <th>Fence</th>\n","      <th>MiscFeature</th>\n","      <th>SaleType</th>\n","      <th>SaleCondition</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>CollgCr</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>RFn</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>FR2</td>\n","      <td>Gtl</td>\n","      <td>Veenker</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>RFn</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>CollgCr</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>RFn</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>70</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Corner</td>\n","      <td>Gtl</td>\n","      <td>Crawfor</td>\n","      <td>...</td>\n","      <td>Detchd</td>\n","      <td>Unf</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Abnorml</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>IR1</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>FR2</td>\n","      <td>Gtl</td>\n","      <td>NoRidge</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>RFn</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1455</th>\n","      <td>60</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>Gilbert</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>RFn</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1456</th>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>NWAmes</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>Unf</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>MnPrv</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1457</th>\n","      <td>70</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>Crawfor</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>RFn</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>GdPrv</td>\n","      <td>Shed</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1458</th>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>NAmes</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>Unf</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1459</th>\n","      <td>20</td>\n","      <td>RL</td>\n","      <td>Pave</td>\n","      <td>no</td>\n","      <td>Reg</td>\n","      <td>Lvl</td>\n","      <td>AllPub</td>\n","      <td>Inside</td>\n","      <td>Gtl</td>\n","      <td>Edwards</td>\n","      <td>...</td>\n","      <td>Attchd</td>\n","      <td>Fin</td>\n","      <td>TA</td>\n","      <td>TA</td>\n","      <td>Y</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>no</td>\n","      <td>WD</td>\n","      <td>Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1460 rows × 43 columns</p>\n","</div>"],"text/plain":["     MSSubClass MSZoning Street Alley LotShape LandContour Utilities  \\\n","0            60       RL   Pave    no      Reg         Lvl    AllPub   \n","1            20       RL   Pave    no      Reg         Lvl    AllPub   \n","2            60       RL   Pave    no      IR1         Lvl    AllPub   \n","3            70       RL   Pave    no      IR1         Lvl    AllPub   \n","4            60       RL   Pave    no      IR1         Lvl    AllPub   \n","...         ...      ...    ...   ...      ...         ...       ...   \n","1455         60       RL   Pave    no      Reg         Lvl    AllPub   \n","1456         20       RL   Pave    no      Reg         Lvl    AllPub   \n","1457         70       RL   Pave    no      Reg         Lvl    AllPub   \n","1458         20       RL   Pave    no      Reg         Lvl    AllPub   \n","1459         20       RL   Pave    no      Reg         Lvl    AllPub   \n","\n","     LotConfig LandSlope Neighborhood  ... GarageType GarageFinish GarageQual  \\\n","0       Inside       Gtl      CollgCr  ...     Attchd          RFn         TA   \n","1          FR2       Gtl      Veenker  ...     Attchd          RFn         TA   \n","2       Inside       Gtl      CollgCr  ...     Attchd          RFn         TA   \n","3       Corner       Gtl      Crawfor  ...     Detchd          Unf         TA   \n","4          FR2       Gtl      NoRidge  ...     Attchd          RFn         TA   \n","...        ...       ...          ...  ...        ...          ...        ...   \n","1455    Inside       Gtl      Gilbert  ...     Attchd          RFn         TA   \n","1456    Inside       Gtl       NWAmes  ...     Attchd          Unf         TA   \n","1457    Inside       Gtl      Crawfor  ...     Attchd          RFn         TA   \n","1458    Inside       Gtl        NAmes  ...     Attchd          Unf         TA   \n","1459    Inside       Gtl      Edwards  ...     Attchd          Fin         TA   \n","\n","     GarageCond PavedDrive PoolQC  Fence MiscFeature SaleType SaleCondition  \n","0            TA          Y     no     no          no       WD        Normal  \n","1            TA          Y     no     no          no       WD        Normal  \n","2            TA          Y     no     no          no       WD        Normal  \n","3            TA          Y     no     no          no       WD       Abnorml  \n","4            TA          Y     no     no          no       WD        Normal  \n","...         ...        ...    ...    ...         ...      ...           ...  \n","1455         TA          Y     no     no          no       WD        Normal  \n","1456         TA          Y     no  MnPrv          no       WD        Normal  \n","1457         TA          Y     no  GdPrv        Shed       WD        Normal  \n","1458         TA          Y     no     no          no       WD        Normal  \n","1459         TA          Y     no     no          no       WD        Normal  \n","\n","[1460 rows x 43 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df_train.select_dtypes(include='object')\n"]},{"cell_type":"markdown","metadata":{},"source":["# One hot encoding "]},{"cell_type":"markdown","metadata":{},"source":["Here, we perform a one-hot encoding of the categorical variables for the training (df_train) and test (df_test) datasets, removing the first category to avoid multicollinearity. Then our code aligns the columns of the two datasets. data to ensure they have the same columns, adding zeros for missing columns in the test dataset. \n","Finally, it checks if the 'SalePrice' column is present in the test dataset and removes it if so."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:34.751929Z","iopub.status.busy":"2024-06-02T12:43:34.751613Z","iopub.status.idle":"2024-06-02T12:43:34.827320Z","shell.execute_reply":"2024-06-02T12:43:34.826616Z","shell.execute_reply.started":"2024-06-02T12:43:34.751904Z"},"trusted":true},"outputs":[],"source":["# One-hot encoding for categorical variables\n","df_train = pd.get_dummies(df_train, drop_first=True)\n","df_test = pd.get_dummies(df_test, drop_first=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:34.913794Z","iopub.status.busy":"2024-06-02T12:43:34.913213Z","iopub.status.idle":"2024-06-02T12:43:34.923315Z","shell.execute_reply":"2024-06-02T12:43:34.922579Z","shell.execute_reply.started":"2024-06-02T12:43:34.913766Z"},"trusted":true},"outputs":[],"source":["# Aligning train and test data to ensure they have the same columns\n","df_train, df_test = df_train.align(df_test, join='left', axis=1, fill_value=0)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:35.069350Z","iopub.status.busy":"2024-06-02T12:43:35.069070Z","iopub.status.idle":"2024-06-02T12:43:35.077014Z","shell.execute_reply":"2024-06-02T12:43:35.075927Z","shell.execute_reply.started":"2024-06-02T12:43:35.069325Z"},"trusted":true},"outputs":[],"source":["# Ensure 'SalePrice' is not in df_test\n","if 'SalePrice' in df_test.columns:\n","    df_test.drop('SalePrice', axis=1, inplace=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Selection"]},{"cell_type":"markdown","metadata":{},"source":["We separate the training data (df_train) into feature variables (X) and target variable (y) by removing the 'SalePrice' column from X and assigning it to y. Next, we align the columns of the training and test datasets (df_train and df_test) to ensure they have the same columns, adding zeros for missing columns in the test dataset."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:36.390629Z","iopub.status.busy":"2024-06-02T12:43:36.390217Z","iopub.status.idle":"2024-06-02T12:43:36.397443Z","shell.execute_reply":"2024-06-02T12:43:36.396235Z","shell.execute_reply.started":"2024-06-02T12:43:36.390596Z"},"trusted":true},"outputs":[],"source":["# Splitting data into features and target variable\n","X = df_train.drop('SalePrice', axis=1)\n","y = df_train['SalePrice']"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:36.834230Z","iopub.status.busy":"2024-06-02T12:43:36.833888Z","iopub.status.idle":"2024-06-02T12:43:36.848307Z","shell.execute_reply":"2024-06-02T12:43:36.847321Z","shell.execute_reply.started":"2024-06-02T12:43:36.834203Z"},"trusted":true},"outputs":[],"source":["# Splitting data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=101)"]},{"cell_type":"markdown","metadata":{},"source":["# Deep Learning Model "]},{"cell_type":"markdown","metadata":{},"source":["Here we install the a specifi version of scikit-learn to be sure that we can import KerasRegressor without error :"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:43:37.920486Z","iopub.status.busy":"2024-06-02T12:43:37.920095Z","iopub.status.idle":"2024-06-02T12:43:42.596018Z","shell.execute_reply":"2024-06-02T12:43:42.594759Z","shell.execute_reply.started":"2024-06-02T12:43:37.920455Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: scikit-learn 1.2.2\n","Uninstalling scikit-learn-1.2.2:\n","  Successfully uninstalled scikit-learn-1.2.2\n","\u001b[33mWARNING: Skipping scikeras as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip uninstall -y scikit-learn scikeras\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:44:25.188839Z","iopub.status.busy":"2024-06-02T12:44:25.188078Z","iopub.status.idle":"2024-06-02T12:44:43.519301Z","shell.execute_reply":"2024-06-02T12:44:43.518193Z","shell.execute_reply.started":"2024-06-02T12:44:25.188790Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting scikit-learn==1.2.2\n","  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting scikeras==0.10.0\n","  Downloading scikeras-0.10.0-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.26.4)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (3.2.0)\n","Requirement already satisfied: packaging>=0.21 in /opt/conda/lib/python3.10/site-packages (from scikeras==0.10.0) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=0.21->scikeras==0.10.0) (3.1.1)\n","Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading scikeras-0.10.0-py3-none-any.whl (27 kB)\n","Installing collected packages: scikit-learn, scikeras\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed scikeras-0.10.0 scikit-learn-1.2.2\n"]}],"source":["!pip install scikit-learn==1.2.2 scikeras==0.10.0\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:44:43.521979Z","iopub.status.busy":"2024-06-02T12:44:43.521591Z","iopub.status.idle":"2024-06-02T12:44:56.441525Z","shell.execute_reply":"2024-06-02T12:44:56.440370Z","shell.execute_reply.started":"2024-06-02T12:44:43.521943Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: keras-tuner in /opt/conda/lib/python3.10/site-packages (1.4.6)\n","Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (3.2.1)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (21.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (2.31.0)\n","Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (1.0.5)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (1.4.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (1.26.4)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (13.7.0)\n","Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (0.0.8)\n","Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (3.10.0)\n","Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (0.11.0)\n","Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras->keras-tuner) (0.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-tuner) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->keras-tuner) (2024.2.2)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras->keras-tuner) (4.9.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras->keras-tuner) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras->keras-tuner) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install keras-tuner\n"]},{"cell_type":"markdown","metadata":{},"source":["Machine and Deep Learning library:"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:45:07.985268Z","iopub.status.busy":"2024-06-02T12:45:07.984280Z","iopub.status.idle":"2024-06-02T12:45:26.647346Z","shell.execute_reply":"2024-06-02T12:45:26.646380Z","shell.execute_reply.started":"2024-06-02T12:45:07.985226Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-02 12:45:11.373175: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-06-02 12:45:11.373295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-06-02 12:45:11.614010: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","/tmp/ipykernel_204/849885283.py:12: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n","  from kerastuner import HyperModel\n"]}],"source":["import random\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.regularizers import l2\n","from sklearn.model_selection import GridSearchCV, ParameterGrid\n","from sklearn.metrics import mean_squared_error\n","from scikeras.wrappers import KerasRegressor \n","from kerastuner import HyperModel\n","from kerastuner.tuners import RandomSearch"]},{"cell_type":"markdown","metadata":{},"source":["We set a random seed value to 42 to ensure reproducibility. Next, we have a set_seed function that sets this seed for different modules: np.random for NumPy, tf.random for TensorFlow, and random for Python's random module. This ensures that the results of our random operations are consistent each time we run the code :"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:45:37.960225Z","iopub.status.busy":"2024-06-02T12:45:37.959566Z","iopub.status.idle":"2024-06-02T12:45:37.964642Z","shell.execute_reply":"2024-06-02T12:45:37.963635Z","shell.execute_reply.started":"2024-06-02T12:45:37.960194Z"},"trusted":true},"outputs":[],"source":["# Fixing the random seed for reproducibility\n","seed_value = 42"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:45:38.540024Z","iopub.status.busy":"2024-06-02T12:45:38.539663Z","iopub.status.idle":"2024-06-02T12:45:38.545281Z","shell.execute_reply":"2024-06-02T12:45:38.544325Z","shell.execute_reply.started":"2024-06-02T12:45:38.539994Z"},"trusted":true},"outputs":[],"source":["def set_seed(seed_value):\n","    np.random.seed(seed_value)\n","    tf.random.set_seed(seed_value)\n","    random.seed(seed_value)"]},{"cell_type":"markdown","metadata":{},"source":["We use a StandardScaler to normalize the data. We adjust and transform the training data (X_train) then transform the validation (X_val) and test (df_test) data using this same scaler. Next, we transform the target variable y (SalePrice) to a logarithmic scale for the training (y_train) and validation (y_val) data to stabilize the variance and make the data more consistent with a normal distribution : "]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:45:39.387918Z","iopub.status.busy":"2024-06-02T12:45:39.386999Z","iopub.status.idle":"2024-06-02T12:45:39.447781Z","shell.execute_reply":"2024-06-02T12:45:39.446903Z","shell.execute_reply.started":"2024-06-02T12:45:39.387881Z"},"trusted":true},"outputs":[],"source":["# Scaling the data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n","X_test_scaled = scaler.transform(df_test)\n","\n","# Transforming y (SalePrice) into logarithmic scale\n","y_train_log = np.log(y_train)\n","y_val_log = np.log(y_val)"]},{"cell_type":"markdown","metadata":{},"source":["We have a function to create a model using Keras. It starts by initializing a sequential model and adds an input layer with the shape of the normalized training data. Next, we add several dense layers with ReLU activation, L2 regularization, and the Glorot Uniform initializer, followed by batch normalization and dropout regularization. Finally, a dense output layer without activation is added for regression, and the model is compiled with a specified optimizer and loss function, measuring the mean square error:"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:45:40.377312Z","iopub.status.busy":"2024-06-02T12:45:40.376931Z","iopub.status.idle":"2024-06-02T12:45:40.387285Z","shell.execute_reply":"2024-06-02T12:45:40.386299Z","shell.execute_reply.started":"2024-06-02T12:45:40.377276Z"},"trusted":true},"outputs":[],"source":["# Function to create model\n","def create_model(optimizer, loss):\n","    model = Sequential()\n","    model.add(Input(shape=(X_train_scaled.shape[1],)))\n","    model.add(Dense(256, activation='relu', kernel_regularizer='l2', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value)))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","    \n","    model.add(Dense(128, activation='relu', kernel_regularizer='l2', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value)))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","    \n","    model.add(Dense(64, activation='relu', kernel_regularizer='l2', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value)))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","    \n","    model.add(Dense(32, activation='relu', kernel_regularizer='l2', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value)))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","    \n","    model.add(Dense(1, activation='linear', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value)))  # No activation for regression\n","    model.compile(optimizer=optimizer, loss=loss, metrics=['mse'])\n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["We define a list of optimizers (Adam, SGD, RMSprop, Nadam) and a list of loss functions (mse, mae, huber) to test. We specify a list of values for the number of epochs (here 150) and additional epochs (100) for further training with different loss functions. We also include a list of learning rates (0.001, 0.01, 0.1) to test. Next, we create a dictionary (unique_params) containing specific parameters for each optimizer. Finally, we initialize a results dictionary to store the results of the different tests :"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:45:41.323168Z","iopub.status.busy":"2024-06-02T12:45:41.322495Z","iopub.status.idle":"2024-06-02T12:45:41.330308Z","shell.execute_reply":"2024-06-02T12:45:41.329121Z","shell.execute_reply.started":"2024-06-02T12:45:41.323134Z"},"trusted":true},"outputs":[],"source":["optimizers = [Adam, SGD, RMSprop, Nadam]\n","loss_functions = ['mse', 'mae', 'huber']  # List of loss functions to test\n","epochs_list = [150]  # Example epoch values\n","additional_epochs = [100]  # Additional training for 100 epochs with different loss functions\n","learning_rates = [0.001, 0.01, 0.1]  # List of learning rates to test\n","unique_params = {\n","    \"Adam\": {\"learning_rate\": 0.01, \"beta_1\": 0.9, \"beta_2\": 0.999, \"epsilon\": 1e-07, \"amsgrad\": False},\n","    \"SGD\": {\"learning_rate\": 0.01, \"momentum\": 0.9, \"nesterov\": True},\n","    \"RMSprop\": {\"learning_rate\": 0.01, \"rho\": 0.9, \"momentum\": 0.9, \"epsilon\": 1e-07, \"centered\": True},\n","    \"Nadam\": {\"learning_rate\": 0.01, \"beta_1\": 0.9, \"beta_2\": 0.999, \"epsilon\": 1e-07},\n","}\n","results = {}"]},{"cell_type":"markdown","metadata":{},"source":["We have a display_optimizer_parameters function that displays the optimizer parameters. For each optimizer in the optimizers list, we create an instance of the optimizer with default settings. We get the optimizer name (opt_name) and its configuration (config). Next, we print the configuration parameters for each optimizer, displaying them as key-value pairs. Each parameter block is followed by a line break for better readability : "]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:45:42.234806Z","iopub.status.busy":"2024-06-02T12:45:42.234173Z","iopub.status.idle":"2024-06-02T12:45:42.240640Z","shell.execute_reply":"2024-06-02T12:45:42.239585Z","shell.execute_reply.started":"2024-06-02T12:45:42.234772Z"},"trusted":true},"outputs":[],"source":["# Function to display optimizer parameters\n","def display_optimizer_parameters(optimizers):\n","    for opt in optimizers:\n","        optimizer_instance = opt()  # Instantiate optimizer with default parameters\n","        opt_name = opt.__name__  # Get the name of the optimizer\n","        config = optimizer_instance.get_config()  # Get optimizer configuration\n","        print(f\"Parameters for {opt_name}:\")\n","        for key, value in config.items():\n","            print(f\"  {key}: {value}\")\n","        print(\"\\n\")\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:45:43.412161Z","iopub.status.busy":"2024-06-02T12:45:43.411440Z","iopub.status.idle":"2024-06-02T12:45:44.507640Z","shell.execute_reply":"2024-06-02T12:45:44.506373Z","shell.execute_reply.started":"2024-06-02T12:45:43.412126Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameters for Adam:\n","  name: adam\n","  learning_rate: 0.0010000000474974513\n","  weight_decay: None\n","  clipnorm: None\n","  global_clipnorm: None\n","  clipvalue: None\n","  use_ema: False\n","  ema_momentum: 0.99\n","  ema_overwrite_frequency: None\n","  loss_scale_factor: None\n","  gradient_accumulation_steps: None\n","  beta_1: 0.9\n","  beta_2: 0.999\n","  epsilon: 1e-07\n","  amsgrad: False\n","\n","\n","Parameters for SGD:\n","  name: SGD\n","  learning_rate: 0.009999999776482582\n","  weight_decay: None\n","  clipnorm: None\n","  global_clipnorm: None\n","  clipvalue: None\n","  use_ema: False\n","  ema_momentum: 0.99\n","  ema_overwrite_frequency: None\n","  loss_scale_factor: None\n","  gradient_accumulation_steps: None\n","  momentum: 0.0\n","  nesterov: False\n","\n","\n","Parameters for RMSprop:\n","  name: rmsprop\n","  learning_rate: 0.0010000000474974513\n","  weight_decay: None\n","  clipnorm: None\n","  global_clipnorm: None\n","  clipvalue: None\n","  use_ema: False\n","  ema_momentum: 0.99\n","  ema_overwrite_frequency: 100\n","  loss_scale_factor: None\n","  gradient_accumulation_steps: None\n","  rho: 0.9\n","  momentum: 0.0\n","  epsilon: 1e-07\n","  centered: False\n","\n","\n","Parameters for Nadam:\n","  name: nadam\n","  learning_rate: 0.0010000000474974513\n","  weight_decay: None\n","  clipnorm: None\n","  global_clipnorm: None\n","  clipvalue: None\n","  use_ema: False\n","  ema_momentum: 0.99\n","  ema_overwrite_frequency: None\n","  loss_scale_factor: None\n","  gradient_accumulation_steps: None\n","  beta_1: 0.9\n","  beta_2: 0.999\n","  epsilon: 1e-07\n","\n","\n"]}],"source":["# Display parameters for each optimizer\n","display_optimizer_parameters(optimizers)"]},{"cell_type":"markdown","metadata":{},"source":["### Adam\n","**Unique parameters:**\n","- **beta_1:** Coefficient for growth and development (default 0.9).\n","- **beta_2:** Coefficient for growth and development (default 0.999).\n","- **epsilon:** Small number to avoid division by zero (default 1e-07).\n","- **amsgrad:** Boolean indicating if the AMSGrad algorithm should be used (default False).\n","\n","### SGD\n","**Unique parameters:**\n","- **momentum:** Rate of momentum for acceleration of gradient descent (default 0.0).\n","- **nesterov:** Boolean indicating if Nesterov momentum should be used (default False).\n","\n","### RMSprop\n","**Unique parameters:**\n","- **rho:** Decay factor for the moving average of squared gradients (default 0.9).\n","- **momentum:** Rate of momentum for acceleration of gradient descent (default 0.0).\n","- **epsilon:** Small number to avoid division by zero (default 1e-07).\n","- **centered:** Boolean indicating if the variance should be centered (default False).\n","\n","### Nadam\n","**Unique parameters:**\n","- **beta_1:** Coefficient for growth and development (default 0.9).\n","- **beta_2:** Coefficient for growth and development (default 0.999).\n","- **epsilon:** Small number to avoid division by zero (default 1e-07).\n","\n","### Common Parameters\n","All these optimizers share the following parameters (although their default values may differ):\n","- **learning_rate:** Learning rate.\n","- **weight_decay:** Weight decay (default None).\n","- **clipnorm:** Norm for gradient clipping (default None).\n","- **global_clipnorm:** Global norm for gradient clipping (default None).\n","- **clipvalue:** Value for gradient clipping (default None).\n","- **use_ema:** Boolean indicating if Exponential Moving Average (EMA) should be used (default False).\n","- **ema_momentum:** Momentum for EMA (default 0.99).\n","- **ema_overwrite_frequency:** Overwrite frequency for EMA (default None).\n","- **loss_scale_factor:** Loss scaling factor (default None).\n","- **gradient_accumulation_steps:** Number of gradient accumulation steps (default None).\n","\n","### Summary of Unique Parameters\n","\n","| Optimizer |  Unique Parameters |\n","|------------|--------------------|\n","| Adam       | beta_1, beta_2, epsilon, amsgrad |\n","| SGD        | momentum, nesterov |\n","| RMSprop    | rho, momentum, epsilon, centered |\n","| Nadam      | beta_1, beta_2, epsilon |\n","\n","These unique parameters are what distinguish each optimizer from the others, affecting how they update the model weights during training."]},{"cell_type":"markdown","metadata":{},"source":["## Test epoch : 0 to 150"]},{"cell_type":"markdown","metadata":{},"source":["We iterate over each optimizer in the optimizers list. For each optimizer, we obtain its class name (opt_name) and initialize an entry in the results dictionary. For each value in epochs_list, we display a message indicating the optimizer and the number of epochs being trained.\n","\n","We reset the random seed to ensure reproducibility (set_seed(seed_value)). Next, we create an instance of the optimizer and a model using the create_model function with the optimizer and the default 'mse' loss function.\n","\n","We define callbacks for early stopping (EarlyStopping) and saving the best model (ModelCheckpoint). We train the model on the normalized and log-transformed training data (X_train_scaled and y_train_log), using 20% cross-validation of the data, for the specified number of epochs.\n","\n","Next, we load the weights of the best model saved by ModelCheckpoint. We make predictions on the normalized validation data (X_val_scaled), calculate the mean square error (MSE) between the predicted and actual log-transformed values, and store the results (MSE and training history) in the results dictionary.\n","\n","Finally, we display the MSE for the optimizer and the number of epochs being trained."]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:46:03.427536Z","iopub.status.busy":"2024-06-02T12:46:03.426873Z","iopub.status.idle":"2024-06-02T12:48:57.957722Z","shell.execute_reply":"2024-06-02T12:48:57.956659Z","shell.execute_reply.started":"2024-06-02T12:46:03.427505Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training with optimizer: Adam, epochs: 150\n","Epoch 1/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 151.8560 - mse: 146.2713  "]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1717332374.741595     269 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 151.7273 - mse: 146.1441\n","Epoch 1: val_loss improved from inf to 144.39944, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 328ms/step - loss: 151.7009 - mse: 146.1179 - val_loss: 144.3994 - val_mse: 138.8348\n","Epoch 2/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 147.1675 - mse: 141.6054 \n","Epoch 2: val_loss improved from 144.39944 to 139.98276, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 146.9201 - mse: 141.3585 - val_loss: 139.9828 - val_mse: 134.4307\n","Epoch 3/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 140.3926 - mse: 134.8424 \n","Epoch 3: val_loss improved from 139.98276 to 135.39253, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 140.3515 - mse: 134.8015 - val_loss: 135.3925 - val_mse: 129.8475\n","Epoch 4/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 135.8861 - mse: 130.3417 \n","Epoch 4: val_loss improved from 135.39253 to 129.22597, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 135.6197 - mse: 130.0756 - val_loss: 129.2260 - val_mse: 123.6852\n","Epoch 5/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.5248 - mse: 125.9845 \n","Epoch 5: val_loss improved from 129.22597 to 123.52956, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 131.3670 - mse: 125.8267 - val_loss: 123.5296 - val_mse: 117.9900\n","Epoch 6/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 124.1503 - mse: 118.6110 \n","Epoch 6: val_loss improved from 123.52956 to 116.66747, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 123.9950 - mse: 118.4557 - val_loss: 116.6675 - val_mse: 111.1295\n","Epoch 7/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 115.8774 - mse: 110.3387 \n","Epoch 7: val_loss improved from 116.66747 to 107.28171, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 115.7553 - mse: 110.2166 - val_loss: 107.2817 - val_mse: 101.7410\n","Epoch 8/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 110.2128 - mse: 104.6716 \n","Epoch 8: val_loss improved from 107.28171 to 97.49488, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 109.8402 - mse: 104.2990 - val_loss: 97.4949 - val_mse: 91.9523\n","Epoch 9/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100.5321 - mse: 94.9887 \n","Epoch 9: val_loss improved from 97.49488 to 88.04699, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 100.1450 - mse: 94.6014 - val_loss: 88.0470 - val_mse: 82.4999\n","Epoch 10/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91.4305 - mse: 85.8826 \n","Epoch 10: val_loss improved from 88.04699 to 78.06598, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 90.8450 - mse: 85.2969 - val_loss: 78.0660 - val_mse: 72.5153\n","Epoch 11/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.1391 - mse: 72.5875 \n","Epoch 11: val_loss improved from 78.06598 to 67.31535, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 77.7782 - mse: 72.2264 - val_loss: 67.3153 - val_mse: 61.7604\n","Epoch 12/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.7378 - mse: 62.1822 \n","Epoch 12: val_loss improved from 67.31535 to 56.45268, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 67.3573 - mse: 61.8016 - val_loss: 56.4527 - val_mse: 50.8937\n","Epoch 13/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57.8275 - mse: 52.2677 \n","Epoch 13: val_loss improved from 56.45268 to 46.37225, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 57.4853 - mse: 51.9254 - val_loss: 46.3722 - val_mse: 40.8097\n","Epoch 14/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.4048 - mse: 42.8417 \n","Epoch 14: val_loss improved from 46.37225 to 37.29024, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 48.1428 - mse: 42.5797 - val_loss: 37.2902 - val_mse: 31.7267\n","Epoch 15/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.5086 - mse: 34.9448 \n","Epoch 15: val_loss improved from 37.29024 to 29.86378, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 39.9165 - mse: 34.3527 - val_loss: 29.8638 - val_mse: 24.3005\n","Epoch 16/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32.4529 - mse: 26.8901 \n","Epoch 16: val_loss improved from 29.86378 to 22.87077, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32.1138 - mse: 26.5511 - val_loss: 22.8708 - val_mse: 17.3102\n","Epoch 17/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28.1651 - mse: 22.6053 \n","Epoch 17: val_loss improved from 22.87077 to 17.74915, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27.8346 - mse: 22.2751 - val_loss: 17.7491 - val_mse: 12.1928\n","Epoch 18/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22.9170 - mse: 17.3617 \n","Epoch 18: val_loss improved from 17.74915 to 14.01122, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.7588 - mse: 17.2037 - val_loss: 14.0112 - val_mse: 8.4600\n","Epoch 19/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18.5637 - mse: 13.0136 \n","Epoch 19: val_loss improved from 14.01122 to 11.48183, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.6108 - mse: 13.0610 - val_loss: 11.4818 - val_mse: 5.9360\n","Epoch 20/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.4584 - mse: 11.9135 \n","Epoch 20: val_loss improved from 11.48183 to 9.26194, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.2922 - mse: 11.7477 - val_loss: 9.2619 - val_mse: 3.7214\n","Epoch 21/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.6094 - mse: 10.0699 \n","Epoch 21: val_loss improved from 9.26194 to 7.97892, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.5803 - mse: 10.0412 - val_loss: 7.9789 - val_mse: 2.4447\n","Epoch 22/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.8213 - mse: 9.2886  \n","Epoch 22: val_loss improved from 7.97892 to 7.18055, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.6779 - mse: 9.1455 - val_loss: 7.1806 - val_mse: 1.6531\n","Epoch 23/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.7976 - mse: 7.2718 \n","Epoch 23: val_loss improved from 7.18055 to 6.63913, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.7614 - mse: 7.2359 - val_loss: 6.6391 - val_mse: 1.1205\n","Epoch 24/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.0157 - mse: 7.4991 \n","Epoch 24: val_loss improved from 6.63913 to 6.41648, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.9834 - mse: 7.4673 - val_loss: 6.4165 - val_mse: 0.9064\n","Epoch 25/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.5597 - mse: 7.0513 \n","Epoch 25: val_loss improved from 6.41648 to 6.17031, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.5881 - mse: 7.0800 - val_loss: 6.1703 - val_mse: 0.6686\n","Epoch 26/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.0017 - mse: 6.5019 \n","Epoch 26: val_loss improved from 6.17031 to 6.10852, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.0157 - mse: 6.5163 - val_loss: 6.1085 - val_mse: 0.6163\n","Epoch 27/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.9310 - mse: 6.4406 \n","Epoch 27: val_loss improved from 6.10852 to 6.04258, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.8970 - mse: 6.4072 - val_loss: 6.0426 - val_mse: 0.5606\n","Epoch 28/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.1858 - mse: 5.7062\n","Epoch 28: val_loss improved from 6.04258 to 5.91700, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.2197 - mse: 5.7405 - val_loss: 5.9170 - val_mse: 0.4460\n","Epoch 29/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.3657 - mse: 5.8969\n","Epoch 29: val_loss improved from 5.91700 to 5.82837, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3608 - mse: 5.8925 - val_loss: 5.8284 - val_mse: 0.3687\n","Epoch 30/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2356 - mse: 6.7782 \n","Epoch 30: val_loss improved from 5.82837 to 5.79699, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.1112 - mse: 6.6542 - val_loss: 5.7970 - val_mse: 0.3477\n","Epoch 31/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.9872 - mse: 5.5402 \n","Epoch 31: val_loss did not improve from 5.79699\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.9580 - mse: 5.5115 - val_loss: 5.8048 - val_mse: 0.3670\n","Epoch 32/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.7232 - mse: 5.2876 \n","Epoch 32: val_loss improved from 5.79699 to 5.74989, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6560 - mse: 5.2209 - val_loss: 5.7499 - val_mse: 0.3237\n","Epoch 33/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6441 - mse: 5.2204 \n","Epoch 33: val_loss improved from 5.74989 to 5.74659, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6357 - mse: 5.2127 - val_loss: 5.7466 - val_mse: 0.3331\n","Epoch 34/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.7853 - mse: 5.3744 \n","Epoch 34: val_loss improved from 5.74659 to 5.67390, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.7743 - mse: 5.3638 - val_loss: 5.6739 - val_mse: 0.2728\n","Epoch 35/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4741 - mse: 5.0758 \n","Epoch 35: val_loss did not improve from 5.67390\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.4753 - mse: 5.0774 - val_loss: 5.6852 - val_mse: 0.2969\n","Epoch 36/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6655 - mse: 5.2798 \n","Epoch 36: val_loss did not improve from 5.67390\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.6231 - mse: 5.2378 - val_loss: 5.6782 - val_mse: 0.3022\n","Epoch 37/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9612 - mse: 4.5882 \n","Epoch 37: val_loss improved from 5.67390 to 5.64751, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.0101 - mse: 4.6376 - val_loss: 5.6475 - val_mse: 0.2857\n","Epoch 38/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4458 - mse: 4.0871 \n","Epoch 38: val_loss improved from 5.64751 to 5.64236, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5037 - mse: 4.1456 - val_loss: 5.6424 - val_mse: 0.2953\n","Epoch 39/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1764 - mse: 4.8325\n","Epoch 39: val_loss improved from 5.64236 to 5.57151, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.2100 - mse: 4.8666 - val_loss: 5.5715 - val_mse: 0.2389\n","Epoch 40/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6662 - mse: 4.3367 \n","Epoch 40: val_loss improved from 5.57151 to 5.54069, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6956 - mse: 4.3667 - val_loss: 5.5407 - val_mse: 0.2225\n","Epoch 41/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4317 - mse: 4.1162 \n","Epoch 41: val_loss did not improve from 5.54069\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.4708 - mse: 4.1562 - val_loss: 5.5625 - val_mse: 0.2590\n","Epoch 42/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1708 - mse: 3.8703  \n","Epoch 42: val_loss improved from 5.54069 to 5.52765, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2044 - mse: 3.9048 - val_loss: 5.5277 - val_mse: 0.2398\n","Epoch 43/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5241 - mse: 4.2393 \n","Epoch 43: val_loss improved from 5.52765 to 5.49161, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.4847 - mse: 4.2006 - val_loss: 5.4916 - val_mse: 0.2193\n","Epoch 44/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1885 - mse: 3.9195 \n","Epoch 44: val_loss improved from 5.49161 to 5.45722, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.1685 - mse: 3.9003 - val_loss: 5.4572 - val_mse: 0.2016\n","Epoch 45/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5199 - mse: 4.2678 \n","Epoch 45: val_loss improved from 5.45722 to 5.45486, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5190 - mse: 4.2675 - val_loss: 5.4549 - val_mse: 0.2160\n","Epoch 46/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9398 - mse: 3.7046 \n","Epoch 46: val_loss improved from 5.45486 to 5.40670, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9436 - mse: 3.7091 - val_loss: 5.4067 - val_mse: 0.1855\n","Epoch 47/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5674 - mse: 4.3498 \n","Epoch 47: val_loss improved from 5.40670 to 5.38643, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5854 - mse: 4.3685 - val_loss: 5.3864 - val_mse: 0.1827\n","Epoch 48/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6098 - mse: 3.4097 \n","Epoch 48: val_loss did not improve from 5.38643\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6680 - mse: 3.4686 - val_loss: 5.4034 - val_mse: 0.2175\n","Epoch 49/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7655 - mse: 3.5835 \n","Epoch 49: val_loss improved from 5.38643 to 5.35821, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7792 - mse: 3.5977 - val_loss: 5.3582 - val_mse: 0.1899\n","Epoch 50/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5724 - mse: 3.4080 \n","Epoch 50: val_loss improved from 5.35821 to 5.33854, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5627 - mse: 3.3989 - val_loss: 5.3385 - val_mse: 0.1888\n","Epoch 51/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7377 - mse: 3.5916 \n","Epoch 51: val_loss improved from 5.33854 to 5.27145, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7939 - mse: 3.6488 - val_loss: 5.2714 - val_mse: 0.1407\n","Epoch 52/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6965 - mse: 3.5696  \n","Epoch 52: val_loss improved from 5.27145 to 5.25533, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6636 - mse: 3.5375 - val_loss: 5.2553 - val_mse: 0.1437\n","Epoch 53/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9420 - mse: 3.8341 \n","Epoch 53: val_loss did not improve from 5.25533\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.8717 - mse: 3.7647 - val_loss: 5.2708 - val_mse: 0.1780\n","Epoch 54/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0017 - mse: 3.9130 \n","Epoch 54: val_loss improved from 5.25533 to 5.20475, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9615 - mse: 3.8735 - val_loss: 5.2047 - val_mse: 0.1316\n","Epoch 55/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.4026 - mse: 3.3337 \n","Epoch 55: val_loss improved from 5.20475 to 5.19456, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.4115 - mse: 3.3433 - val_loss: 5.1946 - val_mse: 0.1414\n","Epoch 56/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3770 - mse: 3.3280 \n","Epoch 56: val_loss improved from 5.19456 to 5.18873, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.3940 - mse: 3.3458 - val_loss: 5.1887 - val_mse: 0.1557\n","Epoch 57/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3322 - mse: 3.3034 \n","Epoch 57: val_loss improved from 5.18873 to 5.15845, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.3097 - mse: 3.2816 - val_loss: 5.1585 - val_mse: 0.1456\n","Epoch 58/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1841 - mse: 3.1756 \n","Epoch 58: val_loss improved from 5.15845 to 5.15418, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2190 - mse: 3.2113 - val_loss: 5.1542 - val_mse: 0.1621\n","Epoch 59/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9090 - mse: 3.9208 \n","Epoch 59: val_loss improved from 5.15418 to 5.09189, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7991 - mse: 3.8120 - val_loss: 5.0919 - val_mse: 0.1205\n","Epoch 60/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0039 - mse: 3.0366 \n","Epoch 60: val_loss improved from 5.09189 to 5.06318, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0073 - mse: 3.0413 - val_loss: 5.0632 - val_mse: 0.1137\n","Epoch 61/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9402 - mse: 2.9952 \n","Epoch 61: val_loss improved from 5.06318 to 5.04612, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9505 - mse: 3.0065 - val_loss: 5.0461 - val_mse: 0.1185\n","Epoch 62/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1676 - mse: 3.2442 \n","Epoch 62: val_loss improved from 5.04612 to 5.02270, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1463 - mse: 3.2242 - val_loss: 5.0227 - val_mse: 0.1173\n","Epoch 63/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9994 - mse: 3.0986 \n","Epoch 63: val_loss improved from 5.02270 to 4.98946, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0034 - mse: 3.1034 - val_loss: 4.9895 - val_mse: 0.1061\n","Epoch 64/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1455 - mse: 3.2669 \n","Epoch 64: val_loss improved from 4.98946 to 4.97769, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1513 - mse: 3.2735 - val_loss: 4.9777 - val_mse: 0.1169\n","Epoch 65/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3263 - mse: 3.4701 \n","Epoch 65: val_loss improved from 4.97769 to 4.94402, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2843 - mse: 3.4290 - val_loss: 4.9440 - val_mse: 0.1056\n","Epoch 66/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1414 - mse: 3.3077 \n","Epoch 66: val_loss improved from 4.94402 to 4.91745, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1333 - mse: 3.3005 - val_loss: 4.9174 - val_mse: 0.1018\n","Epoch 67/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0136 - mse: 3.2027 \n","Epoch 67: val_loss improved from 4.91745 to 4.88479, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0091 - mse: 3.1990 - val_loss: 4.8848 - val_mse: 0.0917\n","Epoch 68/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8749 - mse: 3.0866 \n","Epoch 68: val_loss improved from 4.88479 to 4.86526, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.8264 - mse: 3.0389 - val_loss: 4.8653 - val_mse: 0.0952\n","Epoch 69/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7748 - mse: 3.0093 \n","Epoch 69: val_loss improved from 4.86526 to 4.84919, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7598 - mse: 2.9957 - val_loss: 4.8492 - val_mse: 0.1028\n","Epoch 70/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4676 - mse: 2.7259 \n","Epoch 70: val_loss improved from 4.84919 to 4.82795, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4954 - mse: 2.7551 - val_loss: 4.8280 - val_mse: 0.1060\n","Epoch 71/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0700 - mse: 2.3533 \n","Epoch 71: val_loss improved from 4.82795 to 4.79638, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.0996 - mse: 2.3839 - val_loss: 4.7964 - val_mse: 0.0993\n","Epoch 72/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1908 - mse: 2.4986 \n","Epoch 72: val_loss improved from 4.79638 to 4.77325, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1939 - mse: 2.5028 - val_loss: 4.7733 - val_mse: 0.1008\n","Epoch 73/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6865 - mse: 3.0188 \n","Epoch 73: val_loss improved from 4.77325 to 4.74504, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.6580 - mse: 2.9918 - val_loss: 4.7450 - val_mse: 0.0977\n","Epoch 74/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6334 - mse: 2.9909  \n","Epoch 74: val_loss improved from 4.74504 to 4.70856, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5729 - mse: 2.9318 - val_loss: 4.7086 - val_mse: 0.0866\n","Epoch 75/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5156 - mse: 2.8985 \n","Epoch 75: val_loss improved from 4.70856 to 4.69286, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5030 - mse: 2.8874 - val_loss: 4.6929 - val_mse: 0.0962\n","Epoch 76/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1237 - mse: 2.5314  \n","Epoch 76: val_loss improved from 4.69286 to 4.65915, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1559 - mse: 2.5656 - val_loss: 4.6591 - val_mse: 0.0881\n","Epoch 77/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2479 - mse: 2.6818 \n","Epoch 77: val_loss improved from 4.65915 to 4.63823, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2664 - mse: 2.7017 - val_loss: 4.6382 - val_mse: 0.0926\n","Epoch 78/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0512 - mse: 2.5108 \n","Epoch 78: val_loss improved from 4.63823 to 4.61531, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0738 - mse: 2.5346 - val_loss: 4.6153 - val_mse: 0.0955\n","Epoch 79/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7981 - mse: 2.2839 \n","Epoch 79: val_loss improved from 4.61531 to 4.56622, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8094 - mse: 2.2962 - val_loss: 4.5662 - val_mse: 0.0729\n","Epoch 80/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0463 - mse: 2.5586 \n","Epoch 80: val_loss improved from 4.56622 to 4.54795, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0388 - mse: 2.5521 - val_loss: 4.5479 - val_mse: 0.0811\n","Epoch 81/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8189 - mse: 2.3576 \n","Epoch 81: val_loss improved from 4.54795 to 4.52064, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8322 - mse: 2.3720 - val_loss: 4.5206 - val_mse: 0.0804\n","Epoch 82/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1851 - mse: 2.7506 \n","Epoch 82: val_loss improved from 4.52064 to 4.48695, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1540 - mse: 2.7204 - val_loss: 4.4869 - val_mse: 0.0733\n","Epoch 83/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8230 - mse: 2.4151 \n","Epoch 83: val_loss improved from 4.48695 to 4.45687, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8392 - mse: 2.4323 - val_loss: 4.4569 - val_mse: 0.0700\n","Epoch 84/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7554 - mse: 2.3739 \n","Epoch 84: val_loss improved from 4.45687 to 4.42955, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7862 - mse: 2.4061 - val_loss: 4.4296 - val_mse: 0.0697\n","Epoch 85/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8453 - mse: 2.4904 \n","Epoch 85: val_loss improved from 4.42955 to 4.41487, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7973 - mse: 2.4442 - val_loss: 4.4149 - val_mse: 0.0822\n","Epoch 86/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6983 - mse: 2.3709 \n","Epoch 86: val_loss improved from 4.41487 to 4.39185, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7096 - mse: 2.3837 - val_loss: 4.3918 - val_mse: 0.0865\n","Epoch 87/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7055 - mse: 2.4056 \n","Epoch 87: val_loss improved from 4.39185 to 4.36133, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6738 - mse: 2.3752 - val_loss: 4.3613 - val_mse: 0.0835\n","Epoch 88/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9846 - mse: 2.7125 \n","Epoch 88: val_loss improved from 4.36133 to 4.31894, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.9716 - mse: 2.7008 - val_loss: 4.3189 - val_mse: 0.0692\n","Epoch 89/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0482 - mse: 2.8044 \n","Epoch 89: val_loss improved from 4.31894 to 4.29131, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0360 - mse: 2.7932 - val_loss: 4.2913 - val_mse: 0.0696\n","Epoch 90/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2493 - mse: 2.0336 \n","Epoch 90: val_loss improved from 4.29131 to 4.25946, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2908 - mse: 2.0761 - val_loss: 4.2595 - val_mse: 0.0661\n","Epoch 91/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6885 - mse: 2.5009 \n","Epoch 91: val_loss improved from 4.25946 to 4.24171, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6550 - mse: 2.4687 - val_loss: 4.2417 - val_mse: 0.0768\n","Epoch 92/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4826 - mse: 2.3238 \n","Epoch 92: val_loss improved from 4.24171 to 4.19684, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4931 - mse: 2.3353 - val_loss: 4.1968 - val_mse: 0.0608\n","Epoch 93/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4078 - mse: 2.2775 \n","Epoch 93: val_loss improved from 4.19684 to 4.16796, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4152 - mse: 2.2862 - val_loss: 4.1680 - val_mse: 0.0605\n","Epoch 94/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3124 - mse: 2.2102 \n","Epoch 94: val_loss improved from 4.16796 to 4.15124, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3616 - mse: 2.2613 - val_loss: 4.1512 - val_mse: 0.0729\n","Epoch 95/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4900 - mse: 2.4174 \n","Epoch 95: val_loss did not improve from 4.15124\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.5026 - mse: 2.4316 - val_loss: 4.1545 - val_mse: 0.1055\n","Epoch 96/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1258 - mse: 2.0831 \n","Epoch 96: val_loss improved from 4.15124 to 4.09692, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1309 - mse: 2.0893 - val_loss: 4.0969 - val_mse: 0.0775\n","Epoch 97/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2174 - mse: 2.2043 \n","Epoch 97: val_loss improved from 4.09692 to 4.05250, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2023 - mse: 2.1903 - val_loss: 4.0525 - val_mse: 0.0628\n","Epoch 98/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4661 - mse: 2.4824 \n","Epoch 98: val_loss improved from 4.05250 to 4.02983, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4710 - mse: 2.4887 - val_loss: 4.0298 - val_mse: 0.0699\n","Epoch 99/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0919 - mse: 2.1380 \n","Epoch 99: val_loss improved from 4.02983 to 4.01560, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0889 - mse: 2.1365 - val_loss: 4.0156 - val_mse: 0.0858\n","Epoch 100/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1488 - mse: 2.2254 \n","Epoch 100: val_loss improved from 4.01560 to 3.98453, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1549 - mse: 2.2326 - val_loss: 3.9845 - val_mse: 0.0847\n","Epoch 101/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0393 - mse: 2.1459 \n","Epoch 101: val_loss improved from 3.98453 to 3.94999, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0526 - mse: 2.1603 - val_loss: 3.9500 - val_mse: 0.0805\n","Epoch 102/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9249 - mse: 2.0618 \n","Epoch 102: val_loss improved from 3.94999 to 3.90826, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9356 - mse: 2.0736 - val_loss: 3.9083 - val_mse: 0.0695\n","Epoch 103/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1600 - mse: 2.3271 \n","Epoch 103: val_loss improved from 3.90826 to 3.88143, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1135 - mse: 2.2823 - val_loss: 3.8814 - val_mse: 0.0734\n","Epoch 104/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8993 - mse: 2.0978 \n","Epoch 104: val_loss improved from 3.88143 to 3.85164, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9083 - mse: 2.1080 - val_loss: 3.8516 - val_mse: 0.0746\n","Epoch 105/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9927 - mse: 2.2222 \n","Epoch 105: val_loss improved from 3.85164 to 3.83432, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9826 - mse: 2.2132 - val_loss: 3.8343 - val_mse: 0.0882\n","Epoch 106/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9680 - mse: 2.2284 \n","Epoch 106: val_loss improved from 3.83432 to 3.80756, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9722 - mse: 2.2338 - val_loss: 3.8076 - val_mse: 0.0925\n","Epoch 107/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8739 - mse: 2.1655 \n","Epoch 107: val_loss improved from 3.80756 to 3.77481, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8618 - mse: 2.1546 - val_loss: 3.7748 - val_mse: 0.0911\n","Epoch 108/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6203 - mse: 1.9432 \n","Epoch 108: val_loss improved from 3.77481 to 3.72346, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6303 - mse: 1.9543 - val_loss: 3.7235 - val_mse: 0.0712\n","Epoch 109/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6123 - mse: 1.9667 \n","Epoch 109: val_loss improved from 3.72346 to 3.71074, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6270 - mse: 1.9827 - val_loss: 3.7107 - val_mse: 0.0902\n","Epoch 110/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6943 - mse: 2.0808 \n","Epoch 110: val_loss improved from 3.71074 to 3.67495, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6958 - mse: 2.0831 - val_loss: 3.6750 - val_mse: 0.0861\n","Epoch 111/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8064 - mse: 2.2236 \n","Epoch 111: val_loss improved from 3.67495 to 3.63337, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7771 - mse: 2.1961 - val_loss: 3.6334 - val_mse: 0.0761\n","Epoch 112/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6289 - mse: 2.0781 \n","Epoch 112: val_loss improved from 3.63337 to 3.59225, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6316 - mse: 2.0822 - val_loss: 3.5922 - val_mse: 0.0668\n","Epoch 113/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4195 - mse: 1.9008 \n","Epoch 113: val_loss improved from 3.59225 to 3.58073, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4324 - mse: 1.9148 - val_loss: 3.5807 - val_mse: 0.0872\n","Epoch 114/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4898 - mse: 2.0027 \n","Epoch 114: val_loss improved from 3.58073 to 3.54599, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5002 - mse: 2.0146 - val_loss: 3.5460 - val_mse: 0.0846\n","Epoch 115/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6954 - mse: 2.2408  \n","Epoch 115: val_loss improved from 3.54599 to 3.50876, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.6708 - mse: 2.2173 - val_loss: 3.5088 - val_mse: 0.0795\n","Epoch 116/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4382 - mse: 2.0158 \n","Epoch 116: val_loss improved from 3.50876 to 3.46594, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4168 - mse: 1.9956 - val_loss: 3.4659 - val_mse: 0.0689\n","Epoch 117/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4691 - mse: 2.0792 \n","Epoch 117: val_loss improved from 3.46594 to 3.42413, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4799 - mse: 2.0909 - val_loss: 3.4241 - val_mse: 0.0593\n","Epoch 118/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1762 - mse: 1.8183 \n","Epoch 118: val_loss did not improve from 3.42413\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.1915 - mse: 1.8347 - val_loss: 3.4256 - val_mse: 0.0932\n","Epoch 119/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4253 - mse: 2.0995 \n","Epoch 119: val_loss improved from 3.42413 to 3.36377, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4134 - mse: 2.0891 - val_loss: 3.3638 - val_mse: 0.0640\n","Epoch 120/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1589 - mse: 1.8657 \n","Epoch 120: val_loss improved from 3.36377 to 3.33412, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1693 - mse: 1.8776 - val_loss: 3.3341 - val_mse: 0.0670\n","Epoch 121/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4505 - mse: 2.1903 \n","Epoch 121: val_loss improved from 3.33412 to 3.29155, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4408 - mse: 2.1818 - val_loss: 3.2915 - val_mse: 0.0570\n","Epoch 122/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9997 - mse: 1.7720 \n","Epoch 122: val_loss improved from 3.29155 to 3.25392, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0025 - mse: 1.7761 - val_loss: 3.2539 - val_mse: 0.0520\n","Epoch 123/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3639 - mse: 2.1692 \n","Epoch 123: val_loss improved from 3.25392 to 3.22959, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3523 - mse: 2.1584 - val_loss: 3.2296 - val_mse: 0.0603\n","Epoch 124/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2692 - mse: 2.1066 \n","Epoch 124: val_loss improved from 3.22959 to 3.19615, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2457 - mse: 2.0846 - val_loss: 3.1961 - val_mse: 0.0596\n","Epoch 125/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9964 - mse: 1.8667 \n","Epoch 125: val_loss improved from 3.19615 to 3.18436, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0062 - mse: 1.8778 - val_loss: 3.1844 - val_mse: 0.0806\n","Epoch 126/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1372 - mse: 2.0407 \n","Epoch 126: val_loss improved from 3.18436 to 3.12286, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1382 - mse: 2.0426 - val_loss: 3.1229 - val_mse: 0.0520\n","Epoch 127/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0302 - mse: 1.9662 \n","Epoch 127: val_loss improved from 3.12286 to 3.08577, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0283 - mse: 1.9656 - val_loss: 3.0858 - val_mse: 0.0478\n","Epoch 128/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7752 - mse: 1.7446 \n","Epoch 128: val_loss improved from 3.08577 to 3.05941, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7920 - mse: 1.7624 - val_loss: 3.0594 - val_mse: 0.0547\n","Epoch 129/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9757 - mse: 1.9780 \n","Epoch 129: val_loss improved from 3.05941 to 3.04075, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9790 - mse: 1.9825 - val_loss: 3.0407 - val_mse: 0.0692\n","Epoch 130/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9113 - mse: 1.9467 \n","Epoch 130: val_loss improved from 3.04075 to 2.99245, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9143 - mse: 1.9510 - val_loss: 2.9925 - val_mse: 0.0540\n","Epoch 131/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5988 - mse: 1.6670 \n","Epoch 131: val_loss improved from 2.99245 to 2.97303, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6117 - mse: 1.6815 - val_loss: 2.9730 - val_mse: 0.0677\n","Epoch 132/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7763 - mse: 1.8771 \n","Epoch 132: val_loss improved from 2.97303 to 2.93035, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7653 - mse: 1.8682 - val_loss: 2.9304 - val_mse: 0.0581\n","Epoch 133/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7399 - mse: 1.8740 \n","Epoch 133: val_loss improved from 2.93035 to 2.89389, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7485 - mse: 1.8844 - val_loss: 2.8939 - val_mse: 0.0543\n","Epoch 134/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8890 - mse: 2.0563 \n","Epoch 134: val_loss improved from 2.89389 to 2.85221, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8769 - mse: 2.0454 - val_loss: 2.8522 - val_mse: 0.0454\n","Epoch 135/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4738 - mse: 1.6740 \n","Epoch 135: val_loss improved from 2.85221 to 2.83544, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4865 - mse: 1.6879 - val_loss: 2.8354 - val_mse: 0.0617\n","Epoch 136/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5908 - mse: 1.8240 \n","Epoch 136: val_loss improved from 2.83544 to 2.80438, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6033 - mse: 1.8378 - val_loss: 2.8044 - val_mse: 0.0635\n","Epoch 137/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9140 - mse: 2.1801 \n","Epoch 137: val_loss improved from 2.80438 to 2.75620, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8807 - mse: 2.1480 - val_loss: 2.7562 - val_mse: 0.0482\n","Epoch 138/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4498 - mse: 1.7488 \n","Epoch 138: val_loss improved from 2.75620 to 2.72883, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4531 - mse: 1.7534 - val_loss: 2.7288 - val_mse: 0.0538\n","Epoch 139/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5617 - mse: 1.8937 \n","Epoch 139: val_loss improved from 2.72883 to 2.69617, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5752 - mse: 1.9084 - val_loss: 2.6962 - val_mse: 0.0542\n","Epoch 140/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5738 - mse: 1.9388 \n","Epoch 140: val_loss improved from 2.69617 to 2.67630, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5784 - mse: 1.9446 - val_loss: 2.6763 - val_mse: 0.0672\n","Epoch 141/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6674 - mse: 2.0650 \n","Epoch 141: val_loss improved from 2.67630 to 2.63349, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6787 - mse: 2.0778 - val_loss: 2.6335 - val_mse: 0.0573\n","Epoch 142/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4335 - mse: 1.8640 \n","Epoch 142: val_loss improved from 2.63349 to 2.59318, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4216 - mse: 1.8536 - val_loss: 2.5932 - val_mse: 0.0500\n","Epoch 143/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3300 - mse: 1.7940 \n","Epoch 143: val_loss improved from 2.59318 to 2.57117, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3179 - mse: 1.7829 - val_loss: 2.5712 - val_mse: 0.0607\n","Epoch 144/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3182 - mse: 1.8147 \n","Epoch 144: val_loss improved from 2.57117 to 2.55518, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3176 - mse: 1.8154 - val_loss: 2.5552 - val_mse: 0.0777\n","Epoch 145/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3476 - mse: 1.8772 \n","Epoch 145: val_loss improved from 2.55518 to 2.50816, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3660 - mse: 1.8968 - val_loss: 2.5082 - val_mse: 0.0635\n","Epoch 146/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2893 - mse: 1.8516 \n","Epoch 146: val_loss improved from 2.50816 to 2.47723, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2781 - mse: 1.8415 - val_loss: 2.4772 - val_mse: 0.0654\n","Epoch 147/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1601 - mse: 1.7555 \n","Epoch 147: val_loss improved from 2.47723 to 2.44912, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1716 - mse: 1.7679 - val_loss: 2.4491 - val_mse: 0.0699\n","Epoch 148/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1635 - mse: 1.7914 \n","Epoch 148: val_loss improved from 2.44912 to 2.39654, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1704 - mse: 1.7993 - val_loss: 2.3965 - val_mse: 0.0499\n","Epoch 149/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1899 - mse: 1.8500 \n","Epoch 149: val_loss improved from 2.39654 to 2.38165, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1889 - mse: 1.8503 - val_loss: 2.3817 - val_mse: 0.0674\n","Epoch 150/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0207 - mse: 1.7129 \n","Epoch 150: val_loss improved from 2.38165 to 2.34737, saving model to best_model_Adam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0137 - mse: 1.7074 - val_loss: 2.3474 - val_mse: 0.0655\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n","Optimizer: Adam, Epochs: 150, Test MSE: 0.08118956415375488\n","Training with optimizer: SGD, epochs: 150\n","Epoch 1/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 83.6799 - mse: 77.9403\n","Epoch 1: val_loss improved from inf to 6.51946, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 295ms/step - loss: 82.2457 - mse: 76.5026 - val_loss: 6.5195 - val_mse: 0.5442\n","Epoch 2/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.1962 - mse: 6.2137 \n","Epoch 2: val_loss did not improve from 6.51946\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.0976 - mse: 6.1149 - val_loss: 6.5987 - val_mse: 0.6186\n","Epoch 3/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.5221 - mse: 4.5481 \n","Epoch 3: val_loss improved from 6.51946 to 6.33304, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.5147 - mse: 4.5409 - val_loss: 6.3330 - val_mse: 0.3808\n","Epoch 4/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3321 - mse: 3.3907 \n","Epoch 4: val_loss improved from 6.33304 to 6.25130, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.3320 - mse: 3.3909 - val_loss: 6.2513 - val_mse: 0.3439\n","Epoch 5/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0552 - mse: 3.1570 \n","Epoch 5: val_loss improved from 6.25130 to 6.05004, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.0154 - mse: 3.1204 - val_loss: 6.0500 - val_mse: 0.1918\n","Epoch 6/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7096 - mse: 2.8628  \n","Epoch 6: val_loss improved from 6.05004 to 5.94264, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6948 - mse: 2.8494 - val_loss: 5.9426 - val_mse: 0.1358\n","Epoch 7/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1731 - mse: 2.3782 \n","Epoch 7: val_loss improved from 5.94264 to 5.92912, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1751 - mse: 2.3817 - val_loss: 5.9291 - val_mse: 0.1766\n","Epoch 8/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1402 - mse: 2.3982 \n","Epoch 8: val_loss improved from 5.92912 to 5.78701, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1103 - mse: 2.3715 - val_loss: 5.7870 - val_mse: 0.0893\n","Epoch 9/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9437 - mse: 2.2577 \n","Epoch 9: val_loss did not improve from 5.78701\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.9360 - mse: 2.2520 - val_loss: 5.9239 - val_mse: 0.2812\n","Epoch 10/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7973 - mse: 2.1658 \n","Epoch 10: val_loss improved from 5.78701 to 5.72042, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7784 - mse: 2.1494 - val_loss: 5.7204 - val_mse: 0.1328\n","Epoch 11/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7491 - mse: 2.1736 \n","Epoch 11: val_loss improved from 5.72042 to 5.69648, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7211 - mse: 2.1472 - val_loss: 5.6965 - val_mse: 0.1642\n","Epoch 12/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2358 - mse: 1.7168 \n","Epoch 12: val_loss improved from 5.69648 to 5.54613, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.2391 - mse: 1.7205 - val_loss: 5.5461 - val_mse: 0.0684\n","Epoch 13/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1478 - mse: 1.6831 \n","Epoch 13: val_loss improved from 5.54613 to 5.49570, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1504 - mse: 1.6862 - val_loss: 5.4957 - val_mse: 0.0726\n","Epoch 14/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1628 - mse: 1.7527 \n","Epoch 14: val_loss improved from 5.49570 to 5.44729, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1614 - mse: 1.7519 - val_loss: 5.4473 - val_mse: 0.0782\n","Epoch 15/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9027 - mse: 1.5466 \n","Epoch 15: val_loss improved from 5.44729 to 5.37837, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9004 - mse: 1.5447 - val_loss: 5.3784 - val_mse: 0.0632\n","Epoch 16/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9521 - mse: 1.6499 \n","Epoch 16: val_loss improved from 5.37837 to 5.34839, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9448 - mse: 1.6430 - val_loss: 5.3484 - val_mse: 0.0869\n","Epoch 17/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6762 - mse: 1.4276 \n","Epoch 17: val_loss did not improve from 5.34839\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6798 - mse: 1.4316 - val_loss: 5.3533 - val_mse: 0.1449\n","Epoch 18/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9015 - mse: 1.7052 \n","Epoch 18: val_loss did not improve from 5.34839\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.8913 - mse: 1.6959 - val_loss: 5.4924 - val_mse: 0.3365\n","Epoch 19/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6231 - mse: 1.4793 \n","Epoch 19: val_loss improved from 5.34839 to 5.16953, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6220 - mse: 1.4792 - val_loss: 5.1695 - val_mse: 0.0658\n","Epoch 20/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6244 - mse: 1.5321 \n","Epoch 20: val_loss improved from 5.16953 to 5.11978, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6070 - mse: 1.5161 - val_loss: 5.1198 - val_mse: 0.0678\n","Epoch 21/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4701 - mse: 1.4305 \n","Epoch 21: val_loss improved from 5.11978 to 5.05601, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4695 - mse: 1.4303 - val_loss: 5.0560 - val_mse: 0.0553\n","Epoch 22/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2533 - mse: 1.2648 \n","Epoch 22: val_loss improved from 5.05601 to 5.01069, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2527 - mse: 1.2646 - val_loss: 5.0107 - val_mse: 0.0610\n","Epoch 23/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1670 - mse: 1.2290 \n","Epoch 23: val_loss improved from 5.01069 to 4.95388, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.1624 - mse: 1.2252 - val_loss: 4.9539 - val_mse: 0.0547\n","Epoch 24/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0909 - mse: 1.2034 \n","Epoch 24: val_loss improved from 4.95388 to 4.90356, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0910 - mse: 1.2043 - val_loss: 4.9036 - val_mse: 0.0544\n","Epoch 25/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0307 - mse: 1.1935 \n","Epoch 25: val_loss improved from 4.90356 to 4.86404, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0306 - mse: 1.1938 - val_loss: 4.8640 - val_mse: 0.0644\n","Epoch 26/150\n","\u001b[1m 1/26\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 6.0919 - mse: 1.2923\n","Epoch 26: val_loss improved from 4.86404 to 4.81196, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9112 - mse: 1.1239 - val_loss: 4.8120 - val_mse: 0.0615\n","Epoch 27/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9304 - mse: 1.1916 \n","Epoch 27: val_loss improved from 4.81196 to 4.76316, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9287 - mse: 1.1904 - val_loss: 4.7632 - val_mse: 0.0612\n","Epoch 28/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8148 - mse: 1.1245 \n","Epoch 28: val_loss improved from 4.76316 to 4.69793, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8136 - mse: 1.1237 - val_loss: 4.6979 - val_mse: 0.0441\n","Epoch 29/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8274 - mse: 1.1837 \n","Epoch 29: val_loss improved from 4.69793 to 4.65305, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8193 - mse: 1.1773 - val_loss: 4.6531 - val_mse: 0.0469\n","Epoch 30/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7041 - mse: 1.1084 \n","Epoch 30: val_loss improved from 4.65305 to 4.60946, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7016 - mse: 1.1072 - val_loss: 4.6095 - val_mse: 0.0505\n","Epoch 31/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5182 - mse: 0.9692 \n","Epoch 31: val_loss improved from 4.60946 to 4.57921, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5202 - mse: 0.9730 - val_loss: 4.5792 - val_mse: 0.0670\n","Epoch 32/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5185 - mse: 1.0171 \n","Epoch 32: val_loss improved from 4.57921 to 4.51596, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5158 - mse: 1.0152 - val_loss: 4.5160 - val_mse: 0.0500\n","Epoch 33/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4450 - mse: 0.9902 \n","Epoch 33: val_loss improved from 4.51596 to 4.51189, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4418 - mse: 0.9873 - val_loss: 4.5119 - val_mse: 0.0918\n","Epoch 34/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3830 - mse: 0.9739 \n","Epoch 34: val_loss improved from 4.51189 to 4.42063, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3824 - mse: 0.9737 - val_loss: 4.4206 - val_mse: 0.0459\n","Epoch 35/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3797 - mse: 1.0154 \n","Epoch 35: val_loss improved from 4.42063 to 4.39833, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3762 - mse: 1.0126 - val_loss: 4.3983 - val_mse: 0.0685\n","Epoch 36/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2533 - mse: 0.9342 \n","Epoch 36: val_loss improved from 4.39833 to 4.34829, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2531 - mse: 0.9344 - val_loss: 4.3483 - val_mse: 0.0629\n","Epoch 37/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1697 - mse: 0.8949 \n","Epoch 37: val_loss improved from 4.34829 to 4.31824, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.1707 - mse: 0.8963 - val_loss: 4.3182 - val_mse: 0.0769\n","Epoch 38/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0988 - mse: 0.8679 \n","Epoch 38: val_loss improved from 4.31824 to 4.25031, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0973 - mse: 0.8668 - val_loss: 4.2503 - val_mse: 0.0525\n","Epoch 39/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0848 - mse: 0.8966 \n","Epoch 39: val_loss improved from 4.25031 to 4.21015, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0874 - mse: 0.9004 - val_loss: 4.2102 - val_mse: 0.0556\n","Epoch 40/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9147 - mse: 0.7700 \n","Epoch 40: val_loss improved from 4.21015 to 4.17041, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9170 - mse: 0.7731 - val_loss: 4.1704 - val_mse: 0.0586\n","Epoch 41/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8182 - mse: 0.7162 \n","Epoch 41: val_loss improved from 4.17041 to 4.11939, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8206 - mse: 0.7194 - val_loss: 4.1194 - val_mse: 0.0499\n","Epoch 42/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8529 - mse: 0.7935 \n","Epoch 42: val_loss improved from 4.11939 to 4.07742, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8521 - mse: 0.7931 - val_loss: 4.0774 - val_mse: 0.0498\n","Epoch 43/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8164 - mse: 0.7988 \n","Epoch 43: val_loss improved from 4.07742 to 4.03384, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8158 - mse: 0.7985 - val_loss: 4.0338 - val_mse: 0.0477\n","Epoch 44/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7056 - mse: 0.7294 \n","Epoch 44: val_loss improved from 4.03384 to 4.01111, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7057 - mse: 0.7298 - val_loss: 4.0111 - val_mse: 0.0660\n","Epoch 45/150\n","\u001b[1m 1/26\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.5011 - mse: 0.5561\n","Epoch 45: val_loss improved from 4.01111 to 3.94943, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6810 - mse: 0.7461 - val_loss: 3.9494 - val_mse: 0.0450\n","Epoch 46/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6238 - mse: 0.7291 \n","Epoch 46: val_loss improved from 3.94943 to 3.92836, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6223 - mse: 0.7280 - val_loss: 3.9284 - val_mse: 0.0642\n","Epoch 47/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6633 - mse: 0.8087 \n","Epoch 47: val_loss improved from 3.92836 to 3.92308, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6632 - mse: 0.8090 - val_loss: 3.9231 - val_mse: 0.0987\n","Epoch 48/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4280 - mse: 0.6131 \n","Epoch 48: val_loss improved from 3.92308 to 3.83796, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4284 - mse: 0.6138 - val_loss: 3.8380 - val_mse: 0.0530\n","Epoch 49/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4913 - mse: 0.7153 \n","Epoch 49: val_loss improved from 3.83796 to 3.81669, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4918 - mse: 0.7165 - val_loss: 3.8167 - val_mse: 0.0707\n","Epoch 50/150\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4154 - mse: 0.6754 \n","Epoch 50: val_loss improved from 3.81669 to 3.75447, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3869 - mse: 0.6505 - val_loss: 3.7545 - val_mse: 0.0471\n","Epoch 51/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3619 - mse: 0.6633 \n","Epoch 51: val_loss improved from 3.75447 to 3.72211, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3616 - mse: 0.6637 - val_loss: 3.7221 - val_mse: 0.0529\n","Epoch 52/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2555 - mse: 0.5954 \n","Epoch 52: val_loss improved from 3.72211 to 3.69056, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2554 - mse: 0.5957 - val_loss: 3.6906 - val_mse: 0.0592\n","Epoch 53/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1995 - mse: 0.5772 \n","Epoch 53: val_loss improved from 3.69056 to 3.63689, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2007 - mse: 0.5787 - val_loss: 3.6369 - val_mse: 0.0430\n","Epoch 54/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2860 - mse: 0.7010 \n","Epoch 54: val_loss improved from 3.63689 to 3.61330, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2841 - mse: 0.6995 - val_loss: 3.6133 - val_mse: 0.0564\n","Epoch 55/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1777 - mse: 0.6296 \n","Epoch 55: val_loss improved from 3.61330 to 3.57768, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1772 - mse: 0.6295 - val_loss: 3.5777 - val_mse: 0.0575\n","Epoch 56/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0614 - mse: 0.5497 \n","Epoch 56: val_loss improved from 3.57768 to 3.54372, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0634 - mse: 0.5523 - val_loss: 3.5437 - val_mse: 0.0599\n","Epoch 57/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0985 - mse: 0.6232 \n","Epoch 57: val_loss improved from 3.54372 to 3.49007, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0974 - mse: 0.6225 - val_loss: 3.4901 - val_mse: 0.0421\n","Epoch 58/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9732 - mse: 0.5338 \n","Epoch 58: val_loss improved from 3.49007 to 3.45952, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9739 - mse: 0.5348 - val_loss: 3.4595 - val_mse: 0.0471\n","Epoch 59/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0440 - mse: 0.6401 \n","Epoch 59: val_loss improved from 3.45952 to 3.42423, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0416 - mse: 0.6380 - val_loss: 3.4242 - val_mse: 0.0470\n","Epoch 60/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8527 - mse: 0.4839 \n","Epoch 60: val_loss improved from 3.42423 to 3.39067, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8540 - mse: 0.4855 - val_loss: 3.3907 - val_mse: 0.0483\n","Epoch 61/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8502 - mse: 0.5156 \n","Epoch 61: val_loss improved from 3.39067 to 3.34723, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8513 - mse: 0.5176 - val_loss: 3.3472 - val_mse: 0.0394\n","Epoch 62/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8658 - mse: 0.5658 \n","Epoch 62: val_loss improved from 3.34723 to 3.31273, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8628 - mse: 0.5634 - val_loss: 3.3127 - val_mse: 0.0390\n","Epoch 63/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7831 - mse: 0.5172 \n","Epoch 63: val_loss improved from 3.31273 to 3.31149, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7804 - mse: 0.5151 - val_loss: 3.3115 - val_mse: 0.0715\n","Epoch 64/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7378 - mse: 0.5059 \n","Epoch 64: val_loss improved from 3.31149 to 3.24364, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7387 - mse: 0.5071 - val_loss: 3.2436 - val_mse: 0.0371\n","Epoch 65/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7159 - mse: 0.5174 \n","Epoch 65: val_loss improved from 3.24364 to 3.22775, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7147 - mse: 0.5164 - val_loss: 3.2278 - val_mse: 0.0543\n","Epoch 66/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7266 - mse: 0.5607 \n","Epoch 66: val_loss did not improve from 3.22775\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7241 - mse: 0.5589 - val_loss: 3.2466 - val_mse: 0.1059\n","Epoch 67/150\n","\u001b[1m 1/26\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.6527 - mse: 0.5120\n","Epoch 67: val_loss improved from 3.22775 to 3.15549, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6587 - mse: 0.5261 - val_loss: 3.1555 - val_mse: 0.0472\n","Epoch 68/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5613 - mse: 0.4604 \n","Epoch 68: val_loss improved from 3.15549 to 3.11758, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5603 - mse: 0.4600 - val_loss: 3.1176 - val_mse: 0.0414\n","Epoch 69/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5882 - mse: 0.5196 \n","Epoch 69: val_loss improved from 3.11758 to 3.08881, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5861 - mse: 0.5178 - val_loss: 3.0888 - val_mse: 0.0444\n","Epoch 70/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5317 - mse: 0.4948 \n","Epoch 70: val_loss improved from 3.08881 to 3.05724, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5331 - mse: 0.4965 - val_loss: 3.0572 - val_mse: 0.0442\n","Epoch 71/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4390 - mse: 0.4335 \n","Epoch 71: val_loss improved from 3.05724 to 3.04078, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4386 - mse: 0.4334 - val_loss: 3.0408 - val_mse: 0.0589\n","Epoch 72/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4504 - mse: 0.4753 \n","Epoch 72: val_loss improved from 3.04078 to 3.00093, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4494 - mse: 0.4751 - val_loss: 3.0009 - val_mse: 0.0498\n","Epoch 73/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4126 - mse: 0.4682 \n","Epoch 73: val_loss improved from 3.00093 to 2.95895, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4137 - mse: 0.4701 - val_loss: 2.9590 - val_mse: 0.0383\n","Epoch 74/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3683 - mse: 0.4545 \n","Epoch 74: val_loss improved from 2.95895 to 2.92854, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3666 - mse: 0.4534 - val_loss: 2.9285 - val_mse: 0.0380\n","Epoch 75/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3282 - mse: 0.4448 \n","Epoch 75: val_loss improved from 2.92854 to 2.89641, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3282 - mse: 0.4450 - val_loss: 2.8964 - val_mse: 0.0357\n","Epoch 76/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2574 - mse: 0.4038 \n","Epoch 76: val_loss improved from 2.89641 to 2.86991, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2578 - mse: 0.4044 - val_loss: 2.8699 - val_mse: 0.0387\n","Epoch 77/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2608 - mse: 0.4364 \n","Epoch 77: val_loss improved from 2.86991 to 2.85285, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2589 - mse: 0.4350 - val_loss: 2.8528 - val_mse: 0.0509\n","Epoch 78/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2084 - mse: 0.4134 \n","Epoch 78: val_loss improved from 2.85285 to 2.81179, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2086 - mse: 0.4138 - val_loss: 2.8118 - val_mse: 0.0388\n","Epoch 79/150\n","\u001b[1m 1/26\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.3599 - mse: 0.5869\n","Epoch 79: val_loss improved from 2.81179 to 2.77894, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1729 - mse: 0.4070 - val_loss: 2.7789 - val_mse: 0.0345\n","Epoch 80/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1041 - mse: 0.3665 \n","Epoch 80: val_loss improved from 2.77894 to 2.75165, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1042 - mse: 0.3669 - val_loss: 2.7517 - val_mse: 0.0356\n","Epoch 81/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0497 - mse: 0.3404 \n","Epoch 81: val_loss improved from 2.75165 to 2.72486, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0505 - mse: 0.3414 - val_loss: 2.7249 - val_mse: 0.0368\n","Epoch 82/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1061 - mse: 0.4245 \n","Epoch 82: val_loss improved from 2.72486 to 2.70617, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1053 - mse: 0.4242 - val_loss: 2.7062 - val_mse: 0.0459\n","Epoch 83/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0502 - mse: 0.3961 \n","Epoch 83: val_loss improved from 2.70617 to 2.67187, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0478 - mse: 0.3944 - val_loss: 2.6719 - val_mse: 0.0391\n","Epoch 84/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0114 - mse: 0.3849 \n","Epoch 84: val_loss improved from 2.67187 to 2.64178, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0120 - mse: 0.3860 - val_loss: 2.6418 - val_mse: 0.0361\n","Epoch 85/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9502 - mse: 0.3511 \n","Epoch 85: val_loss improved from 2.64178 to 2.61652, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9494 - mse: 0.3505 - val_loss: 2.6165 - val_mse: 0.0378\n","Epoch 86/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9262 - mse: 0.3539 \n","Epoch 86: val_loss improved from 2.61652 to 2.58599, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9271 - mse: 0.3550 - val_loss: 2.5860 - val_mse: 0.0339\n","Epoch 87/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8683 - mse: 0.3226 \n","Epoch 87: val_loss improved from 2.58599 to 2.56317, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8682 - mse: 0.3227 - val_loss: 2.5632 - val_mse: 0.0374\n","Epoch 88/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8434 - mse: 0.3240 \n","Epoch 88: val_loss improved from 2.56317 to 2.53482, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8434 - mse: 0.3242 - val_loss: 2.5348 - val_mse: 0.0352\n","Epoch 89/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8366 - mse: 0.3431 \n","Epoch 89: val_loss improved from 2.53482 to 2.50964, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8371 - mse: 0.3439 - val_loss: 2.5096 - val_mse: 0.0358\n","Epoch 90/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7517 - mse: 0.2840 \n","Epoch 90: val_loss did not improve from 2.50964\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7527 - mse: 0.2853 - val_loss: 2.5262 - val_mse: 0.0779\n","Epoch 91/150\n","\u001b[1m 1/26\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.0653 - mse: 0.6170\n","Epoch 91: val_loss improved from 2.50964 to 2.46246, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8172 - mse: 0.3752 - val_loss: 2.4625 - val_mse: 0.0394\n","Epoch 92/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7561 - mse: 0.3388 \n","Epoch 92: val_loss improved from 2.46246 to 2.43432, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7558 - mse: 0.3390 - val_loss: 2.4343 - val_mse: 0.0363\n","Epoch 93/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7200 - mse: 0.3275 \n","Epoch 93: val_loss improved from 2.43432 to 2.40822, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7200 - mse: 0.3282 - val_loss: 2.4082 - val_mse: 0.0350\n","Epoch 94/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6715 - mse: 0.3037 \n","Epoch 94: val_loss improved from 2.40822 to 2.38502, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6743 - mse: 0.3072 - val_loss: 2.3850 - val_mse: 0.0363\n","Epoch 95/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6464 - mse: 0.3035 \n","Epoch 95: val_loss improved from 2.38502 to 2.35681, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6460 - mse: 0.3033 - val_loss: 2.3568 - val_mse: 0.0324\n","Epoch 96/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5863 - mse: 0.2676 \n","Epoch 96: val_loss improved from 2.35681 to 2.33504, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5863 - mse: 0.2678 - val_loss: 2.3350 - val_mse: 0.0346\n","Epoch 97/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5808 - mse: 0.2858 \n","Epoch 97: val_loss improved from 2.33504 to 2.31147, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5803 - mse: 0.2858 - val_loss: 2.3115 - val_mse: 0.0348\n","Epoch 98/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5681 - mse: 0.2971 \n","Epoch 98: val_loss improved from 2.31147 to 2.29057, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5684 - mse: 0.2975 - val_loss: 2.2906 - val_mse: 0.0374\n","Epoch 99/150\n","\u001b[1m 1/26\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.5618 - mse: 0.3087\n","Epoch 99: val_loss improved from 2.29057 to 2.26669, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5205 - mse: 0.2731 - val_loss: 2.2667 - val_mse: 0.0368\n","Epoch 100/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5059 - mse: 0.2816 \n","Epoch 100: val_loss improved from 2.26669 to 2.25740, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5057 - mse: 0.2816 - val_loss: 2.2574 - val_mse: 0.0506\n","Epoch 101/150\n","\u001b[1m 1/26\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.5503 - mse: 0.3435\n","Epoch 101: val_loss improved from 2.25740 to 2.22358, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5076 - mse: 0.3064 - val_loss: 2.2236 - val_mse: 0.0395\n","Epoch 102/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4452 - mse: 0.2666 \n","Epoch 102: val_loss improved from 2.22358 to 2.21474, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4455 - mse: 0.2671 - val_loss: 2.2147 - val_mse: 0.0533\n","Epoch 103/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4557 - mse: 0.2996 \n","Epoch 103: val_loss improved from 2.21474 to 2.17413, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4546 - mse: 0.2987 - val_loss: 2.1741 - val_mse: 0.0350\n","Epoch 104/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4199 - mse: 0.2856 \n","Epoch 104: val_loss did not improve from 2.17413\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4175 - mse: 0.2838 - val_loss: 2.1755 - val_mse: 0.0585\n","Epoch 105/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3967 - mse: 0.2846 \n","Epoch 105: val_loss improved from 2.17413 to 2.12784, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3947 - mse: 0.2831 - val_loss: 2.1278 - val_mse: 0.0327\n","Epoch 106/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3701 - mse: 0.2800 \n","Epoch 106: val_loss improved from 2.12784 to 2.11017, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3704 - mse: 0.2806 - val_loss: 2.1102 - val_mse: 0.0366\n","Epoch 107/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3316 - mse: 0.2633 \n","Epoch 107: val_loss improved from 2.11017 to 2.09868, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3313 - mse: 0.2631 - val_loss: 2.0987 - val_mse: 0.0466\n","Epoch 108/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2944 - mse: 0.2474 \n","Epoch 108: val_loss improved from 2.09868 to 2.06435, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2939 - mse: 0.2471 - val_loss: 2.0643 - val_mse: 0.0334\n","Epoch 109/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2781 - mse: 0.2522 \n","Epoch 109: val_loss improved from 2.06435 to 2.04655, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2778 - mse: 0.2522 - val_loss: 2.0466 - val_mse: 0.0366\n","Epoch 110/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2560 - mse: 0.2511 \n","Epoch 110: val_loss improved from 2.04655 to 2.02135, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2561 - mse: 0.2513 - val_loss: 2.0214 - val_mse: 0.0322\n","Epoch 111/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2336 - mse: 0.2494 \n","Epoch 111: val_loss improved from 2.02135 to 2.00198, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2330 - mse: 0.2490 - val_loss: 2.0020 - val_mse: 0.0334\n","Epoch 112/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1938 - mse: 0.2301 \n","Epoch 112: val_loss improved from 2.00198 to 1.99198, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1940 - mse: 0.2304 - val_loss: 1.9920 - val_mse: 0.0437\n","Epoch 113/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1676 - mse: 0.2242 \n","Epoch 113: val_loss improved from 1.99198 to 1.96263, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1681 - mse: 0.2249 - val_loss: 1.9626 - val_mse: 0.0345\n","Epoch 114/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1507 - mse: 0.2274 \n","Epoch 114: val_loss improved from 1.96263 to 1.94113, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1506 - mse: 0.2274 - val_loss: 1.9411 - val_mse: 0.0329\n","Epoch 115/150\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1159 - mse: 0.2107 \n","Epoch 115: val_loss improved from 1.94113 to 1.92167, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1141 - mse: 0.2108 - val_loss: 1.9217 - val_mse: 0.0332\n","Epoch 116/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1263 - mse: 0.2423 \n","Epoch 116: val_loss did not improve from 1.92167\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1252 - mse: 0.2415 - val_loss: 1.9241 - val_mse: 0.0551\n","Epoch 117/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0971 - mse: 0.2322 \n","Epoch 117: val_loss improved from 1.92167 to 1.88427, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0953 - mse: 0.2312 - val_loss: 1.8843 - val_mse: 0.0346\n","Epoch 118/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0332 - mse: 0.1882 \n","Epoch 118: val_loss improved from 1.88427 to 1.87025, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0340 - mse: 0.1891 - val_loss: 1.8702 - val_mse: 0.0397\n","Epoch 119/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0365 - mse: 0.2103 \n","Epoch 119: val_loss improved from 1.87025 to 1.84945, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0368 - mse: 0.2110 - val_loss: 1.8495 - val_mse: 0.0378\n","Epoch 120/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9950 - mse: 0.1878 \n","Epoch 120: val_loss improved from 1.84945 to 1.84574, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9950 - mse: 0.1880 - val_loss: 1.8457 - val_mse: 0.0528\n","Epoch 121/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0406 - mse: 0.2521 \n","Epoch 121: val_loss improved from 1.84574 to 1.80971, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0399 - mse: 0.2516 - val_loss: 1.8097 - val_mse: 0.0353\n","Epoch 122/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9553 - mse: 0.1853 \n","Epoch 122: val_loss improved from 1.80971 to 1.78592, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9553 - mse: 0.1855 - val_loss: 1.7859 - val_mse: 0.0298\n","Epoch 123/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9721 - mse: 0.2204 \n","Epoch 123: val_loss improved from 1.78592 to 1.78060, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9717 - mse: 0.2201 - val_loss: 1.7806 - val_mse: 0.0427\n","Epoch 124/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9475 - mse: 0.2136 \n","Epoch 124: val_loss improved from 1.78060 to 1.75294, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9465 - mse: 0.2130 - val_loss: 1.7529 - val_mse: 0.0330\n","Epoch 125/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9007 - mse: 0.1846 \n","Epoch 125: val_loss improved from 1.75294 to 1.73911, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9016 - mse: 0.1861 - val_loss: 1.7391 - val_mse: 0.0369\n","Epoch 126/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8986 - mse: 0.2003 \n","Epoch 126: val_loss improved from 1.73911 to 1.72383, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8985 - mse: 0.2006 - val_loss: 1.7238 - val_mse: 0.0392\n","Epoch 127/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8923 - mse: 0.2117 \n","Epoch 127: val_loss improved from 1.72383 to 1.70492, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8919 - mse: 0.2116 - val_loss: 1.7049 - val_mse: 0.0377\n","Epoch 128/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8296 - mse: 0.1664 \n","Epoch 128: val_loss improved from 1.70492 to 1.68374, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8306 - mse: 0.1677 - val_loss: 1.6837 - val_mse: 0.0338\n","Epoch 129/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8354 - mse: 0.1896 \n","Epoch 129: val_loss did not improve from 1.68374\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8353 - mse: 0.1896 - val_loss: 1.6875 - val_mse: 0.0546\n","Epoch 130/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8133 - mse: 0.1844 \n","Epoch 130: val_loss improved from 1.68374 to 1.64760, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8132 - mse: 0.1845 - val_loss: 1.6476 - val_mse: 0.0315\n","Epoch 131/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7839 - mse: 0.1719 \n","Epoch 131: val_loss improved from 1.64760 to 1.63153, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7840 - mse: 0.1721 - val_loss: 1.6315 - val_mse: 0.0322\n","Epoch 132/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7569 - mse: 0.1615 \n","Epoch 132: val_loss improved from 1.63153 to 1.62270, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7574 - mse: 0.1622 - val_loss: 1.6227 - val_mse: 0.0399\n","Epoch 133/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7404 - mse: 0.1615 \n","Epoch 133: val_loss improved from 1.62270 to 1.59946, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7404 - mse: 0.1617 - val_loss: 1.5995 - val_mse: 0.0330\n","Epoch 134/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7476 - mse: 0.1850 \n","Epoch 134: val_loss improved from 1.59946 to 1.58363, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7472 - mse: 0.1848 - val_loss: 1.5836 - val_mse: 0.0333\n","Epoch 135/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7089 - mse: 0.1622 \n","Epoch 135: val_loss improved from 1.58363 to 1.56450, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7083 - mse: 0.1620 - val_loss: 1.5645 - val_mse: 0.0302\n","Epoch 136/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6986 - mse: 0.1680 \n","Epoch 136: val_loss improved from 1.56450 to 1.54972, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6987 - mse: 0.1684 - val_loss: 1.5497 - val_mse: 0.0313\n","Epoch 137/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7060 - mse: 0.1911 \n","Epoch 137: val_loss improved from 1.54972 to 1.53724, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7035 - mse: 0.1890 - val_loss: 1.5372 - val_mse: 0.0345\n","Epoch 138/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6552 - mse: 0.1560 \n","Epoch 138: val_loss improved from 1.53724 to 1.51810, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6549 - mse: 0.1561 - val_loss: 1.5181 - val_mse: 0.0309\n","Epoch 139/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6505 - mse: 0.1669 \n","Epoch 139: val_loss improved from 1.51810 to 1.50332, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6498 - mse: 0.1665 - val_loss: 1.5033 - val_mse: 0.0315\n","Epoch 140/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6287 - mse: 0.1604 \n","Epoch 140: val_loss improved from 1.50332 to 1.49116, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6291 - mse: 0.1610 - val_loss: 1.4912 - val_mse: 0.0345\n","Epoch 141/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6373 - mse: 0.1843 \n","Epoch 141: val_loss improved from 1.49116 to 1.47399, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6372 - mse: 0.1844 - val_loss: 1.4740 - val_mse: 0.0324\n","Epoch 142/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5805 - mse: 0.1425 \n","Epoch 142: val_loss improved from 1.47399 to 1.46602, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5804 - mse: 0.1426 - val_loss: 1.4660 - val_mse: 0.0393\n","Epoch 143/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5832 - mse: 0.1601 \n","Epoch 143: val_loss improved from 1.46602 to 1.44510, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5828 - mse: 0.1598 - val_loss: 1.4451 - val_mse: 0.0332\n","Epoch 144/150\n","\u001b[1m 1/26\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5436 - mse: 0.1316\n","Epoch 144: val_loss improved from 1.44510 to 1.43571, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5599 - mse: 0.1516 - val_loss: 1.4357 - val_mse: 0.0384\n","Epoch 145/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5465 - mse: 0.1527 \n","Epoch 145: val_loss improved from 1.43571 to 1.41559, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5468 - mse: 0.1531 - val_loss: 1.4156 - val_mse: 0.0327\n","Epoch 146/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5308 - mse: 0.1513 \n","Epoch 146: val_loss improved from 1.41559 to 1.39937, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5306 - mse: 0.1513 - val_loss: 1.3994 - val_mse: 0.0308\n","Epoch 147/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5090 - mse: 0.1435 \n","Epoch 147: val_loss improved from 1.39937 to 1.39349, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5097 - mse: 0.1446 - val_loss: 1.3935 - val_mse: 0.0390\n","Epoch 148/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4909 - mse: 0.1396 \n","Epoch 148: val_loss improved from 1.39349 to 1.37096, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4906 - mse: 0.1396 - val_loss: 1.3710 - val_mse: 0.0305\n","Epoch 149/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4734 - mse: 0.1362 \n","Epoch 149: val_loss improved from 1.37096 to 1.35781, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4734 - mse: 0.1364 - val_loss: 1.3578 - val_mse: 0.0312\n","Epoch 150/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4634 - mse: 0.1401 \n","Epoch 150: val_loss improved from 1.35781 to 1.34451, saving model to best_model_SGD_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4631 - mse: 0.1400 - val_loss: 1.3445 - val_mse: 0.0316\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: SGD, Epochs: 150, Test MSE: 0.04140587899012006\n","Training with optimizer: RMSprop, epochs: 150\n","Epoch 1/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 150.7485 - mse: 145.1640\n","Epoch 1: val_loss improved from inf to 142.87920, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 298ms/step - loss: 150.7119 - mse: 145.1277 - val_loss: 142.8792 - val_mse: 137.3174\n","Epoch 2/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 145.3593 - mse: 139.8014 \n","Epoch 2: val_loss improved from 142.87920 to 137.55307, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 145.1432 - mse: 139.5860 - val_loss: 137.5531 - val_mse: 132.0095\n","Epoch 3/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.7374 - mse: 133.1972 \n","Epoch 3: val_loss improved from 137.55307 to 131.90288, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 138.7160 - mse: 133.1763 - val_loss: 131.9029 - val_mse: 126.3718\n","Epoch 4/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 134.4144 - mse: 128.8848 \n","Epoch 4: val_loss improved from 131.90288 to 126.26064, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 134.2324 - mse: 128.7030 - val_loss: 126.2606 - val_mse: 120.7366\n","Epoch 5/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 130.2746 - mse: 124.7515 \n","Epoch 5: val_loss improved from 126.26064 to 120.95538, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 129.8726 - mse: 124.3497 - val_loss: 120.9554 - val_mse: 115.4332\n","Epoch 6/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 122.8120 - mse: 117.2920 \n","Epoch 6: val_loss improved from 120.95538 to 114.99468, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 122.6778 - mse: 117.1585 - val_loss: 114.9947 - val_mse: 109.4807\n","Epoch 7/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112.8653 - mse: 107.3526 \n","Epoch 7: val_loss improved from 114.99468 to 107.25484, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 112.7152 - mse: 107.2027 - val_loss: 107.2548 - val_mse: 101.7483\n","Epoch 8/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 106.8257 - mse: 101.3205 \n","Epoch 8: val_loss improved from 107.25484 to 98.67563, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 106.6253 - mse: 101.1203 - val_loss: 98.6756 - val_mse: 93.1733\n","Epoch 9/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 102.4423 - mse: 96.9411  \n","Epoch 9: val_loss improved from 98.67563 to 89.85539, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 101.9776 - mse: 96.4766 - val_loss: 89.8554 - val_mse: 84.3593\n","Epoch 10/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90.5145 - mse: 85.0183 \n","Epoch 10: val_loss improved from 89.85539 to 80.43270, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 90.2706 - mse: 84.7746 - val_loss: 80.4327 - val_mse: 74.9384\n","Epoch 11/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83.0594 - mse: 77.5658 \n","Epoch 11: val_loss improved from 80.43270 to 71.45049, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 82.8148 - mse: 77.3213 - val_loss: 71.4505 - val_mse: 65.9581\n","Epoch 12/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5931 - mse: 68.1012 \n","Epoch 12: val_loss improved from 71.45049 to 64.34290, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 73.3303 - mse: 67.8385 - val_loss: 64.3429 - val_mse: 58.8536\n","Epoch 13/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.0655 - mse: 61.5762 \n","Epoch 13: val_loss improved from 64.34290 to 55.41885, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 66.5691 - mse: 61.0799 - val_loss: 55.4188 - val_mse: 49.9301\n","Epoch 14/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55.8261 - mse: 50.3390 \n","Epoch 14: val_loss improved from 55.41885 to 46.09888, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 55.5787 - mse: 50.0919 - val_loss: 46.0989 - val_mse: 40.6169\n","Epoch 15/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47.7898 - mse: 42.3096 \n","Epoch 15: val_loss improved from 46.09888 to 38.32965, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 47.4998 - mse: 42.0199 - val_loss: 38.3297 - val_mse: 32.8525\n","Epoch 16/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40.4886 - mse: 35.0120 \n","Epoch 16: val_loss improved from 38.32965 to 31.33006, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 40.2503 - mse: 34.7739 - val_loss: 31.3301 - val_mse: 25.8567\n","Epoch 17/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34.3748 - mse: 28.9038 \n","Epoch 17: val_loss improved from 31.33006 to 23.98330, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 34.0658 - mse: 28.5952 - val_loss: 23.9833 - val_mse: 18.5203\n","Epoch 18/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.0429 - mse: 21.5822 \n","Epoch 18: val_loss improved from 23.98330 to 17.20762, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.8888 - mse: 21.4284 - val_loss: 17.2076 - val_mse: 11.7552\n","Epoch 19/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.4135 - mse: 15.9639 \n","Epoch 19: val_loss improved from 17.20762 to 11.80914, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.3119 - mse: 15.8626 - val_loss: 11.8091 - val_mse: 6.3665\n","Epoch 20/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.7974 - mse: 12.3581 \n","Epoch 20: val_loss improved from 11.80914 to 9.01512, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.6767 - mse: 12.2378 - val_loss: 9.0151 - val_mse: 3.5878\n","Epoch 21/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.5784 - mse: 11.1545 \n","Epoch 21: val_loss improved from 9.01512 to 7.33739, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.2658 - mse: 10.8426 - val_loss: 7.3374 - val_mse: 1.9280\n","Epoch 22/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.8416 - mse: 9.4352  \n","Epoch 22: val_loss improved from 7.33739 to 6.45492, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.7023 - mse: 9.2966 - val_loss: 6.4549 - val_mse: 1.0633\n","Epoch 23/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.5679 - mse: 7.1801 \n","Epoch 23: val_loss improved from 6.45492 to 5.81348, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.4729 - mse: 7.0859 - val_loss: 5.8135 - val_mse: 0.4443\n","Epoch 24/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.7915 - mse: 6.4278 \n","Epoch 24: val_loss did not improve from 5.81348\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.7882 - mse: 6.4251 - val_loss: 5.8923 - val_mse: 0.5484\n","Epoch 25/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2023 - mse: 6.8643 \n","Epoch 25: val_loss improved from 5.81348 to 5.80205, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.1466 - mse: 6.8100 - val_loss: 5.8021 - val_mse: 0.4863\n","Epoch 26/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.5592 - mse: 6.2493 \n","Epoch 26: val_loss improved from 5.80205 to 5.63835, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.5465 - mse: 6.2380 - val_loss: 5.6384 - val_mse: 0.3516\n","Epoch 27/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.4666 - mse: 6.1862 \n","Epoch 27: val_loss improved from 5.63835 to 5.61434, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.4692 - mse: 6.1905 - val_loss: 5.6143 - val_mse: 0.3575\n","Epoch 28/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6778 - mse: 5.4288\n","Epoch 28: val_loss improved from 5.61434 to 5.46675, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6675 - mse: 5.4204 - val_loss: 5.4667 - val_mse: 0.2495\n","Epoch 29/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.0554 - mse: 5.8459 \n","Epoch 29: val_loss did not improve from 5.46675\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.9703 - mse: 5.7623 - val_loss: 5.5630 - val_mse: 0.3867\n","Epoch 30/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1661 - mse: 4.9995 \n","Epoch 30: val_loss improved from 5.46675 to 5.41746, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.1758 - mse: 5.0104 - val_loss: 5.4175 - val_mse: 0.2837\n","Epoch 31/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7161 - mse: 4.5922  \n","Epoch 31: val_loss improved from 5.41746 to 5.30310, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.7125 - mse: 4.5905 - val_loss: 5.3031 - val_mse: 0.2162\n","Epoch 32/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6686 - mse: 4.5917 \n","Epoch 32: val_loss did not improve from 5.30310\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.7258 - mse: 4.6507 - val_loss: 5.3727 - val_mse: 0.3340\n","Epoch 33/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2877 - mse: 4.2602 \n","Epoch 33: val_loss improved from 5.30310 to 5.27636, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.3125 - mse: 4.2869 - val_loss: 5.2764 - val_mse: 0.2880\n","Epoch 34/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2551 - mse: 4.2772  \n","Epoch 34: val_loss did not improve from 5.27636\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.2724 - mse: 4.2964 - val_loss: 5.3681 - val_mse: 0.4285\n","Epoch 35/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3170 - mse: 4.3887  \n","Epoch 35: val_loss improved from 5.27636 to 5.08601, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.2958 - mse: 4.3690 - val_loss: 5.0860 - val_mse: 0.1991\n","Epoch 36/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8982 - mse: 4.0235 \n","Epoch 36: val_loss improved from 5.08601 to 5.06004, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9747 - mse: 4.1026 - val_loss: 5.0600 - val_mse: 0.2314\n","Epoch 37/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0993 - mse: 4.2815  \n","Epoch 37: val_loss did not improve from 5.06004\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.0644 - mse: 4.2493 - val_loss: 5.0602 - val_mse: 0.2894\n","Epoch 38/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2091 - mse: 3.4539 \n","Epoch 38: val_loss improved from 5.06004 to 4.87917, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2343 - mse: 3.4811 - val_loss: 4.8792 - val_mse: 0.1778\n","Epoch 39/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9372 - mse: 4.2498 \n","Epoch 39: val_loss improved from 4.87917 to 4.75568, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9532 - mse: 4.2682 - val_loss: 4.7557 - val_mse: 0.1186\n","Epoch 40/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1628 - mse: 3.5418 \n","Epoch 40: val_loss did not improve from 4.75568\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1503 - mse: 3.5314 - val_loss: 4.7667 - val_mse: 0.2073\n","Epoch 41/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9081 - mse: 3.3665 \n","Epoch 41: val_loss improved from 4.75568 to 4.67162, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.9270 - mse: 3.3874 - val_loss: 4.6716 - val_mse: 0.1893\n","Epoch 42/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3673 - mse: 3.9013 \n","Epoch 42: val_loss improved from 4.67162 to 4.53108, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2929 - mse: 3.8299 - val_loss: 4.5311 - val_mse: 0.1303\n","Epoch 43/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7733 - mse: 3.3903 \n","Epoch 43: val_loss improved from 4.53108 to 4.47162, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7659 - mse: 3.3861 - val_loss: 4.4716 - val_mse: 0.1570\n","Epoch 44/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5315 - mse: 3.2372 \n","Epoch 44: val_loss did not improve from 4.47162\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.5453 - mse: 3.2535 - val_loss: 4.5246 - val_mse: 0.2983\n","Epoch 45/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3340 - mse: 3.1278 \n","Epoch 45: val_loss improved from 4.47162 to 4.30505, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3397 - mse: 3.1360 - val_loss: 4.3050 - val_mse: 0.1680\n","Epoch 46/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4802 - mse: 3.3610 \n","Epoch 46: val_loss improved from 4.30505 to 4.20577, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4545 - mse: 3.3405 - val_loss: 4.2058 - val_mse: 0.1605\n","Epoch 47/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1603 - mse: 3.1345 \n","Epoch 47: val_loss improved from 4.20577 to 4.12446, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1902 - mse: 3.1688 - val_loss: 4.1245 - val_mse: 0.1734\n","Epoch 48/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6952 - mse: 2.7666 \n","Epoch 48: val_loss improved from 4.12446 to 3.92867, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6835 - mse: 2.7577 - val_loss: 3.9287 - val_mse: 0.0835\n","Epoch 49/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0192 - mse: 3.1946 \n","Epoch 49: val_loss improved from 3.92867 to 3.85125, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0071 - mse: 3.1861 - val_loss: 3.8513 - val_mse: 0.1050\n","Epoch 50/150\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.3173 - mse: 2.5855 \n","Epoch 50: val_loss improved from 3.85125 to 3.79212, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3262 - mse: 2.6064 - val_loss: 3.7921 - val_mse: 0.1511\n","Epoch 51/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5888 - mse: 2.9684 \n","Epoch 51: val_loss improved from 3.79212 to 3.62531, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5730 - mse: 2.9564 - val_loss: 3.6253 - val_mse: 0.0846\n","Epoch 52/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2930 - mse: 2.7754 \n","Epoch 52: val_loss improved from 3.62531 to 3.57615, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2774 - mse: 2.7627 - val_loss: 3.5762 - val_mse: 0.1445\n","Epoch 53/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1313 - mse: 2.7239 \n","Epoch 53: val_loss improved from 3.57615 to 3.41183, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1246 - mse: 2.7202 - val_loss: 3.4118 - val_mse: 0.0913\n","Epoch 54/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3886 - mse: 3.0903 \n","Epoch 54: val_loss improved from 3.41183 to 3.31755, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3780 - mse: 3.0836 - val_loss: 3.3175 - val_mse: 0.0988\n","Epoch 55/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9713 - mse: 2.7727 \n","Epoch 55: val_loss improved from 3.31755 to 3.23863, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9565 - mse: 2.7629 - val_loss: 3.2386 - val_mse: 0.1259\n","Epoch 56/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9804 - mse: 2.8887 \n","Epoch 56: val_loss improved from 3.23863 to 3.16756, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9799 - mse: 2.8920 - val_loss: 3.1676 - val_mse: 0.1555\n","Epoch 57/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7877 - mse: 2.7952 \n","Epoch 57: val_loss improved from 3.16756 to 2.98752, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7742 - mse: 2.7861 - val_loss: 2.9875 - val_mse: 0.0679\n","Epoch 58/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3643 - mse: 2.4659 \n","Epoch 58: val_loss improved from 2.98752 to 2.87286, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3779 - mse: 2.4831 - val_loss: 2.8729 - val_mse: 0.0515\n","Epoch 59/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6842 - mse: 2.8829 \n","Epoch 59: val_loss improved from 2.87286 to 2.78871, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6373 - mse: 2.8396 - val_loss: 2.7887 - val_mse: 0.0643\n","Epoch 60/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8779 - mse: 2.1739 \n","Epoch 60: val_loss improved from 2.78871 to 2.68424, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9083 - mse: 2.2079 - val_loss: 2.6842 - val_mse: 0.0556\n","Epoch 61/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9777 - mse: 2.3674 \n","Epoch 61: val_loss improved from 2.68424 to 2.58758, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9747 - mse: 2.3677 - val_loss: 2.5876 - val_mse: 0.0483\n","Epoch 62/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2507 - mse: 2.7292 \n","Epoch 62: val_loss did not improve from 2.58758\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2014 - mse: 2.6831 - val_loss: 2.6042 - val_mse: 0.1501\n","Epoch 63/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1125 - mse: 2.6756 \n","Epoch 63: val_loss improved from 2.58758 to 2.45673, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0852 - mse: 2.6512 - val_loss: 2.4567 - val_mse: 0.0844\n","Epoch 64/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7543 - mse: 2.3991 \n","Epoch 64: val_loss improved from 2.45673 to 2.40559, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7627 - mse: 2.4106 - val_loss: 2.4056 - val_mse: 0.1151\n","Epoch 65/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9469 - mse: 2.6707 \n","Epoch 65: val_loss improved from 2.40559 to 2.26613, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8962 - mse: 2.6243 - val_loss: 2.2661 - val_mse: 0.0562\n","Epoch 66/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9502 - mse: 2.7570 \n","Epoch 66: val_loss improved from 2.26613 to 2.17915, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9183 - mse: 2.7282 - val_loss: 2.1791 - val_mse: 0.0511\n","Epoch 67/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4435 - mse: 2.3329 \n","Epoch 67: val_loss improved from 2.17915 to 2.14709, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4369 - mse: 2.3295 - val_loss: 2.1471 - val_mse: 0.1014\n","Epoch 68/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0410 - mse: 2.0138 \n","Epoch 68: val_loss improved from 2.14709 to 2.06590, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0542 - mse: 2.0302 - val_loss: 2.0659 - val_mse: 0.1025\n","Epoch 69/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1927 - mse: 2.2440 \n","Epoch 69: val_loss improved from 2.06590 to 1.96012, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1777 - mse: 2.2317 - val_loss: 1.9601 - val_mse: 0.0686\n","Epoch 70/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2302 - mse: 2.3549 \n","Epoch 70: val_loss improved from 1.96012 to 1.88208, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2481 - mse: 2.3748 - val_loss: 1.8821 - val_mse: 0.0595\n","Epoch 71/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1749 - mse: 2.3662 \n","Epoch 71: val_loss improved from 1.88208 to 1.80350, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1483 - mse: 2.3421 - val_loss: 1.8035 - val_mse: 0.0476\n","Epoch 72/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8156 - mse: 2.0748 \n","Epoch 72: val_loss improved from 1.80350 to 1.72745, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8149 - mse: 2.0768 - val_loss: 1.7275 - val_mse: 0.0424\n","Epoch 73/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7672 - mse: 2.0962 \n","Epoch 73: val_loss improved from 1.72745 to 1.69304, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7812 - mse: 2.1126 - val_loss: 1.6930 - val_mse: 0.0715\n","Epoch 74/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2801 - mse: 2.6691 \n","Epoch 74: val_loss improved from 1.69304 to 1.64333, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2055 - mse: 2.5978 - val_loss: 1.6433 - val_mse: 0.0801\n","Epoch 75/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7776 - mse: 2.2259 \n","Epoch 75: val_loss improved from 1.64333 to 1.56234, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7615 - mse: 2.2133 - val_loss: 1.5623 - val_mse: 0.0598\n","Epoch 76/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4838 - mse: 1.9937 \n","Epoch 76: val_loss improved from 1.56234 to 1.50892, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5003 - mse: 2.0122 - val_loss: 1.5089 - val_mse: 0.0608\n","Epoch 77/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8184 - mse: 2.3813 \n","Epoch 77: val_loss improved from 1.50892 to 1.45783, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7773 - mse: 2.3423 - val_loss: 1.4578 - val_mse: 0.0649\n","Epoch 78/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5686 - mse: 2.1880 \n","Epoch 78: val_loss improved from 1.45783 to 1.41796, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5811 - mse: 2.2026 - val_loss: 1.4180 - val_mse: 0.0800\n","Epoch 79/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6438 - mse: 2.3164 \n","Epoch 79: val_loss improved from 1.41796 to 1.33243, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6175 - mse: 2.2920 - val_loss: 1.3324 - val_mse: 0.0455\n","Epoch 80/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2673 - mse: 1.9919 \n","Epoch 80: val_loss improved from 1.33243 to 1.30834, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2747 - mse: 2.0006 - val_loss: 1.3083 - val_mse: 0.0700\n","Epoch 81/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0001 - mse: 1.7724 \n","Epoch 81: val_loss improved from 1.30834 to 1.26246, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0287 - mse: 1.8027 - val_loss: 1.2625 - val_mse: 0.0683\n","Epoch 82/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3880 - mse: 2.2024 \n","Epoch 82: val_loss improved from 1.26246 to 1.20215, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3876 - mse: 2.2031 - val_loss: 1.2022 - val_mse: 0.0467\n","Epoch 83/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9795 - mse: 1.8314 \n","Epoch 83: val_loss improved from 1.20215 to 1.17702, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0148 - mse: 1.8691 - val_loss: 1.1770 - val_mse: 0.0590\n","Epoch 84/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0771 - mse: 1.9676 \n","Epoch 84: val_loss improved from 1.17702 to 1.12693, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0885 - mse: 1.9811 - val_loss: 1.1269 - val_mse: 0.0509\n","Epoch 85/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0853 - mse: 2.0176 \n","Epoch 85: val_loss improved from 1.12693 to 1.11967, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0533 - mse: 1.9873 - val_loss: 1.1197 - val_mse: 0.0878\n","Epoch 86/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8863 - mse: 1.8637 \n","Epoch 86: val_loss improved from 1.11967 to 1.06178, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8956 - mse: 1.8742 - val_loss: 1.0618 - val_mse: 0.0696\n","Epoch 87/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7817 - mse: 1.7985 \n","Epoch 87: val_loss improved from 1.06178 to 1.02623, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7811 - mse: 1.7989 - val_loss: 1.0262 - val_mse: 0.0732\n","Epoch 88/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7256 - mse: 1.7813 \n","Epoch 88: val_loss improved from 1.02623 to 0.98107, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7375 - mse: 1.7942 - val_loss: 0.9811 - val_mse: 0.0639\n","Epoch 89/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9252 - mse: 2.0158 \n","Epoch 89: val_loss improved from 0.98107 to 0.94074, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9299 - mse: 2.0213 - val_loss: 0.9407 - val_mse: 0.0545\n","Epoch 90/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5671 - mse: 1.6879 \n","Epoch 90: val_loss improved from 0.94074 to 0.92607, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6011 - mse: 1.7231 - val_loss: 0.9261 - val_mse: 0.0692\n","Epoch 91/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0356 - mse: 2.1841 \n","Epoch 91: val_loss improved from 0.92607 to 0.88053, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9819 - mse: 2.1314 - val_loss: 0.8805 - val_mse: 0.0509\n","Epoch 92/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6263 - mse: 1.8027 \n","Epoch 92: val_loss improved from 0.88053 to 0.85117, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6179 - mse: 1.7959 - val_loss: 0.8512 - val_mse: 0.0496\n","Epoch 93/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7215 - mse: 1.9243 \n","Epoch 93: val_loss did not improve from 0.85117\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7013 - mse: 1.9055 - val_loss: 0.8568 - val_mse: 0.0799\n","Epoch 94/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5555 - mse: 1.7843 \n","Epoch 94: val_loss improved from 0.85117 to 0.81362, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5665 - mse: 1.7960 - val_loss: 0.8136 - val_mse: 0.0640\n","Epoch 95/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4365 - mse: 1.6927 \n","Epoch 95: val_loss improved from 0.81362 to 0.79751, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4482 - mse: 1.7058 - val_loss: 0.7975 - val_mse: 0.0761\n","Epoch 96/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3504 - mse: 1.6356 \n","Epoch 96: val_loss improved from 0.79751 to 0.76112, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3599 - mse: 1.6462 - val_loss: 0.7611 - val_mse: 0.0665\n","Epoch 97/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5941 - mse: 1.9047 \n","Epoch 97: val_loss improved from 0.76112 to 0.72237, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5801 - mse: 1.8915 - val_loss: 0.7224 - val_mse: 0.0503\n","Epoch 98/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5007 - mse: 1.8338 \n","Epoch 98: val_loss improved from 0.72237 to 0.71652, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4989 - mse: 1.8329 - val_loss: 0.7165 - val_mse: 0.0664\n","Epoch 99/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2863 - mse: 1.6416 \n","Epoch 99: val_loss improved from 0.71652 to 0.70477, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2967 - mse: 1.6529 - val_loss: 0.7048 - val_mse: 0.0791\n","Epoch 100/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3227 - mse: 1.7027 \n","Epoch 100: val_loss did not improve from 0.70477\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3192 - mse: 1.7001 - val_loss: 0.7221 - val_mse: 0.1200\n","Epoch 101/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3644 - mse: 1.7660 \n","Epoch 101: val_loss improved from 0.70477 to 0.66533, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3671 - mse: 1.7694 - val_loss: 0.6653 - val_mse: 0.0814\n","Epoch 102/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1996 - mse: 1.6201 \n","Epoch 102: val_loss improved from 0.66533 to 0.64056, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2122 - mse: 1.6336 - val_loss: 0.6406 - val_mse: 0.0753\n","Epoch 103/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1464 - mse: 1.5862 \n","Epoch 103: val_loss improved from 0.64056 to 0.62044, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1366 - mse: 1.5775 - val_loss: 0.6204 - val_mse: 0.0779\n","Epoch 104/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3139 - mse: 1.7744 \n","Epoch 104: val_loss did not improve from 0.62044\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3052 - mse: 1.7662 - val_loss: 0.6573 - val_mse: 0.1309\n","Epoch 105/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4099 - mse: 1.8871 \n","Epoch 105: val_loss improved from 0.62044 to 0.61243, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3982 - mse: 1.8758 - val_loss: 0.6124 - val_mse: 0.1018\n","Epoch 106/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2265 - mse: 1.7196 \n","Epoch 106: val_loss improved from 0.61243 to 0.56224, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2283 - mse: 1.7218 - val_loss: 0.5622 - val_mse: 0.0681\n","Epoch 107/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1584 - mse: 1.6680 \n","Epoch 107: val_loss improved from 0.56224 to 0.53427, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1506 - mse: 1.6606 - val_loss: 0.5343 - val_mse: 0.0551\n","Epoch 108/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0018 - mse: 1.5257 \n","Epoch 108: val_loss improved from 0.53427 to 0.52751, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0080 - mse: 1.5323 - val_loss: 0.5275 - val_mse: 0.0628\n","Epoch 109/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1112 - mse: 1.6509 \n","Epoch 109: val_loss improved from 0.52751 to 0.49823, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1224 - mse: 1.6628 - val_loss: 0.4982 - val_mse: 0.0514\n","Epoch 110/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9755 - mse: 1.5327 \n","Epoch 110: val_loss improved from 0.49823 to 0.46657, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9873 - mse: 1.5450 - val_loss: 0.4666 - val_mse: 0.0363\n","Epoch 111/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1461 - mse: 1.7190 \n","Epoch 111: val_loss did not improve from 0.46657\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1395 - mse: 1.7133 - val_loss: 0.4718 - val_mse: 0.0556\n","Epoch 112/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0329 - mse: 1.6197 \n","Epoch 112: val_loss improved from 0.46657 to 0.45530, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0365 - mse: 1.6240 - val_loss: 0.4553 - val_mse: 0.0541\n","Epoch 113/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8505 - mse: 1.4528 \n","Epoch 113: val_loss did not improve from 0.45530\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8693 - mse: 1.4721 - val_loss: 0.4650 - val_mse: 0.0769\n","Epoch 114/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9504 - mse: 1.5652 \n","Epoch 114: val_loss did not improve from 0.45530\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9519 - mse: 1.5670 - val_loss: 0.4602 - val_mse: 0.0820\n","Epoch 115/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9609 - mse: 1.5848 \n","Epoch 115: val_loss improved from 0.45530 to 0.45291, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9542 - mse: 1.5783 - val_loss: 0.4529 - val_mse: 0.0835\n","Epoch 116/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9655 - mse: 1.5988 \n","Epoch 116: val_loss improved from 0.45291 to 0.42456, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9548 - mse: 1.5884 - val_loss: 0.4246 - val_mse: 0.0647\n","Epoch 117/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9174 - mse: 1.5593 \n","Epoch 117: val_loss did not improve from 0.42456\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9328 - mse: 1.5749 - val_loss: 0.4409 - val_mse: 0.0868\n","Epoch 118/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7211 - mse: 1.3695 \n","Epoch 118: val_loss improved from 0.42456 to 0.40498, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7351 - mse: 1.3837 - val_loss: 0.4050 - val_mse: 0.0608\n","Epoch 119/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8070 - mse: 1.4650 \n","Epoch 119: val_loss improved from 0.40498 to 0.37132, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8153 - mse: 1.4736 - val_loss: 0.3713 - val_mse: 0.0369\n","Epoch 120/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9153 - mse: 1.5824 \n","Epoch 120: val_loss improved from 0.37132 to 0.37052, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9126 - mse: 1.5799 - val_loss: 0.3705 - val_mse: 0.0414\n","Epoch 121/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1414 - mse: 1.8134 \n","Epoch 121: val_loss did not improve from 0.37052\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1206 - mse: 1.7931 - val_loss: 0.4011 - val_mse: 0.0779\n","Epoch 122/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6773 - mse: 1.3556 \n","Epoch 122: val_loss improved from 0.37052 to 0.37034, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6842 - mse: 1.3629 - val_loss: 0.3703 - val_mse: 0.0549\n","Epoch 123/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9691 - mse: 1.6550 \n","Epoch 123: val_loss improved from 0.37034 to 0.35521, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9591 - mse: 1.6453 - val_loss: 0.3552 - val_mse: 0.0451\n","Epoch 124/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9640 - mse: 1.6543 \n","Epoch 124: val_loss did not improve from 0.35521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9487 - mse: 1.6391 - val_loss: 0.3831 - val_mse: 0.0776\n","Epoch 125/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7295 - mse: 1.4257 \n","Epoch 125: val_loss improved from 0.35521 to 0.34864, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7411 - mse: 1.4376 - val_loss: 0.3486 - val_mse: 0.0501\n","Epoch 126/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8435 - mse: 1.5466 \n","Epoch 126: val_loss improved from 0.34864 to 0.34790, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8492 - mse: 1.5525 - val_loss: 0.3479 - val_mse: 0.0571\n","Epoch 127/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9267 - mse: 1.6377 \n","Epoch 127: val_loss improved from 0.34790 to 0.33613, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9194 - mse: 1.6307 - val_loss: 0.3361 - val_mse: 0.0551\n","Epoch 128/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6300 - mse: 1.3514 \n","Epoch 128: val_loss improved from 0.33613 to 0.32582, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6425 - mse: 1.3643 - val_loss: 0.3258 - val_mse: 0.0546\n","Epoch 129/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7573 - mse: 1.4883 \n","Epoch 129: val_loss improved from 0.32582 to 0.31125, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7663 - mse: 1.4976 - val_loss: 0.3112 - val_mse: 0.0481\n","Epoch 130/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7493 - mse: 1.4878 \n","Epoch 130: val_loss did not improve from 0.31125\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7527 - mse: 1.4914 - val_loss: 0.3231 - val_mse: 0.0675\n","Epoch 131/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5677 - mse: 1.3145 \n","Epoch 131: val_loss improved from 0.31125 to 0.30018, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5687 - mse: 1.3158 - val_loss: 0.3002 - val_mse: 0.0530\n","Epoch 132/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6246 - mse: 1.3788 \n","Epoch 132: val_loss improved from 0.30018 to 0.28451, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6357 - mse: 1.3901 - val_loss: 0.2845 - val_mse: 0.0432\n","Epoch 133/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7054 - mse: 1.4652 \n","Epoch 133: val_loss improved from 0.28451 to 0.27753, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7178 - mse: 1.4779 - val_loss: 0.2775 - val_mse: 0.0408\n","Epoch 134/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8714 - mse: 1.6354 \n","Epoch 134: val_loss improved from 0.27753 to 0.26754, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8512 - mse: 1.6154 - val_loss: 0.2675 - val_mse: 0.0356\n","Epoch 135/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4848 - mse: 1.2549 \n","Epoch 135: val_loss did not improve from 0.26754\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4987 - mse: 1.2692 - val_loss: 0.3024 - val_mse: 0.0784\n","Epoch 136/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5728 - mse: 1.3507 \n","Epoch 136: val_loss did not improve from 0.26754\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5854 - mse: 1.3636 - val_loss: 0.2761 - val_mse: 0.0576\n","Epoch 137/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8676 - mse: 1.6491 \n","Epoch 137: val_loss improved from 0.26754 to 0.26632, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8513 - mse: 1.6328 - val_loss: 0.2663 - val_mse: 0.0514\n","Epoch 138/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5539 - mse: 1.3399 \n","Epoch 138: val_loss improved from 0.26632 to 0.24803, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5600 - mse: 1.3461 - val_loss: 0.2480 - val_mse: 0.0369\n","Epoch 139/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6245 - mse: 1.4150 \n","Epoch 139: val_loss did not improve from 0.24803\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6325 - mse: 1.4232 - val_loss: 0.2627 - val_mse: 0.0577\n","Epoch 140/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6357 - mse: 1.4323 \n","Epoch 140: val_loss did not improve from 0.24803\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6424 - mse: 1.4391 - val_loss: 0.2567 - val_mse: 0.0570\n","Epoch 141/150\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8141 - mse: 1.6143 \n","Epoch 141: val_loss did not improve from 0.24803\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8168 - mse: 1.6171 - val_loss: 0.2702 - val_mse: 0.0711\n","Epoch 142/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5942 - mse: 1.3966 \n","Epoch 142: val_loss improved from 0.24803 to 0.23835, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5854 - mse: 1.3880 - val_loss: 0.2384 - val_mse: 0.0448\n","Epoch 143/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5768 - mse: 1.3839 \n","Epoch 143: val_loss did not improve from 0.23835\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5604 - mse: 1.3676 - val_loss: 0.2411 - val_mse: 0.0513\n","Epoch 144/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5737 - mse: 1.3841 \n","Epoch 144: val_loss did not improve from 0.23835\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5753 - mse: 1.3858 - val_loss: 0.2425 - val_mse: 0.0551\n","Epoch 145/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6043 - mse: 1.4183 \n","Epoch 145: val_loss did not improve from 0.23835\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6175 - mse: 1.4317 - val_loss: 0.2489 - val_mse: 0.0657\n","Epoch 146/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6034 - mse: 1.4208 \n","Epoch 146: val_loss did not improve from 0.23835\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5942 - mse: 1.4117 - val_loss: 0.2549 - val_mse: 0.0760\n","Epoch 147/150\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4844 - mse: 1.3073 \n","Epoch 147: val_loss improved from 0.23835 to 0.22001, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4962 - mse: 1.3192 - val_loss: 0.2200 - val_mse: 0.0450\n","Epoch 148/150\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5152 - mse: 1.3408 \n","Epoch 148: val_loss did not improve from 0.22001\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5219 - mse: 1.3475 - val_loss: 0.2413 - val_mse: 0.0690\n","Epoch 149/150\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5276 - mse: 1.3559 \n","Epoch 149: val_loss improved from 0.22001 to 0.21452, saving model to best_model_RMSprop_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5415 - mse: 1.3699 - val_loss: 0.2145 - val_mse: 0.0431\n","Epoch 150/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5130 - mse: 1.3421 \n","Epoch 150: val_loss did not improve from 0.21452\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4969 - mse: 1.3264 - val_loss: 0.2185 - val_mse: 0.0513\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: RMSprop, Epochs: 150, Test MSE: 0.05742240259624351\n","Training with optimizer: Nadam, epochs: 150\n","Epoch 1/150\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - loss: 151.9328 - mse: 146.3506\n","Epoch 1: val_loss improved from inf to 144.73209, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 334ms/step - loss: 151.9179 - mse: 146.3361 - val_loss: 144.7321 - val_mse: 139.1732\n","Epoch 2/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.3450 - mse: 141.7882 \n","Epoch 2: val_loss improved from 144.73209 to 139.77116, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 147.0267 - mse: 141.4704 - val_loss: 139.7712 - val_mse: 134.2240\n","Epoch 3/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.5417 - mse: 134.9966 \n","Epoch 3: val_loss improved from 139.77116 to 135.27309, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 140.5960 - mse: 135.0514 - val_loss: 135.2731 - val_mse: 129.7337\n","Epoch 4/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 137.2074 - mse: 131.6682 \n","Epoch 4: val_loss improved from 135.27309 to 128.01175, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 137.0519 - mse: 131.5129 - val_loss: 128.0117 - val_mse: 122.4752\n","Epoch 5/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 134.1069 - mse: 128.5709 \n","Epoch 5: val_loss improved from 128.01175 to 121.07597, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 133.7816 - mse: 128.2456 - val_loss: 121.0760 - val_mse: 115.5393\n","Epoch 6/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.7615 - mse: 119.2243 \n","Epoch 6: val_loss improved from 121.07597 to 114.25851, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 124.6145 - mse: 119.0772 - val_loss: 114.2585 - val_mse: 108.7205\n","Epoch 7/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.3487 - mse: 110.8102 \n","Epoch 7: val_loss improved from 114.25851 to 106.70404, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 116.3340 - mse: 110.7954 - val_loss: 106.7040 - val_mse: 101.1648\n","Epoch 8/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.9653 - mse: 104.4254 \n","Epoch 8: val_loss improved from 106.70404 to 96.87434, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 109.3334 - mse: 103.7933 - val_loss: 96.8743 - val_mse: 91.3312\n","Epoch 9/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.8743 - mse: 97.3304  \n","Epoch 9: val_loss improved from 96.87434 to 86.63572, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 101.9048 - mse: 96.3606 - val_loss: 86.6357 - val_mse: 81.0893\n","Epoch 10/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.9612 - mse: 84.4138 \n","Epoch 10: val_loss improved from 86.63572 to 77.38144, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 89.3636 - mse: 83.8159 - val_loss: 77.3814 - val_mse: 71.8308\n","Epoch 11/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.2352 - mse: 74.6831 \n","Epoch 11: val_loss improved from 77.38144 to 66.53938, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 79.6368 - mse: 74.0844 - val_loss: 66.5394 - val_mse: 60.9817\n","Epoch 12/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.2449 - mse: 63.6867 \n","Epoch 12: val_loss improved from 66.53938 to 54.66636, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 68.4995 - mse: 62.9411 - val_loss: 54.6664 - val_mse: 49.1055\n","Epoch 13/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.2392 - mse: 56.6778 \n","Epoch 13: val_loss improved from 54.66636 to 45.20464, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 61.2696 - mse: 55.7080 - val_loss: 45.2046 - val_mse: 39.6400\n","Epoch 14/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.0315 - mse: 42.4661 \n","Epoch 14: val_loss improved from 45.20464 to 37.22106, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 47.7416 - mse: 42.1760 - val_loss: 37.2211 - val_mse: 31.6539\n","Epoch 15/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.3385 - mse: 35.7711 \n","Epoch 15: val_loss improved from 37.22106 to 30.05518, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 40.7432 - mse: 35.1757 - val_loss: 30.0552 - val_mse: 24.4869\n","Epoch 16/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33.6045 - mse: 28.0364 \n","Epoch 16: val_loss improved from 30.05518 to 23.42591, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 33.2465 - mse: 27.6783 - val_loss: 23.4259 - val_mse: 17.8580\n","Epoch 17/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.2985 - mse: 23.7309 \n","Epoch 17: val_loss improved from 23.42591 to 17.97111, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 28.6556 - mse: 23.0882 - val_loss: 17.9711 - val_mse: 12.4065\n","Epoch 18/150\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24.0744 - mse: 18.5105 \n","Epoch 18: val_loss improved from 17.97111 to 13.78129, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.6917 - mse: 18.1283 - val_loss: 13.7813 - val_mse: 8.2219\n","Epoch 19/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.4672 - mse: 13.9088 \n","Epoch 19: val_loss improved from 13.78129 to 11.04125, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.3984 - mse: 13.8403 - val_loss: 11.0413 - val_mse: 5.4874\n","Epoch 20/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.1099 - mse: 13.5570 \n","Epoch 20: val_loss improved from 11.04125 to 9.42590, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.7973 - mse: 13.2447 - val_loss: 9.4259 - val_mse: 3.8775\n","Epoch 21/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.9383 - mse: 11.3911 \n","Epoch 21: val_loss improved from 9.42590 to 7.95141, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.5545 - mse: 11.0077 - val_loss: 7.9514 - val_mse: 2.4099\n","Epoch 22/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.0632 - mse: 8.5231 \n","Epoch 22: val_loss improved from 7.95141 to 7.29224, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.9816 - mse: 8.4419 - val_loss: 7.2922 - val_mse: 1.7587\n","Epoch 23/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2423 - mse: 7.7105 \n","Epoch 23: val_loss improved from 7.29224 to 6.80147, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.1854 - mse: 7.6542 - val_loss: 6.8015 - val_mse: 1.2773\n","Epoch 24/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.6375 - mse: 7.1151 \n","Epoch 24: val_loss improved from 6.80147 to 6.36367, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.5819 - mse: 7.0600 - val_loss: 6.3637 - val_mse: 0.8487\n","Epoch 25/150\n","\u001b[1m14/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.1747 - mse: 7.6610 \n","Epoch 25: val_loss improved from 6.36367 to 6.28293, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.0914 - mse: 7.5789 - val_loss: 6.2829 - val_mse: 0.7777\n","Epoch 26/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.3277 - mse: 5.8244 \n","Epoch 26: val_loss improved from 6.28293 to 6.27224, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.3952 - mse: 5.8924 - val_loss: 6.2722 - val_mse: 0.7763\n","Epoch 27/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.3223 - mse: 6.8279 \n","Epoch 27: val_loss improved from 6.27224 to 6.09478, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.3050 - mse: 6.8113 - val_loss: 6.0948 - val_mse: 0.6082\n","Epoch 28/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.9935 - mse: 6.5086\n","Epoch 28: val_loss improved from 6.09478 to 5.97888, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.0348 - mse: 6.5505 - val_loss: 5.9789 - val_mse: 0.5018\n","Epoch 29/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.0023 - mse: 6.5270 \n","Epoch 29: val_loss did not improve from 5.97888\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.9579 - mse: 6.4830 - val_loss: 5.9838 - val_mse: 0.5158\n","Epoch 30/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7571 - mse: 5.2911 \n","Epoch 30: val_loss improved from 5.97888 to 5.79692, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.7787 - mse: 5.3133 - val_loss: 5.7969 - val_mse: 0.3399\n","Epoch 31/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.4915 - mse: 6.0367 \n","Epoch 31: val_loss improved from 5.79692 to 5.75873, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3683 - mse: 5.9143 - val_loss: 5.7587 - val_mse: 0.3138\n","Epoch 32/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.4292 - mse: 5.9865 \n","Epoch 32: val_loss improved from 5.75873 to 5.70470, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3533 - mse: 5.9113 - val_loss: 5.7047 - val_mse: 0.2713\n","Epoch 33/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.0934 - mse: 4.6626\n","Epoch 33: val_loss improved from 5.70470 to 5.68813, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.1314 - mse: 4.7012 - val_loss: 5.6881 - val_mse: 0.2682\n","Epoch 34/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3810 - mse: 4.9636 \n","Epoch 34: val_loss improved from 5.68813 to 5.66300, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.4240 - mse: 5.0073 - val_loss: 5.6630 - val_mse: 0.2561\n","Epoch 35/150\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6896 - mse: 5.2848 \n","Epoch 35: val_loss did not improve from 5.66300\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.6899 - mse: 5.2862 - val_loss: 5.6657 - val_mse: 0.2717\n","Epoch 36/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7528 - mse: 5.3612\n","Epoch 36: val_loss improved from 5.66300 to 5.63333, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.7014 - mse: 5.3107 - val_loss: 5.6333 - val_mse: 0.2527\n","Epoch 37/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9688 - mse: 4.5910  \n","Epoch 37: val_loss did not improve from 5.63333\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.9586 - mse: 4.5815 - val_loss: 5.6422 - val_mse: 0.2758\n","Epoch 38/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5293 - mse: 4.1656 \n","Epoch 38: val_loss improved from 5.63333 to 5.60023, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5127 - mse: 4.1499 - val_loss: 5.6002 - val_mse: 0.2487\n","Epoch 39/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1120 - mse: 4.7635\n","Epoch 39: val_loss improved from 5.60023 to 5.51472, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.1917 - mse: 4.8440 - val_loss: 5.5147 - val_mse: 0.1784\n","Epoch 40/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9690 - mse: 3.6354 \n","Epoch 40: val_loss did not improve from 5.51472\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.0555 - mse: 3.7228 - val_loss: 5.5155 - val_mse: 0.1944\n","Epoch 41/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3767 - mse: 4.0588 \n","Epoch 41: val_loss improved from 5.51472 to 5.44432, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.4256 - mse: 4.1085 - val_loss: 5.4443 - val_mse: 0.1394\n","Epoch 42/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.7681 - mse: 4.4663  \n","Epoch 42: val_loss improved from 5.44432 to 5.42671, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6932 - mse: 4.3923 - val_loss: 5.4267 - val_mse: 0.1380\n","Epoch 43/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8085 - mse: 4.5229 \n","Epoch 43: val_loss improved from 5.42671 to 5.41563, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.7119 - mse: 4.4272 - val_loss: 5.4156 - val_mse: 0.1431\n","Epoch 44/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1903 - mse: 3.9209 \n","Epoch 44: val_loss improved from 5.41563 to 5.38453, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2327 - mse: 3.9644 - val_loss: 5.3845 - val_mse: 0.1293\n","Epoch 45/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0234 - mse: 4.7712\n","Epoch 45: val_loss did not improve from 5.38453\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.9683 - mse: 4.7173 - val_loss: 5.3848 - val_mse: 0.1467\n","Epoch 46/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3164 - mse: 4.0812  \n","Epoch 46: val_loss improved from 5.38453 to 5.35565, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2358 - mse: 4.0021 - val_loss: 5.3556 - val_mse: 0.1353\n","Epoch 47/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9496 - mse: 4.7327  \n","Epoch 47: val_loss improved from 5.35565 to 5.32723, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.8674 - mse: 4.6515 - val_loss: 5.3272 - val_mse: 0.1250\n","Epoch 48/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7826 - mse: 3.5838 \n","Epoch 48: val_loss improved from 5.32723 to 5.31627, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7803 - mse: 3.5827 - val_loss: 5.3163 - val_mse: 0.1327\n","Epoch 49/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1726 - mse: 3.9925 \n","Epoch 49: val_loss improved from 5.31627 to 5.28505, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.1818 - mse: 4.0027 - val_loss: 5.2851 - val_mse: 0.1196\n","Epoch 50/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7496 - mse: 3.5876 \n","Epoch 50: val_loss improved from 5.28505 to 5.25892, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7652 - mse: 3.6043 - val_loss: 5.2589 - val_mse: 0.1117\n","Epoch 51/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7908 - mse: 3.6471 \n","Epoch 51: val_loss improved from 5.25892 to 5.24122, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.8274 - mse: 3.6848 - val_loss: 5.2412 - val_mse: 0.1130\n","Epoch 52/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3814 - mse: 3.2569 \n","Epoch 52: val_loss did not improve from 5.24122\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.3992 - mse: 3.2758 - val_loss: 5.2538 - val_mse: 0.1451\n","Epoch 53/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2450 - mse: 3.1401 \n","Epoch 53: val_loss improved from 5.24122 to 5.21256, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.3115 - mse: 3.2077 - val_loss: 5.2126 - val_mse: 0.1237\n","Epoch 54/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1011 - mse: 4.0156 \n","Epoch 54: val_loss improved from 5.21256 to 5.18182, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.0107 - mse: 3.9267 - val_loss: 5.1818 - val_mse: 0.1129\n","Epoch 55/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5275 - mse: 3.4622 \n","Epoch 55: val_loss improved from 5.18182 to 5.16190, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5151 - mse: 3.4511 - val_loss: 5.1619 - val_mse: 0.1132\n","Epoch 56/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6810 - mse: 3.6360 \n","Epoch 56: val_loss improved from 5.16190 to 5.14868, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6972 - mse: 3.6535 - val_loss: 5.1487 - val_mse: 0.1203\n","Epoch 57/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1537 - mse: 3.1294 \n","Epoch 57: val_loss improved from 5.14868 to 5.13697, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1606 - mse: 3.1374 - val_loss: 5.1370 - val_mse: 0.1294\n","Epoch 58/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0782 - mse: 3.0747 \n","Epoch 58: val_loss improved from 5.13697 to 5.09011, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0967 - mse: 3.0943 - val_loss: 5.0901 - val_mse: 0.1036\n","Epoch 59/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6779 - mse: 3.6955 \n","Epoch 59: val_loss improved from 5.09011 to 5.06720, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5653 - mse: 3.5841 - val_loss: 5.0672 - val_mse: 0.1020\n","Epoch 60/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0171 - mse: 3.0561 \n","Epoch 60: val_loss improved from 5.06720 to 5.04661, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0411 - mse: 3.0814 - val_loss: 5.0466 - val_mse: 0.1033\n","Epoch 61/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9660 - mse: 3.0269 \n","Epoch 61: val_loss improved from 5.04661 to 5.02793, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9957 - mse: 3.0579 - val_loss: 5.0279 - val_mse: 0.1069\n","Epoch 62/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6678 - mse: 2.7507 \n","Epoch 62: val_loss improved from 5.02793 to 5.00571, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7403 - mse: 2.8249 - val_loss: 5.0057 - val_mse: 0.1071\n","Epoch 63/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2105 - mse: 3.3161 \n","Epoch 63: val_loss improved from 5.00571 to 4.95860, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2060 - mse: 3.3130 - val_loss: 4.9586 - val_mse: 0.0826\n","Epoch 64/150\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1669 - mse: 3.2944 \n","Epoch 64: val_loss did not improve from 4.95860\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1249 - mse: 3.2546 - val_loss: 4.9691 - val_mse: 0.1160\n","Epoch 65/150\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8874 - mse: 3.0380  \n","Epoch 65: val_loss improved from 4.95860 to 4.92443, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.8528 - mse: 3.0053 - val_loss: 4.9244 - val_mse: 0.0943\n","Epoch 66/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0846 - mse: 3.2585  \n","Epoch 66: val_loss improved from 4.92443 to 4.90024, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0390 - mse: 3.2147 - val_loss: 4.9002 - val_mse: 0.0935\n","Epoch 67/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0621 - mse: 3.2596  \n","Epoch 67: val_loss improved from 4.90024 to 4.85993, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0366 - mse: 3.2357 - val_loss: 4.8599 - val_mse: 0.0764\n","Epoch 68/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3884 - mse: 2.6092 \n","Epoch 68: val_loss improved from 4.85993 to 4.83968, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3791 - mse: 2.6014 - val_loss: 4.8397 - val_mse: 0.0799\n","Epoch 69/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6619 - mse: 2.9062 \n","Epoch 69: val_loss improved from 4.83968 to 4.83384, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.6568 - mse: 2.9029 - val_loss: 4.8338 - val_mse: 0.0980\n","Epoch 70/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6993 - mse: 2.9677 \n","Epoch 70: val_loss improved from 4.83384 to 4.80700, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7529 - mse: 3.0231 - val_loss: 4.8070 - val_mse: 0.0956\n","Epoch 71/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1417 - mse: 2.4349 \n","Epoch 71: val_loss improved from 4.80700 to 4.77734, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1706 - mse: 2.4653 - val_loss: 4.7773 - val_mse: 0.0908\n","Epoch 72/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1404 - mse: 2.4586 \n","Epoch 72: val_loss improved from 4.77734 to 4.75672, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1822 - mse: 2.5018 - val_loss: 4.7567 - val_mse: 0.0948\n","Epoch 73/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4398 - mse: 2.7824 \n","Epoch 73: val_loss improved from 4.75672 to 4.72514, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.4303 - mse: 2.7745 - val_loss: 4.7251 - val_mse: 0.0880\n","Epoch 74/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5065 - mse: 2.8740 \n","Epoch 74: val_loss improved from 4.72514 to 4.70224, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.4858 - mse: 2.8549 - val_loss: 4.7022 - val_mse: 0.0904\n","Epoch 75/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4069 - mse: 2.8000 \n","Epoch 75: val_loss did not improve from 4.70224\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.4009 - mse: 2.7954 - val_loss: 4.7041 - val_mse: 0.1178\n","Epoch 76/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2766 - mse: 2.6951 \n","Epoch 76: val_loss improved from 4.70224 to 4.66060, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2784 - mse: 2.6986 - val_loss: 4.6606 - val_mse: 0.1005\n","Epoch 77/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4048 - mse: 2.8495 \n","Epoch 77: val_loss improved from 4.66060 to 4.62465, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3839 - mse: 2.8303 - val_loss: 4.6247 - val_mse: 0.0908\n","Epoch 78/150\n","\u001b[1m14/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.1562 - mse: 2.6256 \n","Epoch 78: val_loss improved from 4.62465 to 4.59339, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2026 - mse: 2.6753 - val_loss: 4.5934 - val_mse: 0.0859\n","Epoch 79/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1801 - mse: 2.6777 \n","Epoch 79: val_loss improved from 4.59339 to 4.54801, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1750 - mse: 2.6741 - val_loss: 4.5480 - val_mse: 0.0672\n","Epoch 80/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2309 - mse: 2.7550 \n","Epoch 80: val_loss improved from 4.54801 to 4.51895, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2255 - mse: 2.7513 - val_loss: 4.5189 - val_mse: 0.0647\n","Epoch 81/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6927 - mse: 2.2436 \n","Epoch 81: val_loss improved from 4.51895 to 4.49889, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7266 - mse: 2.2790 - val_loss: 4.4989 - val_mse: 0.0716\n","Epoch 82/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1659 - mse: 2.7438 \n","Epoch 82: val_loss improved from 4.49889 to 4.46215, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1201 - mse: 2.6995 - val_loss: 4.4621 - val_mse: 0.0620\n","Epoch 83/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5894 - mse: 2.1945 \n","Epoch 83: val_loss improved from 4.46215 to 4.43713, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6453 - mse: 2.2520 - val_loss: 4.4371 - val_mse: 0.0645\n","Epoch 84/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7441 - mse: 2.3768 \n","Epoch 84: val_loss improved from 4.43713 to 4.40253, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7704 - mse: 2.4046 - val_loss: 4.4025 - val_mse: 0.0575\n","Epoch 85/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9327 - mse: 2.5927 \n","Epoch 85: val_loss improved from 4.40253 to 4.38745, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8701 - mse: 2.5319 - val_loss: 4.3874 - val_mse: 0.0698\n","Epoch 86/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6583 - mse: 2.3458 \n","Epoch 86: val_loss improved from 4.38745 to 4.37093, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6802 - mse: 2.3695 - val_loss: 4.3709 - val_mse: 0.0812\n","Epoch 87/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4990 - mse: 2.2144 \n","Epoch 87: val_loss improved from 4.37093 to 4.34864, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4933 - mse: 2.2106 - val_loss: 4.3486 - val_mse: 0.0872\n","Epoch 88/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8105 - mse: 2.5544 \n","Epoch 88: val_loss improved from 4.34864 to 4.30229, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8024 - mse: 2.5480 - val_loss: 4.3023 - val_mse: 0.0691\n","Epoch 89/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7269 - mse: 2.4992 \n","Epoch 89: val_loss improved from 4.30229 to 4.27041, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7243 - mse: 2.4981 - val_loss: 4.2704 - val_mse: 0.0656\n","Epoch 90/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4949 - mse: 2.2953 \n","Epoch 90: val_loss improved from 4.27041 to 4.23439, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5519 - mse: 2.3542 - val_loss: 4.2344 - val_mse: 0.0583\n","Epoch 91/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5904 - mse: 2.4198 \n","Epoch 91: val_loss improved from 4.23439 to 4.21944, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5407 - mse: 2.3716 - val_loss: 4.2194 - val_mse: 0.0718\n","Epoch 92/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4257 - mse: 2.2837 \n","Epoch 92: val_loss improved from 4.21944 to 4.17363, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4385 - mse: 2.2981 - val_loss: 4.1736 - val_mse: 0.0553\n","Epoch 93/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5466 - mse: 2.4334 \n","Epoch 93: val_loss improved from 4.17363 to 4.14623, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5298 - mse: 2.4188 - val_loss: 4.1462 - val_mse: 0.0573\n","Epoch 94/150\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2930 - mse: 2.2088 \n","Epoch 94: val_loss improved from 4.14623 to 4.12052, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3568 - mse: 2.2751 - val_loss: 4.1205 - val_mse: 0.0609\n","Epoch 95/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2815 - mse: 2.2275 \n","Epoch 95: val_loss improved from 4.12052 to 4.10496, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3082 - mse: 2.2559 - val_loss: 4.1050 - val_mse: 0.0749\n","Epoch 96/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2222 - mse: 2.1976 \n","Epoch 96: val_loss improved from 4.10496 to 4.06184, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2357 - mse: 2.2130 - val_loss: 4.0618 - val_mse: 0.0616\n","Epoch 97/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2972 - mse: 2.3027 \n","Epoch 97: val_loss improved from 4.06184 to 4.02503, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2769 - mse: 2.2842 - val_loss: 4.0250 - val_mse: 0.0550\n","Epoch 98/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4883 - mse: 2.5241 \n","Epoch 98: val_loss improved from 4.02503 to 4.00116, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4712 - mse: 2.5086 - val_loss: 4.0012 - val_mse: 0.0613\n","Epoch 99/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8856 - mse: 1.9516 \n","Epoch 99: val_loss improved from 4.00116 to 3.98344, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9232 - mse: 1.9908 - val_loss: 3.9834 - val_mse: 0.0740\n","Epoch 100/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2597 - mse: 2.3561 \n","Epoch 100: val_loss improved from 3.98344 to 3.95714, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2413 - mse: 2.3394 - val_loss: 3.9571 - val_mse: 0.0783\n","Epoch 101/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0485 - mse: 2.1752 \n","Epoch 101: val_loss improved from 3.95714 to 3.91776, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0710 - mse: 2.1997 - val_loss: 3.9178 - val_mse: 0.0695\n","Epoch 102/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0079 - mse: 2.1650 \n","Epoch 102: val_loss improved from 3.91776 to 3.87434, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0143 - mse: 2.1737 - val_loss: 3.8743 - val_mse: 0.0570\n","Epoch 103/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1475 - mse: 2.3355 \n","Epoch 103: val_loss improved from 3.87434 to 3.84764, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0922 - mse: 2.2826 - val_loss: 3.8476 - val_mse: 0.0612\n","Epoch 104/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8579 - mse: 2.0774 \n","Epoch 104: val_loss improved from 3.84764 to 3.81282, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8735 - mse: 2.0947 - val_loss: 3.8128 - val_mse: 0.0574\n","Epoch 105/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0390 - mse: 2.2896 \n","Epoch 105: val_loss improved from 3.81282 to 3.80097, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0232 - mse: 2.2755 - val_loss: 3.8010 - val_mse: 0.0768\n","Epoch 106/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7033 - mse: 1.9852 \n","Epoch 106: val_loss improved from 3.80097 to 3.76694, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7358 - mse: 2.0194 - val_loss: 3.7669 - val_mse: 0.0740\n","Epoch 107/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8033 - mse: 2.1164 \n","Epoch 107: val_loss improved from 3.76694 to 3.73383, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8078 - mse: 2.1226 - val_loss: 3.7338 - val_mse: 0.0724\n","Epoch 108/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6700 - mse: 2.0150 \n","Epoch 108: val_loss improved from 3.73383 to 3.68293, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6928 - mse: 2.0392 - val_loss: 3.6829 - val_mse: 0.0531\n","Epoch 109/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7367 - mse: 2.1130 \n","Epoch 109: val_loss improved from 3.68293 to 3.66794, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7373 - mse: 2.1153 - val_loss: 3.6679 - val_mse: 0.0699\n","Epoch 110/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5854 - mse: 1.9936 \n","Epoch 110: val_loss improved from 3.66794 to 3.63028, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5990 - mse: 2.0089 - val_loss: 3.6303 - val_mse: 0.0642\n","Epoch 111/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7415 - mse: 2.1813 \n","Epoch 111: val_loss improved from 3.63028 to 3.60320, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7346 - mse: 2.1765 - val_loss: 3.6032 - val_mse: 0.0691\n","Epoch 112/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6348 - mse: 2.1066 \n","Epoch 112: val_loss improved from 3.60320 to 3.56581, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.6335 - mse: 2.1074 - val_loss: 3.5658 - val_mse: 0.0638\n","Epoch 113/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3948 - mse: 1.8990 \n","Epoch 113: val_loss improved from 3.56581 to 3.54143, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4302 - mse: 1.9362 - val_loss: 3.5414 - val_mse: 0.0716\n","Epoch 114/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4508 - mse: 1.9868 \n","Epoch 114: val_loss improved from 3.54143 to 3.50853, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4683 - mse: 2.0064 - val_loss: 3.5085 - val_mse: 0.0708\n","Epoch 115/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4771 - mse: 2.0456 \n","Epoch 115: val_loss improved from 3.50853 to 3.47620, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4574 - mse: 2.0278 - val_loss: 3.4762 - val_mse: 0.0709\n","Epoch 116/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4503 - mse: 2.0515 \n","Epoch 116: val_loss improved from 3.47620 to 3.42997, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4296 - mse: 2.0324 - val_loss: 3.4300 - val_mse: 0.0570\n","Epoch 117/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2273 - mse: 1.8607 \n","Epoch 117: val_loss improved from 3.42997 to 3.39472, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2757 - mse: 1.9109 - val_loss: 3.3947 - val_mse: 0.0545\n","Epoch 118/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1784 - mse: 1.8448 \n","Epoch 118: val_loss improved from 3.39472 to 3.38637, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1952 - mse: 1.8631 - val_loss: 3.3864 - val_mse: 0.0787\n","Epoch 119/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2443 - mse: 1.9429 \n","Epoch 119: val_loss improved from 3.38637 to 3.32877, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.2605 - mse: 1.9610 - val_loss: 3.3288 - val_mse: 0.0538\n","Epoch 120/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1264 - mse: 1.8571 \n","Epoch 120: val_loss improved from 3.32877 to 3.30064, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.1225 - mse: 1.8557 - val_loss: 3.3006 - val_mse: 0.0586\n","Epoch 121/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4946 - mse: 2.2586 \n","Epoch 121: val_loss improved from 3.30064 to 3.25739, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4811 - mse: 2.2472 - val_loss: 3.2574 - val_mse: 0.0483\n","Epoch 122/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8170 - mse: 1.6143 \n","Epoch 122: val_loss improved from 3.25739 to 3.22050, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8358 - mse: 1.6350 - val_loss: 3.2205 - val_mse: 0.0446\n","Epoch 123/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3793 - mse: 2.2094 \n","Epoch 123: val_loss improved from 3.22050 to 3.20477, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3431 - mse: 2.1754 - val_loss: 3.2048 - val_mse: 0.0618\n","Epoch 124/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2124 - mse: 2.0761 \n","Epoch 124: val_loss improved from 3.20477 to 3.16765, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1906 - mse: 2.0558 - val_loss: 3.1677 - val_mse: 0.0577\n","Epoch 125/150\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8696 - mse: 1.7664 \n","Epoch 125: val_loss improved from 3.16765 to 3.14228, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8859 - mse: 1.7842 - val_loss: 3.1423 - val_mse: 0.0655\n","Epoch 126/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1664 - mse: 2.0959 \n","Epoch 126: val_loss improved from 3.14228 to 3.08747, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1526 - mse: 2.0840 - val_loss: 3.0875 - val_mse: 0.0439\n","Epoch 127/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9829 - mse: 1.9457 \n","Epoch 127: val_loss improved from 3.08747 to 3.05123, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9924 - mse: 1.9571 - val_loss: 3.0512 - val_mse: 0.0409\n","Epoch 128/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6643 - mse: 1.6604 \n","Epoch 128: val_loss improved from 3.05123 to 3.02489, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6993 - mse: 1.6973 - val_loss: 3.0249 - val_mse: 0.0479\n","Epoch 129/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9387 - mse: 1.9676 \n","Epoch 129: val_loss improved from 3.02489 to 3.00882, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.9593 - mse: 1.9907 - val_loss: 3.0088 - val_mse: 0.0651\n","Epoch 130/150\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9358 - mse: 1.9975 \n","Epoch 130: val_loss improved from 3.00882 to 2.96088, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.9186 - mse: 1.9831 - val_loss: 2.9609 - val_mse: 0.0504\n","Epoch 131/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5329 - mse: 1.6285 \n","Epoch 131: val_loss improved from 2.96088 to 2.93195, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5395 - mse: 1.6373 - val_loss: 2.9320 - val_mse: 0.0548\n","Epoch 132/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7351 - mse: 1.8640 \n","Epoch 132: val_loss improved from 2.93195 to 2.89248, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7333 - mse: 1.8645 - val_loss: 2.8925 - val_mse: 0.0488\n","Epoch 133/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6592 - mse: 1.8219 \n","Epoch 133: val_loss improved from 2.89248 to 2.86306, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6728 - mse: 1.8374 - val_loss: 2.8631 - val_mse: 0.0526\n","Epoch 134/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9057 - mse: 2.1016 \n","Epoch 134: val_loss improved from 2.86306 to 2.81781, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8719 - mse: 2.0697 - val_loss: 2.8178 - val_mse: 0.0405\n","Epoch 135/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4517 - mse: 1.6807 \n","Epoch 135: val_loss improved from 2.81781 to 2.79572, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4798 - mse: 1.7107 - val_loss: 2.7957 - val_mse: 0.0516\n","Epoch 136/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5281 - mse: 1.7901 \n","Epoch 136: val_loss improved from 2.79572 to 2.76307, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5508 - mse: 1.8150 - val_loss: 2.7631 - val_mse: 0.0523\n","Epoch 137/150\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8348 - mse: 2.1298 \n","Epoch 137: val_loss improved from 2.76307 to 2.71973, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7709 - mse: 2.0683 - val_loss: 2.7197 - val_mse: 0.0420\n","Epoch 138/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2617 - mse: 1.5904 \n","Epoch 138: val_loss improved from 2.71973 to 2.69057, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2769 - mse: 1.6075 - val_loss: 2.6906 - val_mse: 0.0461\n","Epoch 139/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4513 - mse: 1.8132 \n","Epoch 139: val_loss improved from 2.69057 to 2.65839, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4707 - mse: 1.8345 - val_loss: 2.6584 - val_mse: 0.0470\n","Epoch 140/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5296 - mse: 1.9245 \n","Epoch 140: val_loss improved from 2.65839 to 2.63675, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5469 - mse: 1.9438 - val_loss: 2.6368 - val_mse: 0.0584\n","Epoch 141/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6330 - mse: 2.0610 \n","Epoch 141: val_loss improved from 2.63675 to 2.59955, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6250 - mse: 2.0548 - val_loss: 2.5995 - val_mse: 0.0542\n","Epoch 142/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3806 - mse: 1.8415 \n","Epoch 142: val_loss improved from 2.59955 to 2.55525, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3694 - mse: 1.8322 - val_loss: 2.5552 - val_mse: 0.0430\n","Epoch 143/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2651 - mse: 1.7592 \n","Epoch 143: val_loss improved from 2.55525 to 2.53300, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2430 - mse: 1.7390 - val_loss: 2.5330 - val_mse: 0.0537\n","Epoch 144/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2901 - mse: 1.8168 \n","Epoch 144: val_loss improved from 2.53300 to 2.51580, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2886 - mse: 1.8175 - val_loss: 2.5158 - val_mse: 0.0692\n","Epoch 145/150\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3753 - mse: 1.9341 \n","Epoch 145: val_loss improved from 2.51580 to 2.47079, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4122 - mse: 1.9739 - val_loss: 2.4708 - val_mse: 0.0571\n","Epoch 146/150\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2392 - mse: 1.8315 \n","Epoch 146: val_loss improved from 2.47079 to 2.43767, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2305 - mse: 1.8250 - val_loss: 2.4377 - val_mse: 0.0568\n","Epoch 147/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0982 - mse: 1.7237 \n","Epoch 147: val_loss improved from 2.43767 to 2.40939, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1246 - mse: 1.7519 - val_loss: 2.4094 - val_mse: 0.0614\n","Epoch 148/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1662 - mse: 1.8244 \n","Epoch 148: val_loss improved from 2.40939 to 2.36218, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1683 - mse: 1.8284 - val_loss: 2.3622 - val_mse: 0.0465\n","Epoch 149/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1360 - mse: 1.8266 \n","Epoch 149: val_loss improved from 2.36218 to 2.33840, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1390 - mse: 1.8314 - val_loss: 2.3384 - val_mse: 0.0551\n","Epoch 150/150\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9653 - mse: 1.6882 \n","Epoch 150: val_loss improved from 2.33840 to 2.30307, saving model to best_model_Nadam_epochs_150.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9641 - mse: 1.6888 - val_loss: 2.3031 - val_mse: 0.0522\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: Nadam, Epochs: 150, Test MSE: 0.07433677695152703\n"]}],"source":["for opt in optimizers:\n","    opt_name = opt.__name__  # Access the class name directly\n","    results[opt_name] = {}\n","    for epochs in epochs_list:\n","        print(f\"Training with optimizer: {opt_name}, epochs: {epochs}\")\n","        \n","        set_seed(seed_value)  # Reinitialize the seed before creating the model\n","        optimizer_instance = opt()  # Create a new instance of the optimizer\n","        model = create_model(optimizer=optimizer_instance, loss='mse')  # Default loss function 'mse'\n","        \n","        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","        model_checkpoint = ModelCheckpoint(f'best_model_{opt_name}_epochs_{epochs}.keras', monitor='val_loss', save_best_only=True, verbose=1)\n","        \n","        history = model.fit(X_train_scaled, y_train_log, validation_split=0.2, epochs=epochs, batch_size=32, \n","                            callbacks=[early_stopping, model_checkpoint], verbose=1)\n","        \n","        # Load the best model saved by ModelCheckpoint\n","        model.load_weights(f'best_model_{opt_name}_epochs_{epochs}.keras')\n","        \n","        y_pred_log = model.predict(X_val_scaled)\n","        \n","        mse = mean_squared_error(y_val_log, y_pred_log.flatten())\n","        \n","        results[opt_name][f'epochs_{epochs}_mse'] = {'MSE': mse, 'history': history.history}\n","        \n","        print(f\"Optimizer: {opt_name}, Epochs: {epochs}, Test MSE: {mse}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### test loss function "]},{"cell_type":"markdown","metadata":{},"source":["We perform additional training with different loss functions for each optimizer in the optimizers list. For each combination of optimizer and loss function in loss_functions, and for each value of epochs in additional_epochs, we display a message indicating the optimizer, loss function, and number of epochs being trained.\n","\n","We reset the random seed to ensure reproducibility (set_seed(seed_value)). Next, we create an instance of the optimizer and a model using the create_model function with the optimizer and the specified loss function.\n","\n","We define callbacks for early stopping (EarlyStopping) and saving the best model (ModelCheckpoint). We train the model on the normalized and log-transformed training data (X_train_scaled and y_train_log), using 20% cross-validation of the data, for the specified number of epochs.\n","\n","Next, we load the weights of the best model saved by ModelCheckpoint. We make predictions on the normalized validation data (X_val_scaled), calculate the mean square error (MSE) between the predicted and actual log-transformed values, and store the results (MSE and training history) in the results dictionary.\n","\n","Finally, we display the MSE for the optimizer, the loss function, and the number of epochs being trained."]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T12:52:12.165086Z","iopub.status.busy":"2024-06-02T12:52:12.164358Z","iopub.status.idle":"2024-06-02T12:59:15.531817Z","shell.execute_reply":"2024-06-02T12:59:15.530839Z","shell.execute_reply.started":"2024-06-02T12:52:12.165049Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training with optimizer: Adam, loss function: mse, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 151.7273 - mse: 146.1441\n","Epoch 1: val_loss improved from inf to 144.39944, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 308ms/step - loss: 151.7009 - mse: 146.1179 - val_loss: 144.3994 - val_mse: 138.8348\n","Epoch 2/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.3045 - mse: 141.7422 \n","Epoch 2: val_loss improved from 144.39944 to 139.98276, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 146.9201 - mse: 141.3585 - val_loss: 139.9828 - val_mse: 134.4307\n","Epoch 3/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.3944 - mse: 134.8440 \n","Epoch 3: val_loss improved from 139.98276 to 135.39253, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 140.3515 - mse: 134.8015 - val_loss: 135.3925 - val_mse: 129.8475\n","Epoch 4/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 135.8320 - mse: 130.2877 \n","Epoch 4: val_loss improved from 135.39253 to 129.22597, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 135.6197 - mse: 130.0756 - val_loss: 129.2260 - val_mse: 123.6852\n","Epoch 5/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.5114 - mse: 125.9711 \n","Epoch 5: val_loss improved from 129.22597 to 123.52956, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 131.3670 - mse: 125.8267 - val_loss: 123.5296 - val_mse: 117.9900\n","Epoch 6/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.1759 - mse: 118.6366 \n","Epoch 6: val_loss improved from 123.52956 to 116.66747, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 123.9950 - mse: 118.4557 - val_loss: 116.6675 - val_mse: 111.1295\n","Epoch 7/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 115.9658 - mse: 110.4273 \n","Epoch 7: val_loss improved from 116.66747 to 107.28171, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 115.7553 - mse: 110.2166 - val_loss: 107.2817 - val_mse: 101.7410\n","Epoch 8/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 110.4828 - mse: 104.9417 \n","Epoch 8: val_loss improved from 107.28171 to 97.49488, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 109.8402 - mse: 104.2990 - val_loss: 97.4949 - val_mse: 91.9523\n","Epoch 9/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.8980 - mse: 95.3548 \n","Epoch 9: val_loss improved from 97.49488 to 88.04699, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 100.1450 - mse: 94.6014 - val_loss: 88.0470 - val_mse: 82.4999\n","Epoch 10/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91.5505 - mse: 86.0026 \n","Epoch 10: val_loss improved from 88.04699 to 78.06598, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 90.8450 - mse: 85.2969 - val_loss: 78.0660 - val_mse: 72.5153\n","Epoch 11/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.2326 - mse: 72.6810 \n","Epoch 11: val_loss improved from 78.06598 to 67.31535, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 77.7782 - mse: 72.2264 - val_loss: 67.3153 - val_mse: 61.7604\n","Epoch 12/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67.9123 - mse: 62.3568 \n","Epoch 12: val_loss improved from 67.31535 to 56.45268, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 67.3573 - mse: 61.8016 - val_loss: 56.4527 - val_mse: 50.8937\n","Epoch 13/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57.9316 - mse: 52.3719 \n","Epoch 13: val_loss improved from 56.45268 to 46.37225, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 57.4853 - mse: 51.9254 - val_loss: 46.3722 - val_mse: 40.8097\n","Epoch 14/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48.3683 - mse: 42.8052 \n","Epoch 14: val_loss improved from 46.37225 to 37.29024, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 48.1428 - mse: 42.5797 - val_loss: 37.2902 - val_mse: 31.7267\n","Epoch 15/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.5086 - mse: 34.9448 \n","Epoch 15: val_loss improved from 37.29024 to 29.86378, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 39.9165 - mse: 34.3527 - val_loss: 29.8638 - val_mse: 24.3005\n","Epoch 16/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.8390 - mse: 27.2761 \n","Epoch 16: val_loss improved from 29.86378 to 22.87077, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 32.1138 - mse: 26.5511 - val_loss: 22.8708 - val_mse: 17.3102\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.2935 - mse: 22.7337 \n","Epoch 17: val_loss improved from 22.87077 to 17.74915, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27.8346 - mse: 22.2751 - val_loss: 17.7491 - val_mse: 12.1928\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 22.9857 - mse: 17.4303 \n","Epoch 18: val_loss improved from 17.74915 to 14.01122, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.7588 - mse: 17.2037 - val_loss: 14.0112 - val_mse: 8.4600\n","Epoch 19/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18.5111 - mse: 12.9609 \n","Epoch 19: val_loss improved from 14.01122 to 11.48183, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.6108 - mse: 13.0610 - val_loss: 11.4818 - val_mse: 5.9360\n","Epoch 20/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.4584 - mse: 11.9135 \n","Epoch 20: val_loss improved from 11.48183 to 9.26194, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.2922 - mse: 11.7477 - val_loss: 9.2619 - val_mse: 3.7214\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.6094 - mse: 10.0699 \n","Epoch 21: val_loss improved from 9.26194 to 7.97892, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.5803 - mse: 10.0412 - val_loss: 7.9789 - val_mse: 2.4447\n","Epoch 22/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.8213 - mse: 9.2886  \n","Epoch 22: val_loss improved from 7.97892 to 7.18055, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.6779 - mse: 9.1455 - val_loss: 7.1806 - val_mse: 1.6531\n","Epoch 23/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.8116 - mse: 7.2856 \n","Epoch 23: val_loss improved from 7.18055 to 6.63913, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.7614 - mse: 7.2359 - val_loss: 6.6391 - val_mse: 1.1205\n","Epoch 24/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.0252 - mse: 7.5084 \n","Epoch 24: val_loss improved from 6.63913 to 6.41648, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.9834 - mse: 7.4673 - val_loss: 6.4165 - val_mse: 0.9064\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.5363 - mse: 7.0277 \n","Epoch 25: val_loss improved from 6.41648 to 6.17031, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.5881 - mse: 7.0800 - val_loss: 6.1703 - val_mse: 0.6686\n","Epoch 26/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.0149 - mse: 6.5149 \n","Epoch 26: val_loss improved from 6.17031 to 6.10852, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.0157 - mse: 6.5163 - val_loss: 6.1085 - val_mse: 0.6163\n","Epoch 27/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.9310 - mse: 6.4406 \n","Epoch 27: val_loss improved from 6.10852 to 6.04258, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.8970 - mse: 6.4072 - val_loss: 6.0426 - val_mse: 0.5606\n","Epoch 28/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.1627 - mse: 5.6829\n","Epoch 28: val_loss improved from 6.04258 to 5.91700, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.2197 - mse: 5.7405 - val_loss: 5.9170 - val_mse: 0.4460\n","Epoch 29/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.3674 - mse: 5.8984\n","Epoch 29: val_loss improved from 5.91700 to 5.82837, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3608 - mse: 5.8925 - val_loss: 5.8284 - val_mse: 0.3687\n","Epoch 30/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.3130 - mse: 6.8553 \n","Epoch 30: val_loss improved from 5.82837 to 5.79699, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.1112 - mse: 6.6542 - val_loss: 5.7970 - val_mse: 0.3477\n","Epoch 31/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.0036 - mse: 5.5565 \n","Epoch 31: val_loss did not improve from 5.79699\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.9580 - mse: 5.5115 - val_loss: 5.8048 - val_mse: 0.3670\n","Epoch 32/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7380 - mse: 5.3023 \n","Epoch 32: val_loss improved from 5.79699 to 5.74989, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6560 - mse: 5.2209 - val_loss: 5.7499 - val_mse: 0.3237\n","Epoch 33/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6612 - mse: 5.2371 \n","Epoch 33: val_loss improved from 5.74989 to 5.74659, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.6357 - mse: 5.2127 - val_loss: 5.7466 - val_mse: 0.3331\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.8117 - mse: 5.4005 \n","Epoch 34: val_loss improved from 5.74659 to 5.67390, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.7743 - mse: 5.3638 - val_loss: 5.6739 - val_mse: 0.2728\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.4803 - mse: 5.0816 \n","Epoch 35: val_loss did not improve from 5.67390\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.4753 - mse: 5.0774 - val_loss: 5.6852 - val_mse: 0.2969\n","Epoch 36/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6745 - mse: 5.2886 \n","Epoch 36: val_loss did not improve from 5.67390\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.6231 - mse: 5.2378 - val_loss: 5.6782 - val_mse: 0.3022\n","Epoch 37/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9450 - mse: 4.5719 \n","Epoch 37: val_loss improved from 5.67390 to 5.64751, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.0101 - mse: 4.6376 - val_loss: 5.6475 - val_mse: 0.2857\n","Epoch 38/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4226 - mse: 4.0637 \n","Epoch 38: val_loss improved from 5.64751 to 5.64236, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5037 - mse: 4.1456 - val_loss: 5.6424 - val_mse: 0.2953\n","Epoch 39/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1439 - mse: 4.7997\n","Epoch 39: val_loss improved from 5.64236 to 5.57151, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.2100 - mse: 4.8666 - val_loss: 5.5715 - val_mse: 0.2389\n","Epoch 40/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6502 - mse: 4.3205 \n","Epoch 40: val_loss improved from 5.57151 to 5.54069, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6956 - mse: 4.3667 - val_loss: 5.5407 - val_mse: 0.2225\n","Epoch 41/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4317 - mse: 4.1162 \n","Epoch 41: val_loss did not improve from 5.54069\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.4708 - mse: 4.1562 - val_loss: 5.5625 - val_mse: 0.2590\n","Epoch 42/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1734 - mse: 3.8730  \n","Epoch 42: val_loss improved from 5.54069 to 5.52765, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.2044 - mse: 3.9048 - val_loss: 5.5277 - val_mse: 0.2398\n","Epoch 43/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5427 - mse: 4.2576 \n","Epoch 43: val_loss improved from 5.52765 to 5.49161, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.4847 - mse: 4.2006 - val_loss: 5.4916 - val_mse: 0.2193\n","Epoch 44/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1983 - mse: 3.9291 \n","Epoch 44: val_loss improved from 5.49161 to 5.45722, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.1685 - mse: 3.9003 - val_loss: 5.4572 - val_mse: 0.2016\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5075 - mse: 4.2549 \n","Epoch 45: val_loss improved from 5.45722 to 5.45486, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5190 - mse: 4.2675 - val_loss: 5.4549 - val_mse: 0.2160\n","Epoch 46/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9443 - mse: 3.7088 \n","Epoch 46: val_loss improved from 5.45486 to 5.40670, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9436 - mse: 3.7091 - val_loss: 5.4067 - val_mse: 0.1855\n","Epoch 47/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5474 - mse: 4.3296 \n","Epoch 47: val_loss improved from 5.40670 to 5.38643, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5854 - mse: 4.3685 - val_loss: 5.3864 - val_mse: 0.1827\n","Epoch 48/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5965 - mse: 3.3961 \n","Epoch 48: val_loss did not improve from 5.38643\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6680 - mse: 3.4686 - val_loss: 5.4034 - val_mse: 0.2175\n","Epoch 49/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7383 - mse: 3.5558 \n","Epoch 49: val_loss improved from 5.38643 to 5.35821, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7792 - mse: 3.5977 - val_loss: 5.3582 - val_mse: 0.1899\n","Epoch 50/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5767 - mse: 3.4121 \n","Epoch 50: val_loss improved from 5.35821 to 5.33854, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5627 - mse: 3.3989 - val_loss: 5.3385 - val_mse: 0.1888\n","Epoch 51/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7038 - mse: 3.5572 \n","Epoch 51: val_loss improved from 5.33854 to 5.27145, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7939 - mse: 3.6488 - val_loss: 5.2714 - val_mse: 0.1407\n","Epoch 52/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7106 - mse: 3.5833  \n","Epoch 52: val_loss improved from 5.27145 to 5.25533, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6636 - mse: 3.5375 - val_loss: 5.2553 - val_mse: 0.1437\n","Epoch 53/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9575 - mse: 3.8495 \n","Epoch 53: val_loss did not improve from 5.25533\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.8717 - mse: 3.7647 - val_loss: 5.2708 - val_mse: 0.1780\n","Epoch 54/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0414 - mse: 3.9521 \n","Epoch 54: val_loss improved from 5.25533 to 5.20475, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9615 - mse: 3.8735 - val_loss: 5.2047 - val_mse: 0.1316\n","Epoch 55/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3924 - mse: 3.3231 \n","Epoch 55: val_loss improved from 5.20475 to 5.19456, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.4115 - mse: 3.3433 - val_loss: 5.1946 - val_mse: 0.1414\n","Epoch 56/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3661 - mse: 3.3167 \n","Epoch 56: val_loss improved from 5.19456 to 5.18873, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.3940 - mse: 3.3458 - val_loss: 5.1887 - val_mse: 0.1557\n","Epoch 57/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3397 - mse: 3.3105 \n","Epoch 57: val_loss improved from 5.18873 to 5.15845, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3097 - mse: 3.2816 - val_loss: 5.1585 - val_mse: 0.1456\n","Epoch 58/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1650 - mse: 3.1562 \n","Epoch 58: val_loss improved from 5.15845 to 5.15418, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2190 - mse: 3.2113 - val_loss: 5.1542 - val_mse: 0.1621\n","Epoch 59/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8887 - mse: 3.9007 \n","Epoch 59: val_loss improved from 5.15418 to 5.09189, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7991 - mse: 3.8120 - val_loss: 5.0919 - val_mse: 0.1205\n","Epoch 60/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0103 - mse: 3.0427 \n","Epoch 60: val_loss improved from 5.09189 to 5.06318, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0073 - mse: 3.0413 - val_loss: 5.0632 - val_mse: 0.1137\n","Epoch 61/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9364 - mse: 2.9912 \n","Epoch 61: val_loss improved from 5.06318 to 5.04612, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9505 - mse: 3.0065 - val_loss: 5.0461 - val_mse: 0.1185\n","Epoch 62/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1626 - mse: 3.2395 \n","Epoch 62: val_loss improved from 5.04612 to 5.02270, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1463 - mse: 3.2242 - val_loss: 5.0227 - val_mse: 0.1173\n","Epoch 63/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9959 - mse: 3.0947 \n","Epoch 63: val_loss improved from 5.02270 to 4.98946, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0034 - mse: 3.1034 - val_loss: 4.9895 - val_mse: 0.1061\n","Epoch 64/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1484 - mse: 3.2696 \n","Epoch 64: val_loss improved from 4.98946 to 4.97769, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1513 - mse: 3.2735 - val_loss: 4.9777 - val_mse: 0.1169\n","Epoch 65/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3263 - mse: 3.4701 \n","Epoch 65: val_loss improved from 4.97769 to 4.94402, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2843 - mse: 3.4290 - val_loss: 4.9440 - val_mse: 0.1056\n","Epoch 66/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1456 - mse: 3.3117 \n","Epoch 66: val_loss improved from 4.94402 to 4.91745, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1333 - mse: 3.3005 - val_loss: 4.9174 - val_mse: 0.1018\n","Epoch 67/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0007 - mse: 3.1891 \n","Epoch 67: val_loss improved from 4.91745 to 4.88479, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0091 - mse: 3.1990 - val_loss: 4.8848 - val_mse: 0.0917\n","Epoch 68/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9431 - mse: 3.1539 \n","Epoch 68: val_loss improved from 4.88479 to 4.86526, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.8264 - mse: 3.0389 - val_loss: 4.8653 - val_mse: 0.0952\n","Epoch 69/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7748 - mse: 3.0093 \n","Epoch 69: val_loss improved from 4.86526 to 4.84919, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7598 - mse: 2.9957 - val_loss: 4.8492 - val_mse: 0.1028\n","Epoch 70/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4676 - mse: 2.7259 \n","Epoch 70: val_loss improved from 4.84919 to 4.82795, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4954 - mse: 2.7551 - val_loss: 4.8280 - val_mse: 0.1060\n","Epoch 71/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0531 - mse: 2.3360 \n","Epoch 71: val_loss improved from 4.82795 to 4.79638, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0996 - mse: 2.3839 - val_loss: 4.7964 - val_mse: 0.0993\n","Epoch 72/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1911 - mse: 2.4987 \n","Epoch 72: val_loss improved from 4.79638 to 4.77325, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1939 - mse: 2.5028 - val_loss: 4.7733 - val_mse: 0.1008\n","Epoch 73/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6865 - mse: 3.0188 \n","Epoch 73: val_loss improved from 4.77325 to 4.74504, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6580 - mse: 2.9918 - val_loss: 4.7450 - val_mse: 0.0977\n","Epoch 74/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6334 - mse: 2.9909 \n","Epoch 74: val_loss improved from 4.74504 to 4.70856, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5729 - mse: 2.9318 - val_loss: 4.7086 - val_mse: 0.0866\n","Epoch 75/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5190 - mse: 2.9017 \n","Epoch 75: val_loss improved from 4.70856 to 4.69286, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5030 - mse: 2.8874 - val_loss: 4.6929 - val_mse: 0.0962\n","Epoch 76/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1237 - mse: 2.5314 \n","Epoch 76: val_loss improved from 4.69286 to 4.65915, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1559 - mse: 2.5656 - val_loss: 4.6591 - val_mse: 0.0881\n","Epoch 77/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2428 - mse: 2.6764 \n","Epoch 77: val_loss improved from 4.65915 to 4.63823, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2664 - mse: 2.7017 - val_loss: 4.6382 - val_mse: 0.0926\n","Epoch 78/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0472 - mse: 2.5065 \n","Epoch 78: val_loss improved from 4.63823 to 4.61531, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0738 - mse: 2.5346 - val_loss: 4.6153 - val_mse: 0.0955\n","Epoch 79/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7925 - mse: 2.2778 \n","Epoch 79: val_loss improved from 4.61531 to 4.56622, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8094 - mse: 2.2962 - val_loss: 4.5662 - val_mse: 0.0729\n","Epoch 80/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0474 - mse: 2.5592 \n","Epoch 80: val_loss improved from 4.56622 to 4.54795, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0388 - mse: 2.5521 - val_loss: 4.5479 - val_mse: 0.0811\n","Epoch 81/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8178 - mse: 2.3563 \n","Epoch 81: val_loss improved from 4.54795 to 4.52064, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8322 - mse: 2.3720 - val_loss: 4.5206 - val_mse: 0.0804\n","Epoch 82/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1986 - mse: 2.7638 \n","Epoch 82: val_loss improved from 4.52064 to 4.48695, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1540 - mse: 2.7204 - val_loss: 4.4869 - val_mse: 0.0733\n","Epoch 83/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8171 - mse: 2.4088 \n","Epoch 83: val_loss improved from 4.48695 to 4.45687, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8392 - mse: 2.4323 - val_loss: 4.4569 - val_mse: 0.0700\n","Epoch 84/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7500 - mse: 2.3683 \n","Epoch 84: val_loss improved from 4.45687 to 4.42955, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7862 - mse: 2.4061 - val_loss: 4.4296 - val_mse: 0.0697\n","Epoch 85/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8516 - mse: 2.4964 \n","Epoch 85: val_loss improved from 4.42955 to 4.41487, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7973 - mse: 2.4442 - val_loss: 4.4149 - val_mse: 0.0822\n","Epoch 86/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6979 - mse: 2.3702 \n","Epoch 86: val_loss improved from 4.41487 to 4.39185, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7096 - mse: 2.3837 - val_loss: 4.3918 - val_mse: 0.0865\n","Epoch 87/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7145 - mse: 2.4144 \n","Epoch 87: val_loss improved from 4.39185 to 4.36133, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6738 - mse: 2.3752 - val_loss: 4.3613 - val_mse: 0.0835\n","Epoch 88/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9901 - mse: 2.7177 \n","Epoch 88: val_loss improved from 4.36133 to 4.31894, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.9716 - mse: 2.7008 - val_loss: 4.3189 - val_mse: 0.0692\n","Epoch 89/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0523 - mse: 2.8080 \n","Epoch 89: val_loss improved from 4.31894 to 4.29131, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0360 - mse: 2.7932 - val_loss: 4.2913 - val_mse: 0.0696\n","Epoch 90/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2261 - mse: 2.0097 \n","Epoch 90: val_loss improved from 4.29131 to 4.25946, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2908 - mse: 2.0761 - val_loss: 4.2595 - val_mse: 0.0661\n","Epoch 91/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6982 - mse: 2.5103 \n","Epoch 91: val_loss improved from 4.25946 to 4.24171, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6550 - mse: 2.4687 - val_loss: 4.2417 - val_mse: 0.0768\n","Epoch 92/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4804 - mse: 2.3210 \n","Epoch 92: val_loss improved from 4.24171 to 4.19684, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4931 - mse: 2.3353 - val_loss: 4.1968 - val_mse: 0.0608\n","Epoch 93/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3856 - mse: 2.2542 \n","Epoch 93: val_loss improved from 4.19684 to 4.16796, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4152 - mse: 2.2862 - val_loss: 4.1680 - val_mse: 0.0605\n","Epoch 94/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3029 - mse: 2.2004 \n","Epoch 94: val_loss improved from 4.16796 to 4.15124, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3616 - mse: 2.2613 - val_loss: 4.1512 - val_mse: 0.0729\n","Epoch 95/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4900 - mse: 2.4174 \n","Epoch 95: val_loss did not improve from 4.15124\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.5026 - mse: 2.4316 - val_loss: 4.1545 - val_mse: 0.1055\n","Epoch 96/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1204 - mse: 2.0771 \n","Epoch 96: val_loss improved from 4.15124 to 4.09692, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1309 - mse: 2.0893 - val_loss: 4.0969 - val_mse: 0.0775\n","Epoch 97/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2251 - mse: 2.2114 \n","Epoch 97: val_loss improved from 4.09692 to 4.05250, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2023 - mse: 2.1903 - val_loss: 4.0525 - val_mse: 0.0628\n","Epoch 98/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4625 - mse: 2.4785 \n","Epoch 98: val_loss improved from 4.05250 to 4.02983, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4710 - mse: 2.4887 - val_loss: 4.0298 - val_mse: 0.0699\n","Epoch 99/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0919 - mse: 2.1380 \n","Epoch 99: val_loss improved from 4.02983 to 4.01560, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0889 - mse: 2.1365 - val_loss: 4.0156 - val_mse: 0.0858\n","Epoch 100/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1429 - mse: 2.2189 \n","Epoch 100: val_loss improved from 4.01560 to 3.98453, saving model to best_model_Adam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1549 - mse: 2.2326 - val_loss: 3.9845 - val_mse: 0.0847\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: Adam, Loss Function: mse, Epochs: 100, Test MSE: 0.10457430003660123\n","Training with optimizer: Adam, loss function: mae, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - loss: 17.3799 - mse: 146.3107\n","Epoch 1: val_loss improved from inf to 16.81006, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 333ms/step - loss: 17.3742 - mse: 146.2840 - val_loss: 16.8101 - val_mse: 137.7882\n","Epoch 2/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.8045 - mse: 142.5959 \n","Epoch 2: val_loss improved from 16.81006 to 16.28989, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.7652 - mse: 142.1354 - val_loss: 16.2899 - val_mse: 132.5209\n","Epoch 3/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.1964 - mse: 134.6347 \n","Epoch 3: val_loss improved from 16.28989 to 15.83881, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.1858 - mse: 134.7323 - val_loss: 15.8388 - val_mse: 127.3616\n","Epoch 4/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.8161 - mse: 131.0246 \n","Epoch 4: val_loss improved from 15.83881 to 15.33036, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.7939 - mse: 130.8584 - val_loss: 15.3304 - val_mse: 120.2635\n","Epoch 5/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.3807 - mse: 126.0632 \n","Epoch 5: val_loss improved from 15.33036 to 14.89948, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.3467 - mse: 125.7071 - val_loss: 14.8995 - val_mse: 114.5664\n","Epoch 6/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.9128 - mse: 120.3055 \n","Epoch 6: val_loss improved from 14.89948 to 14.51567, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.8821 - mse: 119.8780 - val_loss: 14.5157 - val_mse: 109.7694\n","Epoch 7/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.2079 - mse: 108.9179 \n","Epoch 7: val_loss improved from 14.51567 to 13.86857, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.1818 - mse: 108.5775 - val_loss: 13.8686 - val_mse: 99.2795\n","Epoch 8/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.6094 - mse: 100.8464 \n","Epoch 8: val_loss improved from 13.86857 to 13.18380, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.5748 - mse: 100.3855 - val_loss: 13.1838 - val_mse: 88.9502\n","Epoch 9/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.0299 - mse: 92.5437  \n","Epoch 9: val_loss improved from 13.18380 to 12.33907, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.9667 - mse: 91.6174 - val_loss: 12.3391 - val_mse: 76.2388\n","Epoch 10/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.2143 - mse: 81.0620 \n","Epoch 10: val_loss improved from 12.33907 to 11.53393, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.1498 - mse: 80.2340 - val_loss: 11.5339 - val_mse: 64.4908\n","Epoch 11/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.2550 - mse: 68.0798 \n","Epoch 11: val_loss improved from 11.53393 to 10.47489, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.1925 - mse: 67.2558 - val_loss: 10.4749 - val_mse: 50.6394\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3061 - mse: 55.3982 \n","Epoch 12: val_loss improved from 10.47489 to 9.11086, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.2277 - mse: 54.4546 - val_loss: 9.1109 - val_mse: 37.3101\n","Epoch 13/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3232 - mse: 45.2679 \n","Epoch 13: val_loss improved from 9.11086 to 7.90706, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2535 - mse: 44.2656 - val_loss: 7.9071 - val_mse: 24.7169\n","Epoch 14/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0806 - mse: 30.5636 \n","Epoch 14: val_loss improved from 7.90706 to 6.75330, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0309 - mse: 30.3059 - val_loss: 6.7533 - val_mse: 15.1061\n","Epoch 15/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1435 - mse: 20.5321 \n","Epoch 15: val_loss improved from 6.75330 to 6.03433, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.0989 - mse: 20.3170 - val_loss: 6.0343 - val_mse: 11.8612\n","Epoch 16/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3275 - mse: 15.3039 \n","Epoch 16: val_loss improved from 6.03433 to 5.02414, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3045 - mse: 15.2655 - val_loss: 5.0241 - val_mse: 5.0911\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0858 - mse: 12.9680 \n","Epoch 17: val_loss improved from 5.02414 to 4.35214, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0328 - mse: 12.4949 - val_loss: 4.3521 - val_mse: 2.7312\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5334 - mse: 8.6728 \n","Epoch 18: val_loss improved from 4.35214 to 4.07599, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5318 - mse: 8.7415 - val_loss: 4.0760 - val_mse: 3.5758\n","Epoch 19/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4025 - mse: 8.5968 \n","Epoch 19: val_loss improved from 4.07599 to 3.83520, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4030 - mse: 8.6180 - val_loss: 3.8352 - val_mse: 1.3661\n","Epoch 20/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1564 - mse: 7.4458 \n","Epoch 20: val_loss improved from 3.83520 to 3.73236, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1639 - mse: 7.5519 - val_loss: 3.7324 - val_mse: 0.8933\n","Epoch 21/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0584 - mse: 7.3071 \n","Epoch 21: val_loss improved from 3.73236 to 3.58105, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0573 - mse: 7.3077 - val_loss: 3.5811 - val_mse: 0.5659\n","Epoch 22/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0794 - mse: 8.4000  \n","Epoch 22: val_loss did not improve from 3.58105\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0383 - mse: 8.0716 - val_loss: 3.6697 - val_mse: 2.4990\n","Epoch 23/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8063 - mse: 6.1742 \n","Epoch 23: val_loss improved from 3.58105 to 3.38925, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7843 - mse: 6.0438 - val_loss: 3.3893 - val_mse: 0.4205\n","Epoch 24/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6849 - mse: 5.7753 \n","Epoch 24: val_loss improved from 3.38925 to 3.30691, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6976 - mse: 5.9015 - val_loss: 3.3069 - val_mse: 0.3268\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7715 - mse: 6.5118  \n","Epoch 25: val_loss improved from 3.30691 to 3.23915, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7438 - mse: 6.3690 - val_loss: 3.2392 - val_mse: 0.3696\n","Epoch 26/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6106 - mse: 6.6314  \n","Epoch 26: val_loss improved from 3.23915 to 3.16361, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6022 - mse: 6.5389 - val_loss: 3.1636 - val_mse: 0.3329\n","Epoch 27/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5117 - mse: 6.2889 \n","Epoch 27: val_loss improved from 3.16361 to 3.07344, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5036 - mse: 6.2681 - val_loss: 3.0734 - val_mse: 0.3842\n","Epoch 28/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4195 - mse: 5.9937 \n","Epoch 28: val_loss improved from 3.07344 to 2.98210, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4121 - mse: 5.9561 - val_loss: 2.9821 - val_mse: 0.4972\n","Epoch 29/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1394 - mse: 4.3015 \n","Epoch 29: val_loss improved from 2.98210 to 2.85750, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1522 - mse: 4.4740 - val_loss: 2.8575 - val_mse: 0.1695\n","Epoch 30/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2111 - mse: 5.7949 \n","Epoch 30: val_loss improved from 2.85750 to 2.81918, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1925 - mse: 5.6890 - val_loss: 2.8192 - val_mse: 0.2801\n","Epoch 31/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1503 - mse: 6.6189  \n","Epoch 31: val_loss improved from 2.81918 to 2.78450, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.1278 - mse: 6.2146 - val_loss: 2.7845 - val_mse: 0.2395\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0665 - mse: 5.6766 \n","Epoch 32: val_loss improved from 2.78450 to 2.66233, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0405 - mse: 5.5183 - val_loss: 2.6623 - val_mse: 0.1357\n","Epoch 33/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8606 - mse: 4.8355 \n","Epoch 33: val_loss improved from 2.66233 to 2.65279, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8528 - mse: 4.7732 - val_loss: 2.6528 - val_mse: 0.5390\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8452 - mse: 5.7443  \n","Epoch 34: val_loss did not improve from 2.65279\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8287 - mse: 5.4394 - val_loss: 2.7357 - val_mse: 0.8349\n","Epoch 35/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7195 - mse: 4.0614 \n","Epoch 35: val_loss did not improve from 2.65279\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7208 - mse: 4.1660 - val_loss: 2.6543 - val_mse: 1.1003\n","Epoch 36/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6282 - mse: 3.6022 \n","Epoch 36: val_loss improved from 2.65279 to 2.58809, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6253 - mse: 3.7119 - val_loss: 2.5881 - val_mse: 0.9028\n","Epoch 37/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5873 - mse: 5.1815 \n","Epoch 37: val_loss improved from 2.58809 to 2.42878, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5890 - mse: 5.2630 - val_loss: 2.4288 - val_mse: 0.1868\n","Epoch 38/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4579 - mse: 3.6631 \n","Epoch 38: val_loss improved from 2.42878 to 2.41551, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4621 - mse: 3.8029 - val_loss: 2.4155 - val_mse: 0.2230\n","Epoch 39/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3995 - mse: 3.4071 \n","Epoch 39: val_loss improved from 2.41551 to 2.34150, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4153 - mse: 3.6586 - val_loss: 2.3415 - val_mse: 0.2561\n","Epoch 40/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2856 - mse: 3.8347 \n","Epoch 40: val_loss improved from 2.34150 to 2.27074, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3065 - mse: 4.1000 - val_loss: 2.2707 - val_mse: 0.2895\n","Epoch 41/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2432 - mse: 4.3644 \n","Epoch 41: val_loss improved from 2.27074 to 2.07691, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2575 - mse: 4.4995 - val_loss: 2.0769 - val_mse: 0.1005\n","Epoch 42/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2841 - mse: 4.6199 \n","Epoch 42: val_loss did not improve from 2.07691\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2719 - mse: 4.6039 - val_loss: 2.1088 - val_mse: 0.2537\n","Epoch 43/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1615 - mse: 4.6283 \n","Epoch 43: val_loss improved from 2.07691 to 1.95914, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1543 - mse: 4.5454 - val_loss: 1.9591 - val_mse: 0.0892\n","Epoch 44/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0607 - mse: 3.3392 \n","Epoch 44: val_loss improved from 1.95914 to 1.95177, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0584 - mse: 3.3481 - val_loss: 1.9518 - val_mse: 0.1123\n","Epoch 45/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0133 - mse: 3.7660 \n","Epoch 45: val_loss improved from 1.95177 to 1.90646, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0225 - mse: 3.9885 - val_loss: 1.9065 - val_mse: 0.1138\n","Epoch 46/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9670 - mse: 3.9926 \n","Epoch 46: val_loss improved from 1.90646 to 1.82480, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9617 - mse: 3.9799 - val_loss: 1.8248 - val_mse: 0.1211\n","Epoch 47/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0162 - mse: 4.1479 \n","Epoch 47: val_loss improved from 1.82480 to 1.78306, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0174 - mse: 4.1433 - val_loss: 1.7831 - val_mse: 0.1574\n","Epoch 48/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9094 - mse: 4.6839  \n","Epoch 48: val_loss improved from 1.78306 to 1.73784, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8832 - mse: 4.4458 - val_loss: 1.7378 - val_mse: 0.1066\n","Epoch 49/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8250 - mse: 3.8028 \n","Epoch 49: val_loss improved from 1.73784 to 1.72934, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8330 - mse: 4.0071 - val_loss: 1.7293 - val_mse: 0.1356\n","Epoch 50/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7003 - mse: 3.4326 \n","Epoch 50: val_loss improved from 1.72934 to 1.60695, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6954 - mse: 3.4321 - val_loss: 1.6070 - val_mse: 0.0840\n","Epoch 51/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6529 - mse: 3.0247 \n","Epoch 51: val_loss improved from 1.60695 to 1.59662, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6570 - mse: 3.2160 - val_loss: 1.5966 - val_mse: 0.1004\n","Epoch 52/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5804 - mse: 3.2234 \n","Epoch 52: val_loss did not improve from 1.59662\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5760 - mse: 3.2744 - val_loss: 1.6304 - val_mse: 0.1532\n","Epoch 53/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5334 - mse: 3.5660 \n","Epoch 53: val_loss improved from 1.59662 to 1.56622, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5348 - mse: 3.5832 - val_loss: 1.5662 - val_mse: 0.1442\n","Epoch 54/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5809 - mse: 3.7130 \n","Epoch 54: val_loss improved from 1.56622 to 1.45185, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5798 - mse: 3.7059 - val_loss: 1.4518 - val_mse: 0.1579\n","Epoch 55/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5068 - mse: 3.4106 \n","Epoch 55: val_loss improved from 1.45185 to 1.42078, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4973 - mse: 3.3539 - val_loss: 1.4208 - val_mse: 0.1509\n","Epoch 56/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4528 - mse: 3.5210 \n","Epoch 56: val_loss improved from 1.42078 to 1.41189, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4546 - mse: 3.5950 - val_loss: 1.4119 - val_mse: 0.1701\n","Epoch 57/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4206 - mse: 3.1361 \n","Epoch 57: val_loss improved from 1.41189 to 1.35869, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4051 - mse: 3.1011 - val_loss: 1.3587 - val_mse: 0.1164\n","Epoch 58/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3595 - mse: 3.9377 \n","Epoch 58: val_loss improved from 1.35869 to 1.26638, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3556 - mse: 3.8326 - val_loss: 1.2664 - val_mse: 0.0876\n","Epoch 59/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3590 - mse: 3.4103 \n","Epoch 59: val_loss improved from 1.26638 to 1.25418, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3458 - mse: 3.4119 - val_loss: 1.2542 - val_mse: 0.1168\n","Epoch 60/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2730 - mse: 3.9284 \n","Epoch 60: val_loss improved from 1.25418 to 1.24531, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2570 - mse: 3.7470 - val_loss: 1.2453 - val_mse: 0.1394\n","Epoch 61/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2381 - mse: 3.6191 \n","Epoch 61: val_loss improved from 1.24531 to 1.16830, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2370 - mse: 3.6158 - val_loss: 1.1683 - val_mse: 0.0932\n","Epoch 62/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2380 - mse: 3.3522 \n","Epoch 62: val_loss improved from 1.16830 to 1.14811, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2349 - mse: 3.4693 - val_loss: 1.1481 - val_mse: 0.0984\n","Epoch 63/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1881 - mse: 3.8864 \n","Epoch 63: val_loss did not improve from 1.14811\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1847 - mse: 3.8378 - val_loss: 1.1628 - val_mse: 0.1295\n","Epoch 64/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1478 - mse: 3.3194 \n","Epoch 64: val_loss improved from 1.14811 to 1.14790, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1391 - mse: 3.2736 - val_loss: 1.1479 - val_mse: 0.1321\n","Epoch 65/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1959 - mse: 3.8512 \n","Epoch 65: val_loss improved from 1.14790 to 1.06439, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1697 - mse: 3.7842 - val_loss: 1.0644 - val_mse: 0.0982\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1236 - mse: 3.4810 \n","Epoch 66: val_loss improved from 1.06439 to 1.01166, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1177 - mse: 3.4596 - val_loss: 1.0117 - val_mse: 0.0872\n","Epoch 67/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0383 - mse: 3.2910 \n","Epoch 67: val_loss improved from 1.01166 to 1.00289, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0424 - mse: 3.4627 - val_loss: 1.0029 - val_mse: 0.1041\n","Epoch 68/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8469 - mse: 2.4413 \n","Epoch 68: val_loss improved from 1.00289 to 0.96881, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8527 - mse: 2.5148 - val_loss: 0.9688 - val_mse: 0.0994\n","Epoch 69/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9877 - mse: 3.3422 \n","Epoch 69: val_loss improved from 0.96881 to 0.95854, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9731 - mse: 3.2993 - val_loss: 0.9585 - val_mse: 0.1317\n","Epoch 70/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9975 - mse: 3.7578 \n","Epoch 70: val_loss improved from 0.95854 to 0.94734, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9986 - mse: 3.7659 - val_loss: 0.9473 - val_mse: 0.1274\n","Epoch 71/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8409 - mse: 2.4967 \n","Epoch 71: val_loss improved from 0.94734 to 0.85362, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8424 - mse: 2.5725 - val_loss: 0.8536 - val_mse: 0.0736\n","Epoch 72/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8737 - mse: 3.0822 \n","Epoch 72: val_loss did not improve from 0.85362\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8672 - mse: 3.0599 - val_loss: 0.8741 - val_mse: 0.0966\n","Epoch 73/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8514 - mse: 3.1599 \n","Epoch 73: val_loss improved from 0.85362 to 0.80347, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8445 - mse: 3.1571 - val_loss: 0.8035 - val_mse: 0.0720\n","Epoch 74/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8597 - mse: 4.4910  \n","Epoch 74: val_loss did not improve from 0.80347\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8420 - mse: 4.1926 - val_loss: 0.8609 - val_mse: 0.1227\n","Epoch 75/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8781 - mse: 3.9791 \n","Epoch 75: val_loss did not improve from 0.80347\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8613 - mse: 3.7930 - val_loss: 0.8796 - val_mse: 0.1583\n","Epoch 76/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8058 - mse: 3.9287 \n","Epoch 76: val_loss improved from 0.80347 to 0.76491, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7959 - mse: 3.8165 - val_loss: 0.7649 - val_mse: 0.0832\n","Epoch 77/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8645 - mse: 3.7668 \n","Epoch 77: val_loss improved from 0.76491 to 0.75564, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8486 - mse: 3.6722 - val_loss: 0.7556 - val_mse: 0.0954\n","Epoch 78/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6819 - mse: 2.7263 \n","Epoch 78: val_loss did not improve from 0.75564\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7023 - mse: 2.8350 - val_loss: 0.7599 - val_mse: 0.1177\n","Epoch 79/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6991 - mse: 2.9814 \n","Epoch 79: val_loss improved from 0.75564 to 0.71188, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7048 - mse: 3.0462 - val_loss: 0.7119 - val_mse: 0.1273\n","Epoch 80/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7352 - mse: 3.5322 \n","Epoch 80: val_loss did not improve from 0.71188\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7290 - mse: 3.4810 - val_loss: 0.7400 - val_mse: 0.2073\n","Epoch 81/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5317 - mse: 2.2993 \n","Epoch 81: val_loss improved from 0.71188 to 0.68803, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5409 - mse: 2.3467 - val_loss: 0.6880 - val_mse: 0.1345\n","Epoch 82/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7164 - mse: 3.4532 \n","Epoch 82: val_loss improved from 0.68803 to 0.61572, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6998 - mse: 3.3840 - val_loss: 0.6157 - val_mse: 0.0704\n","Epoch 83/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5968 - mse: 2.6008 \n","Epoch 83: val_loss did not improve from 0.61572\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6078 - mse: 2.6806 - val_loss: 0.6512 - val_mse: 0.1255\n","Epoch 84/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6426 - mse: 3.2633 \n","Epoch 84: val_loss improved from 0.61572 to 0.60306, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6450 - mse: 3.2815 - val_loss: 0.6031 - val_mse: 0.0851\n","Epoch 85/100\n","\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6726 - mse: 4.4524 \n","Epoch 85: val_loss did not improve from 0.60306\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6273 - mse: 3.9667 - val_loss: 0.6555 - val_mse: 0.1241\n","Epoch 86/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5171 - mse: 2.8170 \n","Epoch 86: val_loss did not improve from 0.60306\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5235 - mse: 2.8158 - val_loss: 0.6052 - val_mse: 0.1084\n","Epoch 87/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5559 - mse: 3.0269 \n","Epoch 87: val_loss improved from 0.60306 to 0.58898, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5442 - mse: 2.9708 - val_loss: 0.5890 - val_mse: 0.1428\n","Epoch 88/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5497 - mse: 2.8901 \n","Epoch 88: val_loss improved from 0.58898 to 0.55450, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5555 - mse: 3.0061 - val_loss: 0.5545 - val_mse: 0.0977\n","Epoch 89/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5602 - mse: 2.7782 \n","Epoch 89: val_loss did not improve from 0.55450\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5559 - mse: 2.7748 - val_loss: 0.5753 - val_mse: 0.1266\n","Epoch 90/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4138 - mse: 2.2801 \n","Epoch 90: val_loss improved from 0.55450 to 0.55130, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4256 - mse: 2.2996 - val_loss: 0.5513 - val_mse: 0.1028\n","Epoch 91/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4993 - mse: 2.6152 \n","Epoch 91: val_loss did not improve from 0.55130\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4848 - mse: 2.5704 - val_loss: 0.5748 - val_mse: 0.1212\n","Epoch 92/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5315 - mse: 3.6660 \n","Epoch 92: val_loss improved from 0.55130 to 0.49213, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5310 - mse: 3.6299 - val_loss: 0.4921 - val_mse: 0.0765\n","Epoch 93/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4232 - mse: 2.6020 \n","Epoch 93: val_loss did not improve from 0.49213\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4266 - mse: 2.6165 - val_loss: 0.5256 - val_mse: 0.1364\n","Epoch 94/100\n","\u001b[1m14/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4379 - mse: 2.9749 \n","Epoch 94: val_loss did not improve from 0.49213\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4629 - mse: 2.8818 - val_loss: 0.5526 - val_mse: 0.2080\n","Epoch 95/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5017 - mse: 3.5877 \n","Epoch 95: val_loss did not improve from 0.49213\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4916 - mse: 3.4127 - val_loss: 0.6098 - val_mse: 0.3248\n","Epoch 96/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4372 - mse: 2.7785 \n","Epoch 96: val_loss did not improve from 0.49213\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4344 - mse: 2.7842 - val_loss: 0.5302 - val_mse: 0.2525\n","Epoch 97/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4589 - mse: 2.8385 \n","Epoch 97: val_loss improved from 0.49213 to 0.45404, saving model to best_model_Adam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4589 - mse: 2.9474 - val_loss: 0.4540 - val_mse: 0.1408\n","Epoch 98/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4495 - mse: 3.0578 \n","Epoch 98: val_loss did not improve from 0.45404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4569 - mse: 3.1034 - val_loss: 0.5217 - val_mse: 0.1818\n","Epoch 99/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3457 - mse: 2.3495 \n","Epoch 99: val_loss did not improve from 0.45404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3531 - mse: 2.4726 - val_loss: 0.4735 - val_mse: 0.0977\n","Epoch 100/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4497 - mse: 3.0407 \n","Epoch 100: val_loss did not improve from 0.45404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4450 - mse: 3.0537 - val_loss: 0.5104 - val_mse: 0.1337\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: Adam, Loss Function: mae, Epochs: 100, Test MSE: 0.2586397978711276\n","Training with optimizer: Adam, loss function: huber, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - loss: 16.8799 - mse: 146.3107\n","Epoch 1: val_loss improved from inf to 16.31006, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 342ms/step - loss: 16.8742 - mse: 146.2840 - val_loss: 16.3101 - val_mse: 137.7882\n","Epoch 2/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.3107 - mse: 142.6705 \n","Epoch 2: val_loss improved from 16.31006 to 15.78989, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.2652 - mse: 142.1354 - val_loss: 15.7899 - val_mse: 132.5209\n","Epoch 3/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.6964 - mse: 134.6347 \n","Epoch 3: val_loss improved from 15.78989 to 15.33881, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.6858 - mse: 134.7323 - val_loss: 15.3388 - val_mse: 127.3616\n","Epoch 4/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.3132 - mse: 131.0054 \n","Epoch 4: val_loss improved from 15.33881 to 14.83036, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.2939 - mse: 130.8584 - val_loss: 14.8304 - val_mse: 120.2635\n","Epoch 5/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.8698 - mse: 125.9620 \n","Epoch 5: val_loss improved from 14.83036 to 14.39948, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.8467 - mse: 125.7071 - val_loss: 14.3995 - val_mse: 114.5664\n","Epoch 6/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.4024 - mse: 120.1638 \n","Epoch 6: val_loss improved from 14.39948 to 14.01567, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.3821 - mse: 119.8780 - val_loss: 14.0157 - val_mse: 109.7694\n","Epoch 7/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.7079 - mse: 108.9179 \n","Epoch 7: val_loss improved from 14.01567 to 13.36857, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.6818 - mse: 108.5775 - val_loss: 13.3686 - val_mse: 99.2795\n","Epoch 8/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.1046 - mse: 100.7934 \n","Epoch 8: val_loss improved from 13.36857 to 12.68380, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.0748 - mse: 100.3855 - val_loss: 12.6838 - val_mse: 88.9502\n","Epoch 9/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.5425 - mse: 92.7361  \n","Epoch 9: val_loss improved from 12.68380 to 11.85933, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.4677 - mse: 91.6219 - val_loss: 11.8593 - val_mse: 76.5679\n","Epoch 10/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.7138 - mse: 80.8472 \n","Epoch 10: val_loss improved from 11.85933 to 10.99089, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.6420 - mse: 79.9692 - val_loss: 10.9909 - val_mse: 63.7043\n","Epoch 11/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7580 - mse: 67.9788 \n","Epoch 11: val_loss improved from 10.99089 to 9.86938, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.6784 - mse: 66.8635 - val_loss: 9.8694 - val_mse: 49.8121\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.7444 - mse: 54.8921  \n","Epoch 12: val_loss improved from 9.86938 to 8.77168, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.6793 - mse: 54.1887 - val_loss: 8.7717 - val_mse: 40.8023\n","Epoch 13/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7838 - mse: 44.7734 \n","Epoch 13: val_loss improved from 8.77168 to 7.56109, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7010 - mse: 43.5732 - val_loss: 7.5611 - val_mse: 28.7018\n","Epoch 14/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6588 - mse: 31.9469 \n","Epoch 14: val_loss improved from 7.56109 to 6.50113, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6067 - mse: 31.4907 - val_loss: 6.5011 - val_mse: 20.1619\n","Epoch 15/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6873 - mse: 21.8934 \n","Epoch 15: val_loss improved from 6.50113 to 5.65420, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6502 - mse: 21.6584 - val_loss: 5.6542 - val_mse: 11.8777\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6509 - mse: 12.8024\n","Epoch 16: val_loss improved from 5.65420 to 4.63386, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6482 - mse: 12.9075 - val_loss: 4.6339 - val_mse: 8.0872\n","Epoch 17/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5728 - mse: 13.1665 \n","Epoch 17: val_loss improved from 4.63386 to 4.22938, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5318 - mse: 12.7898 - val_loss: 4.2294 - val_mse: 4.4544\n","Epoch 18/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0434 - mse: 9.4514 \n","Epoch 18: val_loss improved from 4.22938 to 3.73442, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.0392 - mse: 9.3858 - val_loss: 3.7344 - val_mse: 3.3878\n","Epoch 19/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7271 - mse: 7.6158 \n","Epoch 19: val_loss improved from 3.73442 to 3.49425, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7426 - mse: 7.6381 - val_loss: 3.4943 - val_mse: 1.5133\n","Epoch 20/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7625 - mse: 7.2823 \n","Epoch 20: val_loss improved from 3.49425 to 3.46067, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7552 - mse: 7.3419 - val_loss: 3.4607 - val_mse: 1.1701\n","Epoch 21/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6401 - mse: 7.9374 \n","Epoch 21: val_loss improved from 3.46067 to 3.31638, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6278 - mse: 7.8034 - val_loss: 3.3164 - val_mse: 0.8347\n","Epoch 22/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4959 - mse: 7.0858 \n","Epoch 22: val_loss improved from 3.31638 to 3.26259, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4847 - mse: 7.0465 - val_loss: 3.2626 - val_mse: 1.9085\n","Epoch 23/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2744 - mse: 6.2637 \n","Epoch 23: val_loss improved from 3.26259 to 3.14097, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2602 - mse: 6.1344 - val_loss: 3.1410 - val_mse: 1.4983\n","Epoch 24/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2487 - mse: 6.3795  \n","Epoch 24: val_loss improved from 3.14097 to 3.01056, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2434 - mse: 6.3339 - val_loss: 3.0106 - val_mse: 0.9031\n","Epoch 25/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2208 - mse: 7.3052  \n","Epoch 25: val_loss did not improve from 3.01056\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2029 - mse: 7.0940 - val_loss: 3.0491 - val_mse: 2.7435\n","Epoch 26/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1613 - mse: 8.9177  \n","Epoch 26: val_loss improved from 3.01056 to 2.85698, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1500 - mse: 8.7042 - val_loss: 2.8570 - val_mse: 0.5042\n","Epoch 27/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9791 - mse: 5.9563 \n","Epoch 27: val_loss improved from 2.85698 to 2.76934, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9669 - mse: 5.9717 - val_loss: 2.7693 - val_mse: 0.6208\n","Epoch 28/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9402 - mse: 6.1299  \n","Epoch 28: val_loss improved from 2.76934 to 2.61497, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9155 - mse: 5.9811 - val_loss: 2.6150 - val_mse: 0.2301\n","Epoch 29/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6276 - mse: 4.4263 \n","Epoch 29: val_loss improved from 2.61497 to 2.53810, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6346 - mse: 4.5918 - val_loss: 2.5381 - val_mse: 0.2098\n","Epoch 30/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6645 - mse: 6.0313  \n","Epoch 30: val_loss did not improve from 2.53810\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6518 - mse: 5.9454 - val_loss: 2.5574 - val_mse: 0.6652\n","Epoch 31/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4754 - mse: 5.0385  \n","Epoch 31: val_loss improved from 2.53810 to 2.40218, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4728 - mse: 4.9678 - val_loss: 2.4022 - val_mse: 0.2549\n","Epoch 32/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4642 - mse: 5.2500 \n","Epoch 32: val_loss improved from 2.40218 to 2.31938, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4462 - mse: 5.1678 - val_loss: 2.3194 - val_mse: 0.1765\n","Epoch 33/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2384 - mse: 4.6187 \n","Epoch 33: val_loss improved from 2.31938 to 2.23854, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2355 - mse: 4.5843 - val_loss: 2.2385 - val_mse: 0.1488\n","Epoch 34/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3380 - mse: 5.8004 \n","Epoch 34: val_loss improved from 2.23854 to 2.19025, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3164 - mse: 5.5615 - val_loss: 2.1903 - val_mse: 0.1769\n","Epoch 35/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1861 - mse: 4.4573 \n","Epoch 35: val_loss improved from 2.19025 to 2.11745, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1790 - mse: 4.4938 - val_loss: 2.1175 - val_mse: 0.1690\n","Epoch 36/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0126 - mse: 3.4163 \n","Epoch 36: val_loss improved from 2.11745 to 2.06046, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0079 - mse: 3.5073 - val_loss: 2.0605 - val_mse: 0.1915\n","Epoch 37/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0323 - mse: 5.7061 \n","Epoch 37: val_loss improved from 2.06046 to 2.00130, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0193 - mse: 5.6467 - val_loss: 2.0013 - val_mse: 0.1987\n","Epoch 38/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8450 - mse: 3.6989 \n","Epoch 38: val_loss improved from 2.00130 to 1.93946, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8437 - mse: 3.7530 - val_loss: 1.9395 - val_mse: 0.1854\n","Epoch 39/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8503 - mse: 4.1815 \n","Epoch 39: val_loss improved from 1.93946 to 1.86202, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8618 - mse: 4.4219 - val_loss: 1.8620 - val_mse: 0.1484\n","Epoch 40/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7546 - mse: 4.3261 \n","Epoch 40: val_loss improved from 1.86202 to 1.83016, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7606 - mse: 4.4782 - val_loss: 1.8302 - val_mse: 0.2410\n","Epoch 41/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6610 - mse: 4.3053 \n","Epoch 41: val_loss improved from 1.83016 to 1.71469, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6689 - mse: 4.3735 - val_loss: 1.7147 - val_mse: 0.0904\n","Epoch 42/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6269 - mse: 4.1061 \n","Epoch 42: val_loss improved from 1.71469 to 1.70361, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6226 - mse: 4.1002 - val_loss: 1.7036 - val_mse: 0.2090\n","Epoch 43/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6046 - mse: 4.6865 \n","Epoch 43: val_loss improved from 1.70361 to 1.60651, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5883 - mse: 4.5912 - val_loss: 1.6065 - val_mse: 0.1001\n","Epoch 44/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3620 - mse: 2.6851 \n","Epoch 44: val_loss improved from 1.60651 to 1.57021, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3748 - mse: 2.8626 - val_loss: 1.5702 - val_mse: 0.1360\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4392 - mse: 4.1673 \n","Epoch 45: val_loss improved from 1.57021 to 1.52079, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4394 - mse: 4.3530 - val_loss: 1.5208 - val_mse: 0.1439\n","Epoch 46/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3864 - mse: 4.1810 \n","Epoch 46: val_loss improved from 1.52079 to 1.45720, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3686 - mse: 4.0736 - val_loss: 1.4572 - val_mse: 0.1203\n","Epoch 47/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3596 - mse: 3.8898 \n","Epoch 47: val_loss improved from 1.45720 to 1.40652, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3613 - mse: 3.9036 - val_loss: 1.4065 - val_mse: 0.1196\n","Epoch 48/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3591 - mse: 5.3074  \n","Epoch 48: val_loss improved from 1.40652 to 1.34837, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3300 - mse: 5.0262 - val_loss: 1.3484 - val_mse: 0.0989\n","Epoch 49/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2468 - mse: 3.9859 \n","Epoch 49: val_loss improved from 1.34837 to 1.31190, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2468 - mse: 4.0731 - val_loss: 1.3119 - val_mse: 0.1210\n","Epoch 50/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0872 - mse: 3.1946 \n","Epoch 50: val_loss improved from 1.31190 to 1.25274, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0802 - mse: 3.2105 - val_loss: 1.2527 - val_mse: 0.0956\n","Epoch 51/100\n","\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0667 - mse: 3.0356 \n","Epoch 51: val_loss improved from 1.25274 to 1.20727, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.0732 - mse: 3.2755 - val_loss: 1.2073 - val_mse: 0.0949\n","Epoch 52/100\n","\u001b[1m12/26\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0038 - mse: 3.2095 \n","Epoch 52: val_loss improved from 1.20727 to 1.18215, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.9920 - mse: 3.4192 - val_loss: 1.1822 - val_mse: 0.1333\n","Epoch 53/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9352 - mse: 3.5105  \n","Epoch 53: val_loss improved from 1.18215 to 1.14110, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9463 - mse: 3.5861 - val_loss: 1.1411 - val_mse: 0.1379\n","Epoch 54/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0475 - mse: 4.0976 \n","Epoch 54: val_loss improved from 1.14110 to 1.09285, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0335 - mse: 4.0344 - val_loss: 1.0928 - val_mse: 0.1254\n","Epoch 55/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9196 - mse: 3.2048 \n","Epoch 55: val_loss improved from 1.09285 to 1.03191, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9083 - mse: 3.1545 - val_loss: 1.0319 - val_mse: 0.0813\n","Epoch 56/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8425 - mse: 3.6379 \n","Epoch 56: val_loss improved from 1.03191 to 1.01716, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8528 - mse: 3.7455 - val_loss: 1.0172 - val_mse: 0.1293\n","Epoch 57/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8180 - mse: 2.9965 \n","Epoch 57: val_loss improved from 1.01716 to 1.01381, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8094 - mse: 3.0501 - val_loss: 1.0138 - val_mse: 0.2229\n","Epoch 58/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8124 - mse: 3.8363 \n","Epoch 58: val_loss improved from 1.01381 to 0.92706, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8071 - mse: 3.7524 - val_loss: 0.9271 - val_mse: 0.0964\n","Epoch 59/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7373 - mse: 3.2411 \n","Epoch 59: val_loss improved from 0.92706 to 0.91702, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7298 - mse: 3.2473 - val_loss: 0.9170 - val_mse: 0.1577\n","Epoch 60/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7165 - mse: 4.1451  \n","Epoch 60: val_loss improved from 0.91702 to 0.88781, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7066 - mse: 3.9972 - val_loss: 0.8878 - val_mse: 0.1611\n","Epoch 61/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6307 - mse: 3.4886 \n","Epoch 61: val_loss improved from 0.88781 to 0.84705, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6331 - mse: 3.5079 - val_loss: 0.8471 - val_mse: 0.1445\n","Epoch 62/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7273 - mse: 4.3824 \n","Epoch 62: val_loss improved from 0.84705 to 0.80183, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7166 - mse: 4.3230 - val_loss: 0.8018 - val_mse: 0.1114\n","Epoch 63/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6008 - mse: 3.0880 \n","Epoch 63: val_loss improved from 0.80183 to 0.76425, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5981 - mse: 3.1478 - val_loss: 0.7642 - val_mse: 0.0961\n","Epoch 64/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5919 - mse: 3.1523 \n","Epoch 64: val_loss improved from 0.76425 to 0.75656, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5842 - mse: 3.1413 - val_loss: 0.7566 - val_mse: 0.1391\n","Epoch 65/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6865 - mse: 4.3744 \n","Epoch 65: val_loss improved from 0.75656 to 0.70608, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6594 - mse: 4.2675 - val_loss: 0.7061 - val_mse: 0.0948\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5534 - mse: 3.5568 \n","Epoch 66: val_loss improved from 0.70608 to 0.67384, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5440 - mse: 3.4819 - val_loss: 0.6738 - val_mse: 0.0840\n","Epoch 67/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5252 - mse: 3.8488 \n","Epoch 67: val_loss improved from 0.67384 to 0.65270, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5206 - mse: 3.8882 - val_loss: 0.6527 - val_mse: 0.0944\n","Epoch 68/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3189 - mse: 2.4874 \n","Epoch 68: val_loss improved from 0.65270 to 0.64026, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3212 - mse: 2.5486 - val_loss: 0.6403 - val_mse: 0.1187\n","Epoch 69/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4459 - mse: 3.5178 \n","Epoch 69: val_loss improved from 0.64026 to 0.62080, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4411 - mse: 3.5203 - val_loss: 0.6208 - val_mse: 0.1261\n","Epoch 70/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4776 - mse: 3.9237 \n","Epoch 70: val_loss improved from 0.62080 to 0.59553, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4717 - mse: 3.8420 - val_loss: 0.5955 - val_mse: 0.1222\n","Epoch 71/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2916 - mse: 2.3114 \n","Epoch 71: val_loss improved from 0.59553 to 0.56034, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2890 - mse: 2.3721 - val_loss: 0.5603 - val_mse: 0.0967\n","Epoch 72/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2835 - mse: 2.6630 \n","Epoch 72: val_loss improved from 0.56034 to 0.55023, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2839 - mse: 2.7014 - val_loss: 0.5502 - val_mse: 0.1204\n","Epoch 73/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3609 - mse: 4.3079  \n","Epoch 73: val_loss improved from 0.55023 to 0.51629, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3481 - mse: 4.1299 - val_loss: 0.5163 - val_mse: 0.0930\n","Epoch 74/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3330 - mse: 3.7903 \n","Epoch 74: val_loss improved from 0.51629 to 0.50804, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3196 - mse: 3.6368 - val_loss: 0.5080 - val_mse: 0.1154\n","Epoch 75/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3614 - mse: 4.0366 \n","Epoch 75: val_loss improved from 0.50804 to 0.49070, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3397 - mse: 3.8424 - val_loss: 0.4907 - val_mse: 0.1173\n","Epoch 76/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3146 - mse: 3.7686 \n","Epoch 76: val_loss improved from 0.49070 to 0.46270, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3064 - mse: 3.6821 - val_loss: 0.4627 - val_mse: 0.0968\n","Epoch 77/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3671 - mse: 3.8594 \n","Epoch 77: val_loss improved from 0.46270 to 0.45109, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3503 - mse: 3.7736 - val_loss: 0.4511 - val_mse: 0.1070\n","Epoch 78/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2201 - mse: 2.7969 \n","Epoch 78: val_loss improved from 0.45109 to 0.43232, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2221 - mse: 2.8204 - val_loss: 0.4323 - val_mse: 0.1037\n","Epoch 79/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1936 - mse: 2.9244 \n","Epoch 79: val_loss improved from 0.43232 to 0.41404, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1990 - mse: 3.0232 - val_loss: 0.4140 - val_mse: 0.0993\n","Epoch 80/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1801 - mse: 2.9179 \n","Epoch 80: val_loss did not improve from 0.41404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1837 - mse: 3.0149 - val_loss: 0.4159 - val_mse: 0.1366\n","Epoch 81/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0893 - mse: 2.6495 \n","Epoch 81: val_loss improved from 0.41404 to 0.38489, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0889 - mse: 2.6497 - val_loss: 0.3849 - val_mse: 0.0986\n","Epoch 82/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1752 - mse: 3.5452 \n","Epoch 82: val_loss improved from 0.38489 to 0.35961, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1665 - mse: 3.4616 - val_loss: 0.3596 - val_mse: 0.0723\n","Epoch 83/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1781 - mse: 3.5195 \n","Epoch 83: val_loss did not improve from 0.35961\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1710 - mse: 3.4135 - val_loss: 0.3716 - val_mse: 0.1211\n","Epoch 84/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1391 - mse: 3.4305 \n","Epoch 84: val_loss improved from 0.35961 to 0.34884, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1441 - mse: 3.4348 - val_loss: 0.3488 - val_mse: 0.1010\n","Epoch 85/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1538 - mse: 3.5375 \n","Epoch 85: val_loss did not improve from 0.34884\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1332 - mse: 3.3961 - val_loss: 0.3515 - val_mse: 0.1268\n","Epoch 86/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0840 - mse: 3.0339 \n","Epoch 86: val_loss improved from 0.34884 to 0.33341, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0841 - mse: 3.0274 - val_loss: 0.3334 - val_mse: 0.1125\n","Epoch 87/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0436 - mse: 2.7837 \n","Epoch 87: val_loss did not improve from 0.33341\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0362 - mse: 2.7423 - val_loss: 0.3385 - val_mse: 0.1620\n","Epoch 88/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0980 - mse: 3.1090 \n","Epoch 88: val_loss improved from 0.33341 to 0.30572, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0951 - mse: 3.1222 - val_loss: 0.3057 - val_mse: 0.1028\n","Epoch 89/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0061 - mse: 2.2774 \n","Epoch 89: val_loss improved from 0.30572 to 0.29545, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0145 - mse: 2.3757 - val_loss: 0.2954 - val_mse: 0.1006\n","Epoch 90/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9216 - mse: 2.0601 \n","Epoch 90: val_loss improved from 0.29545 to 0.27567, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9342 - mse: 2.1099 - val_loss: 0.2757 - val_mse: 0.0800\n","Epoch 91/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0395 - mse: 3.0347 \n","Epoch 91: val_loss did not improve from 0.27567\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0231 - mse: 2.9259 - val_loss: 0.2805 - val_mse: 0.1088\n","Epoch 92/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9516 - mse: 2.6691 \n","Epoch 92: val_loss improved from 0.27567 to 0.25575, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9588 - mse: 2.7074 - val_loss: 0.2558 - val_mse: 0.0780\n","Epoch 93/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9492 - mse: 2.4963 \n","Epoch 93: val_loss improved from 0.25575 to 0.24478, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9505 - mse: 2.5034 - val_loss: 0.2448 - val_mse: 0.0732\n","Epoch 94/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9515 - mse: 2.5946 \n","Epoch 94: val_loss improved from 0.24478 to 0.23500, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9602 - mse: 2.6063 - val_loss: 0.2350 - val_mse: 0.0675\n","Epoch 95/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9303 - mse: 2.2168 \n","Epoch 95: val_loss did not improve from 0.23500\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9361 - mse: 2.2653 - val_loss: 0.2604 - val_mse: 0.1341\n","Epoch 96/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9195 - mse: 2.4682 \n","Epoch 96: val_loss improved from 0.23500 to 0.21908, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9243 - mse: 2.5042 - val_loss: 0.2191 - val_mse: 0.0621\n","Epoch 97/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9826 - mse: 3.1323 \n","Epoch 97: val_loss improved from 0.21908 to 0.20499, saving model to best_model_Adam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9835 - mse: 3.1847 - val_loss: 0.2050 - val_mse: 0.0483\n","Epoch 98/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9934 - mse: 3.3693 \n","Epoch 98: val_loss did not improve from 0.20499\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9954 - mse: 3.3395 - val_loss: 0.2258 - val_mse: 0.1010\n","Epoch 99/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8695 - mse: 2.2706 \n","Epoch 99: val_loss did not improve from 0.20499\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8826 - mse: 2.4561 - val_loss: 0.2165 - val_mse: 0.0940\n","Epoch 100/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9565 - mse: 3.2679 \n","Epoch 100: val_loss did not improve from 0.20499\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9526 - mse: 3.2221 - val_loss: 0.2164 - val_mse: 0.1018\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: Adam, Loss Function: huber, Epochs: 100, Test MSE: 0.06811535269303809\n","Training with optimizer: SGD, loss function: mse, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 83.6799 - mse: 77.9403\n","Epoch 1: val_loss improved from inf to 6.51946, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 282ms/step - loss: 82.2457 - mse: 76.5026 - val_loss: 6.5195 - val_mse: 0.5442\n","Epoch 2/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.1628 - mse: 6.1802 \n","Epoch 2: val_loss did not improve from 6.51946\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.0976 - mse: 6.1149 - val_loss: 6.5987 - val_mse: 0.6186\n","Epoch 3/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.5302 - mse: 4.5558 \n","Epoch 3: val_loss improved from 6.51946 to 6.33304, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.5147 - mse: 4.5409 - val_loss: 6.3330 - val_mse: 0.3808\n","Epoch 4/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3316 - mse: 3.3894 \n","Epoch 4: val_loss improved from 6.33304 to 6.25130, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.3320 - mse: 3.3909 - val_loss: 6.2513 - val_mse: 0.3439\n","Epoch 5/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0227 - mse: 3.1269 \n","Epoch 5: val_loss improved from 6.25130 to 6.05004, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.0154 - mse: 3.1204 - val_loss: 6.0500 - val_mse: 0.1918\n","Epoch 6/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6990 - mse: 2.8531 \n","Epoch 6: val_loss improved from 6.05004 to 5.94264, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.6948 - mse: 2.8494 - val_loss: 5.9426 - val_mse: 0.1358\n","Epoch 7/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1744 - mse: 2.3801 \n","Epoch 7: val_loss improved from 5.94264 to 5.92912, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1751 - mse: 2.3817 - val_loss: 5.9291 - val_mse: 0.1766\n","Epoch 8/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1163 - mse: 2.3766 \n","Epoch 8: val_loss improved from 5.92912 to 5.78701, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1103 - mse: 2.3715 - val_loss: 5.7870 - val_mse: 0.0893\n","Epoch 9/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9392 - mse: 2.2542 \n","Epoch 9: val_loss did not improve from 5.78701\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.9360 - mse: 2.2520 - val_loss: 5.9239 - val_mse: 0.2812\n","Epoch 10/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7923 - mse: 2.1613 \n","Epoch 10: val_loss improved from 5.78701 to 5.72042, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7784 - mse: 2.1494 - val_loss: 5.7204 - val_mse: 0.1328\n","Epoch 11/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7600 - mse: 2.1840 \n","Epoch 11: val_loss improved from 5.72042 to 5.69648, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7211 - mse: 2.1472 - val_loss: 5.6965 - val_mse: 0.1642\n","Epoch 12/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2323 - mse: 1.7127 \n","Epoch 12: val_loss improved from 5.69648 to 5.54613, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.2391 - mse: 1.7205 - val_loss: 5.5461 - val_mse: 0.0684\n","Epoch 13/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1449 - mse: 1.6798 \n","Epoch 13: val_loss improved from 5.54613 to 5.49570, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1504 - mse: 1.6862 - val_loss: 5.4957 - val_mse: 0.0726\n","Epoch 14/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1643 - mse: 1.7537 \n","Epoch 14: val_loss improved from 5.49570 to 5.44729, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1614 - mse: 1.7519 - val_loss: 5.4473 - val_mse: 0.0782\n","Epoch 15/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9053 - mse: 1.5487 \n","Epoch 15: val_loss improved from 5.44729 to 5.37837, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9004 - mse: 1.5447 - val_loss: 5.3784 - val_mse: 0.0632\n","Epoch 16/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9601 - mse: 1.6574 \n","Epoch 16: val_loss improved from 5.37837 to 5.34839, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9448 - mse: 1.6430 - val_loss: 5.3484 - val_mse: 0.0869\n","Epoch 17/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6724 - mse: 1.4232 \n","Epoch 17: val_loss did not improve from 5.34839\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.6798 - mse: 1.4316 - val_loss: 5.3533 - val_mse: 0.1449\n","Epoch 18/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8962 - mse: 1.7004 \n","Epoch 18: val_loss did not improve from 5.34839\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8913 - mse: 1.6959 - val_loss: 5.4924 - val_mse: 0.3365\n","Epoch 19/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6231 - mse: 1.4793 \n","Epoch 19: val_loss improved from 5.34839 to 5.16953, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6220 - mse: 1.4792 - val_loss: 5.1695 - val_mse: 0.0658\n","Epoch 20/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6305 - mse: 1.5378 \n","Epoch 20: val_loss improved from 5.16953 to 5.11978, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6070 - mse: 1.5161 - val_loss: 5.1198 - val_mse: 0.0678\n","Epoch 21/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4727 - mse: 1.4316 \n","Epoch 21: val_loss improved from 5.11978 to 5.05601, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4695 - mse: 1.4303 - val_loss: 5.0560 - val_mse: 0.0553\n","Epoch 22/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2539 - mse: 1.2645 \n","Epoch 22: val_loss improved from 5.05601 to 5.01069, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2527 - mse: 1.2646 - val_loss: 5.0107 - val_mse: 0.0610\n","Epoch 23/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1670 - mse: 1.2290 \n","Epoch 23: val_loss improved from 5.01069 to 4.95388, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.1624 - mse: 1.2252 - val_loss: 4.9539 - val_mse: 0.0547\n","Epoch 24/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0917 - mse: 1.2036 \n","Epoch 24: val_loss improved from 4.95388 to 4.90356, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0910 - mse: 1.2043 - val_loss: 4.9036 - val_mse: 0.0544\n","Epoch 25/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0308 - mse: 1.1931 \n","Epoch 25: val_loss improved from 4.90356 to 4.86404, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0306 - mse: 1.1938 - val_loss: 4.8640 - val_mse: 0.0644\n","Epoch 26/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9093 - mse: 1.1210 \n","Epoch 26: val_loss improved from 4.86404 to 4.81196, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9112 - mse: 1.1239 - val_loss: 4.8120 - val_mse: 0.0615\n","Epoch 27/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9322 - mse: 1.1929 \n","Epoch 27: val_loss improved from 4.81196 to 4.76316, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9287 - mse: 1.1904 - val_loss: 4.7632 - val_mse: 0.0612\n","Epoch 28/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8162 - mse: 1.1254 \n","Epoch 28: val_loss improved from 4.76316 to 4.69793, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8136 - mse: 1.1237 - val_loss: 4.6979 - val_mse: 0.0441\n","Epoch 29/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8212 - mse: 1.1789 \n","Epoch 29: val_loss improved from 4.69793 to 4.65305, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8193 - mse: 1.1773 - val_loss: 4.6531 - val_mse: 0.0469\n","Epoch 30/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7035 - mse: 1.1083 \n","Epoch 30: val_loss improved from 4.65305 to 4.60946, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7016 - mse: 1.1072 - val_loss: 4.6095 - val_mse: 0.0505\n","Epoch 31/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5174 - mse: 0.9679 \n","Epoch 31: val_loss improved from 4.60946 to 4.57921, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5202 - mse: 0.9730 - val_loss: 4.5792 - val_mse: 0.0670\n","Epoch 32/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5231 - mse: 1.0208 \n","Epoch 32: val_loss improved from 4.57921 to 4.51596, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5158 - mse: 1.0152 - val_loss: 4.5160 - val_mse: 0.0500\n","Epoch 33/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4486 - mse: 0.9933 \n","Epoch 33: val_loss improved from 4.51596 to 4.51189, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4418 - mse: 0.9873 - val_loss: 4.5119 - val_mse: 0.0918\n","Epoch 34/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3844 - mse: 0.9744 \n","Epoch 34: val_loss improved from 4.51189 to 4.42063, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3824 - mse: 0.9737 - val_loss: 4.4206 - val_mse: 0.0459\n","Epoch 35/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3797 - mse: 1.0154 \n","Epoch 35: val_loss improved from 4.42063 to 4.39833, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3762 - mse: 1.0126 - val_loss: 4.3983 - val_mse: 0.0685\n","Epoch 36/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2535 - mse: 0.9340 \n","Epoch 36: val_loss improved from 4.39833 to 4.34829, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2531 - mse: 0.9344 - val_loss: 4.3483 - val_mse: 0.0629\n","Epoch 37/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1686 - mse: 0.8934 \n","Epoch 37: val_loss improved from 4.34829 to 4.31824, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.1707 - mse: 0.8963 - val_loss: 4.3182 - val_mse: 0.0769\n","Epoch 38/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1004 - mse: 0.8691 \n","Epoch 38: val_loss improved from 4.31824 to 4.25031, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0973 - mse: 0.8668 - val_loss: 4.2503 - val_mse: 0.0525\n","Epoch 39/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0840 - mse: 0.8954 \n","Epoch 39: val_loss improved from 4.25031 to 4.21015, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0874 - mse: 0.9004 - val_loss: 4.2102 - val_mse: 0.0556\n","Epoch 40/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8966 - mse: 0.7490 \n","Epoch 40: val_loss improved from 4.21015 to 4.17041, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9170 - mse: 0.7731 - val_loss: 4.1704 - val_mse: 0.0586\n","Epoch 41/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8153 - mse: 0.7125 \n","Epoch 41: val_loss improved from 4.17041 to 4.11939, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8206 - mse: 0.7194 - val_loss: 4.1194 - val_mse: 0.0499\n","Epoch 42/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8563 - mse: 0.7957 \n","Epoch 42: val_loss improved from 4.11939 to 4.07742, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8521 - mse: 0.7931 - val_loss: 4.0774 - val_mse: 0.0498\n","Epoch 43/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8182 - mse: 0.7994 \n","Epoch 43: val_loss improved from 4.07742 to 4.03384, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8158 - mse: 0.7985 - val_loss: 4.0338 - val_mse: 0.0477\n","Epoch 44/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7053 - mse: 0.7283 \n","Epoch 44: val_loss improved from 4.03384 to 4.01111, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7057 - mse: 0.7298 - val_loss: 4.0111 - val_mse: 0.0660\n","Epoch 45/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6805 - mse: 0.7437 \n","Epoch 45: val_loss improved from 4.01111 to 3.94943, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6810 - mse: 0.7461 - val_loss: 3.9494 - val_mse: 0.0450\n","Epoch 46/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6312 - mse: 0.7349 \n","Epoch 46: val_loss improved from 3.94943 to 3.92836, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6223 - mse: 0.7280 - val_loss: 3.9284 - val_mse: 0.0642\n","Epoch 47/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6556 - mse: 0.7976 \n","Epoch 47: val_loss improved from 3.92836 to 3.92308, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6632 - mse: 0.8090 - val_loss: 3.9231 - val_mse: 0.0987\n","Epoch 48/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4274 - mse: 0.6114 \n","Epoch 48: val_loss improved from 3.92308 to 3.83796, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4284 - mse: 0.6138 - val_loss: 3.8380 - val_mse: 0.0530\n","Epoch 49/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4902 - mse: 0.7135 \n","Epoch 49: val_loss improved from 3.83796 to 3.81669, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4918 - mse: 0.7165 - val_loss: 3.8167 - val_mse: 0.0707\n","Epoch 50/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4014 - mse: 0.6628 \n","Epoch 50: val_loss improved from 3.81669 to 3.75447, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3869 - mse: 0.6505 - val_loss: 3.7545 - val_mse: 0.0471\n","Epoch 51/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3613 - mse: 0.6617  \n","Epoch 51: val_loss improved from 3.75447 to 3.72211, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3616 - mse: 0.6637 - val_loss: 3.7221 - val_mse: 0.0529\n","Epoch 52/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2556 - mse: 0.5952 \n","Epoch 52: val_loss improved from 3.72211 to 3.69056, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2554 - mse: 0.5957 - val_loss: 3.6906 - val_mse: 0.0592\n","Epoch 53/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1982 - mse: 0.5755 \n","Epoch 53: val_loss improved from 3.69056 to 3.63689, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2007 - mse: 0.5787 - val_loss: 3.6369 - val_mse: 0.0430\n","Epoch 54/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2879 - mse: 0.7026 \n","Epoch 54: val_loss improved from 3.63689 to 3.61330, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2841 - mse: 0.6995 - val_loss: 3.6133 - val_mse: 0.0564\n","Epoch 55/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1781 - mse: 0.6298 \n","Epoch 55: val_loss improved from 3.61330 to 3.57768, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1772 - mse: 0.6295 - val_loss: 3.5777 - val_mse: 0.0575\n","Epoch 56/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0614 - mse: 0.5497 \n","Epoch 56: val_loss improved from 3.57768 to 3.54372, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0634 - mse: 0.5523 - val_loss: 3.5437 - val_mse: 0.0599\n","Epoch 57/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0985 - mse: 0.6232 \n","Epoch 57: val_loss improved from 3.54372 to 3.49007, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0974 - mse: 0.6225 - val_loss: 3.4901 - val_mse: 0.0421\n","Epoch 58/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9732 - mse: 0.5338 \n","Epoch 58: val_loss improved from 3.49007 to 3.45952, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9739 - mse: 0.5348 - val_loss: 3.4595 - val_mse: 0.0471\n","Epoch 59/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0467 - mse: 0.6425 \n","Epoch 59: val_loss improved from 3.45952 to 3.42423, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0416 - mse: 0.6380 - val_loss: 3.4242 - val_mse: 0.0470\n","Epoch 60/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8481 - mse: 0.4783 \n","Epoch 60: val_loss improved from 3.42423 to 3.39067, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8540 - mse: 0.4855 - val_loss: 3.3907 - val_mse: 0.0483\n","Epoch 61/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8502 - mse: 0.5156 \n","Epoch 61: val_loss improved from 3.39067 to 3.34723, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8513 - mse: 0.5176 - val_loss: 3.3472 - val_mse: 0.0394\n","Epoch 62/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8674 - mse: 0.5671 \n","Epoch 62: val_loss improved from 3.34723 to 3.31273, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8628 - mse: 0.5634 - val_loss: 3.3127 - val_mse: 0.0390\n","Epoch 63/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7845 - mse: 0.5182 \n","Epoch 63: val_loss improved from 3.31273 to 3.31149, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7804 - mse: 0.5151 - val_loss: 3.3115 - val_mse: 0.0715\n","Epoch 64/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7368 - mse: 0.5046 \n","Epoch 64: val_loss improved from 3.31149 to 3.24364, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7387 - mse: 0.5071 - val_loss: 3.2436 - val_mse: 0.0371\n","Epoch 65/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7159 - mse: 0.5174 \n","Epoch 65: val_loss improved from 3.24364 to 3.22775, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7147 - mse: 0.5164 - val_loss: 3.2278 - val_mse: 0.0543\n","Epoch 66/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7266 - mse: 0.5607 \n","Epoch 66: val_loss did not improve from 3.22775\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7241 - mse: 0.5589 - val_loss: 3.2466 - val_mse: 0.1059\n","Epoch 67/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6609 - mse: 0.5277 \n","Epoch 67: val_loss improved from 3.22775 to 3.15549, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6587 - mse: 0.5261 - val_loss: 3.1555 - val_mse: 0.0472\n","Epoch 68/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5619 - mse: 0.4607 \n","Epoch 68: val_loss improved from 3.15549 to 3.11758, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5603 - mse: 0.4600 - val_loss: 3.1176 - val_mse: 0.0414\n","Epoch 69/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6054 - mse: 0.5351 \n","Epoch 69: val_loss improved from 3.11758 to 3.08881, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5861 - mse: 0.5178 - val_loss: 3.0888 - val_mse: 0.0444\n","Epoch 70/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5268 - mse: 0.4890 \n","Epoch 70: val_loss improved from 3.08881 to 3.05724, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5331 - mse: 0.4965 - val_loss: 3.0572 - val_mse: 0.0442\n","Epoch 71/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4403 - mse: 0.4339 \n","Epoch 71: val_loss improved from 3.05724 to 3.04078, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4386 - mse: 0.4334 - val_loss: 3.0408 - val_mse: 0.0589\n","Epoch 72/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4502 - mse: 0.4754 \n","Epoch 72: val_loss improved from 3.04078 to 3.00093, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4494 - mse: 0.4751 - val_loss: 3.0009 - val_mse: 0.0498\n","Epoch 73/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4131 - mse: 0.4690 \n","Epoch 73: val_loss improved from 3.00093 to 2.95895, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4137 - mse: 0.4701 - val_loss: 2.9590 - val_mse: 0.0383\n","Epoch 74/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3683 - mse: 0.4545 \n","Epoch 74: val_loss improved from 2.95895 to 2.92854, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3666 - mse: 0.4534 - val_loss: 2.9285 - val_mse: 0.0380\n","Epoch 75/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3282 - mse: 0.4446 \n","Epoch 75: val_loss improved from 2.92854 to 2.89641, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3282 - mse: 0.4450 - val_loss: 2.8964 - val_mse: 0.0357\n","Epoch 76/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2565 - mse: 0.4024 \n","Epoch 76: val_loss improved from 2.89641 to 2.86991, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2578 - mse: 0.4044 - val_loss: 2.8699 - val_mse: 0.0387\n","Epoch 77/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2598 - mse: 0.4357 \n","Epoch 77: val_loss improved from 2.86991 to 2.85285, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2589 - mse: 0.4350 - val_loss: 2.8528 - val_mse: 0.0509\n","Epoch 78/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2082 - mse: 0.4129 \n","Epoch 78: val_loss improved from 2.85285 to 2.81179, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2086 - mse: 0.4138 - val_loss: 2.8118 - val_mse: 0.0388\n","Epoch 79/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1742 - mse: 0.4078 \n","Epoch 79: val_loss improved from 2.81179 to 2.77894, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1729 - mse: 0.4070 - val_loss: 2.7789 - val_mse: 0.0345\n","Epoch 80/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1027 - mse: 0.3640 \n","Epoch 80: val_loss improved from 2.77894 to 2.75165, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1042 - mse: 0.3669 - val_loss: 2.7517 - val_mse: 0.0356\n","Epoch 81/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0470 - mse: 0.3369 \n","Epoch 81: val_loss improved from 2.75165 to 2.72486, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0505 - mse: 0.3414 - val_loss: 2.7249 - val_mse: 0.0368\n","Epoch 82/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1061 - mse: 0.4245 \n","Epoch 82: val_loss improved from 2.72486 to 2.70617, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1053 - mse: 0.4242 - val_loss: 2.7062 - val_mse: 0.0459\n","Epoch 83/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0502 - mse: 0.3961 \n","Epoch 83: val_loss improved from 2.70617 to 2.67187, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0478 - mse: 0.3944 - val_loss: 2.6719 - val_mse: 0.0391\n","Epoch 84/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0117 - mse: 0.3854 \n","Epoch 84: val_loss improved from 2.67187 to 2.64178, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0120 - mse: 0.3860 - val_loss: 2.6418 - val_mse: 0.0361\n","Epoch 85/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9502 - mse: 0.3511 \n","Epoch 85: val_loss improved from 2.64178 to 2.61652, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9494 - mse: 0.3505 - val_loss: 2.6165 - val_mse: 0.0378\n","Epoch 86/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9253 - mse: 0.3527 \n","Epoch 86: val_loss improved from 2.61652 to 2.58599, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9271 - mse: 0.3550 - val_loss: 2.5860 - val_mse: 0.0339\n","Epoch 87/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8688 - mse: 0.3226 \n","Epoch 87: val_loss improved from 2.58599 to 2.56317, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8682 - mse: 0.3227 - val_loss: 2.5632 - val_mse: 0.0374\n","Epoch 88/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8435 - mse: 0.3238 \n","Epoch 88: val_loss improved from 2.56317 to 2.53482, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8434 - mse: 0.3242 - val_loss: 2.5348 - val_mse: 0.0352\n","Epoch 89/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8352 - mse: 0.3413 \n","Epoch 89: val_loss improved from 2.53482 to 2.50964, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8371 - mse: 0.3439 - val_loss: 2.5096 - val_mse: 0.0358\n","Epoch 90/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7469 - mse: 0.2782 \n","Epoch 90: val_loss did not improve from 2.50964\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7527 - mse: 0.2853 - val_loss: 2.5262 - val_mse: 0.0779\n","Epoch 91/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8254 - mse: 0.3825 \n","Epoch 91: val_loss improved from 2.50964 to 2.46246, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8172 - mse: 0.3752 - val_loss: 2.4625 - val_mse: 0.0394\n","Epoch 92/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7572 - mse: 0.3392 \n","Epoch 92: val_loss improved from 2.46246 to 2.43432, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7558 - mse: 0.3390 - val_loss: 2.4343 - val_mse: 0.0363\n","Epoch 93/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7200 - mse: 0.3280 \n","Epoch 93: val_loss improved from 2.43432 to 2.40822, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7200 - mse: 0.3282 - val_loss: 2.4082 - val_mse: 0.0350\n","Epoch 94/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6715 - mse: 0.3037 \n","Epoch 94: val_loss improved from 2.40822 to 2.38502, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6743 - mse: 0.3072 - val_loss: 2.3850 - val_mse: 0.0363\n","Epoch 95/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6471 - mse: 0.3038 \n","Epoch 95: val_loss improved from 2.38502 to 2.35681, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6460 - mse: 0.3033 - val_loss: 2.3568 - val_mse: 0.0324\n","Epoch 96/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5863 - mse: 0.2674 \n","Epoch 96: val_loss improved from 2.35681 to 2.33504, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5863 - mse: 0.2678 - val_loss: 2.3350 - val_mse: 0.0346\n","Epoch 97/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5808 - mse: 0.2858 \n","Epoch 97: val_loss improved from 2.33504 to 2.31147, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5803 - mse: 0.2858 - val_loss: 2.3115 - val_mse: 0.0348\n","Epoch 98/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5679 - mse: 0.2967 \n","Epoch 98: val_loss improved from 2.31147 to 2.29057, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5684 - mse: 0.2975 - val_loss: 2.2906 - val_mse: 0.0374\n","Epoch 99/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5210 - mse: 0.2732 \n","Epoch 99: val_loss improved from 2.29057 to 2.26669, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5205 - mse: 0.2731 - val_loss: 2.2667 - val_mse: 0.0368\n","Epoch 100/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5062 - mse: 0.2816 \n","Epoch 100: val_loss improved from 2.26669 to 2.25740, saving model to best_model_SGD_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5057 - mse: 0.2816 - val_loss: 2.2574 - val_mse: 0.0506\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: SGD, Loss Function: mse, Epochs: 100, Test MSE: 0.06418110654842957\n","Training with optimizer: SGD, loss function: mae, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 17.3927 - mse: 143.1647\n","Epoch 1: val_loss improved from inf to 16.77892, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 311ms/step - loss: 17.3871 - mse: 143.0458 - val_loss: 16.7789 - val_mse: 126.5414\n","Epoch 2/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.5413 - mse: 125.1524 \n","Epoch 2: val_loss improved from 16.77892 to 15.80585, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.5176 - mse: 124.6966 - val_loss: 15.8059 - val_mse: 106.8032\n","Epoch 3/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.5249 - mse: 105.3767 \n","Epoch 3: val_loss improved from 15.80585 to 14.51817, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.4973 - mse: 104.9318 - val_loss: 14.5182 - val_mse: 82.9557\n","Epoch 4/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.1612 - mse: 83.1170 \n","Epoch 4: val_loss improved from 14.51817 to 12.64277, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.1333 - mse: 82.7185 - val_loss: 12.6428 - val_mse: 53.3571\n","Epoch 5/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.3732 - mse: 57.9203 \n","Epoch 5: val_loss improved from 12.64277 to 10.07206, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.3316 - mse: 57.4094 - val_loss: 10.0721 - val_mse: 23.4905\n","Epoch 6/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9785 - mse: 31.5128  \n","Epoch 6: val_loss improved from 10.07206 to 7.51502, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.9484 - mse: 31.1817 - val_loss: 7.5150 - val_mse: 6.4328\n","Epoch 7/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5475 - mse: 16.5877 \n","Epoch 7: val_loss improved from 7.51502 to 6.44389, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.5416 - mse: 16.6222 - val_loss: 6.4439 - val_mse: 2.6359\n","Epoch 8/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1613 - mse: 14.6691 \n","Epoch 8: val_loss improved from 6.44389 to 6.17344, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1538 - mse: 14.5757 - val_loss: 6.1734 - val_mse: 1.6631\n","Epoch 9/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7305 - mse: 10.2731\n","Epoch 9: val_loss improved from 6.17344 to 5.95530, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7268 - mse: 10.2471 - val_loss: 5.9553 - val_mse: 1.0258\n","Epoch 10/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5999 - mse: 9.4310 \n","Epoch 10: val_loss improved from 5.95530 to 5.87323, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.5956 - mse: 9.4016 - val_loss: 5.8732 - val_mse: 1.0164\n","Epoch 11/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4097 - mse: 8.0848  \n","Epoch 11: val_loss improved from 5.87323 to 5.76346, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.4039 - mse: 8.0540 - val_loss: 5.7635 - val_mse: 0.7771\n","Epoch 12/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0808 - mse: 6.0793 \n","Epoch 12: val_loss improved from 5.76346 to 5.72754, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.0848 - mse: 6.1149 - val_loss: 5.7275 - val_mse: 0.7480\n","Epoch 13/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9305 - mse: 5.9519 \n","Epoch 13: val_loss improved from 5.72754 to 5.59238, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9352 - mse: 5.9781 - val_loss: 5.5924 - val_mse: 0.5977\n","Epoch 14/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8032 - mse: 5.3847 \n","Epoch 14: val_loss improved from 5.59238 to 5.50287, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.8061 - mse: 5.4027 - val_loss: 5.5029 - val_mse: 0.5959\n","Epoch 15/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8019 - mse: 5.4764 \n","Epoch 15: val_loss improved from 5.50287 to 5.48676, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7984 - mse: 5.4588 - val_loss: 5.4868 - val_mse: 0.6267\n","Epoch 16/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8371 - mse: 6.1356 \n","Epoch 16: val_loss improved from 5.48676 to 5.40143, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.8340 - mse: 6.1103 - val_loss: 5.4014 - val_mse: 0.5426\n","Epoch 17/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5932 - mse: 4.7911 \n","Epoch 17: val_loss improved from 5.40143 to 5.29136, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5926 - mse: 4.7977 - val_loss: 5.2914 - val_mse: 0.3917\n","Epoch 18/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6134 - mse: 5.3583 \n","Epoch 18: val_loss improved from 5.29136 to 5.24897, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6081 - mse: 5.3351 - val_loss: 5.2490 - val_mse: 0.4488\n","Epoch 19/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4447 - mse: 4.5897 \n","Epoch 19: val_loss improved from 5.24897 to 5.23190, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4431 - mse: 4.5835 - val_loss: 5.2319 - val_mse: 0.5434\n","Epoch 20/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4073 - mse: 4.5302 \n","Epoch 20: val_loss improved from 5.23190 to 5.17212, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4031 - mse: 4.5134 - val_loss: 5.1721 - val_mse: 0.5055\n","Epoch 21/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2772 - mse: 4.2728 \n","Epoch 21: val_loss improved from 5.17212 to 5.11485, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2754 - mse: 4.2628 - val_loss: 5.1149 - val_mse: 0.5235\n","Epoch 22/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2325 - mse: 3.9636 \n","Epoch 22: val_loss improved from 5.11485 to 5.04253, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2348 - mse: 3.9772 - val_loss: 5.0425 - val_mse: 0.3884\n","Epoch 23/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1765 - mse: 4.2407 \n","Epoch 23: val_loss improved from 5.04253 to 4.95794, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.1701 - mse: 4.2085 - val_loss: 4.9579 - val_mse: 0.3032\n","Epoch 24/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0140 - mse: 3.4279 \n","Epoch 24: val_loss did not improve from 4.95794\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.0200 - mse: 3.4588 - val_loss: 4.9619 - val_mse: 0.3559\n","Epoch 25/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0918 - mse: 4.2169 \n","Epoch 25: val_loss improved from 4.95794 to 4.85851, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0888 - mse: 4.2018 - val_loss: 4.8585 - val_mse: 0.2634\n","Epoch 26/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9144 - mse: 3.3888 \n","Epoch 26: val_loss did not improve from 4.85851\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.9205 - mse: 3.4235 - val_loss: 4.8925 - val_mse: 0.3249\n","Epoch 27/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9696 - mse: 3.7378 \n","Epoch 27: val_loss improved from 4.85851 to 4.77361, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9660 - mse: 3.7304 - val_loss: 4.7736 - val_mse: 0.2372\n","Epoch 28/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8219 - mse: 3.4008 \n","Epoch 28: val_loss improved from 4.77361 to 4.66206, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8183 - mse: 3.3990 - val_loss: 4.6621 - val_mse: 0.1710\n","Epoch 29/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7730 - mse: 3.5665 \n","Epoch 29: val_loss improved from 4.66206 to 4.62536, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7728 - mse: 3.5647 - val_loss: 4.6254 - val_mse: 0.1755\n","Epoch 30/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7607 - mse: 3.6228 \n","Epoch 30: val_loss improved from 4.62536 to 4.59758, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7586 - mse: 3.6131 - val_loss: 4.5976 - val_mse: 0.1849\n","Epoch 31/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6003 - mse: 2.9304 \n","Epoch 31: val_loss improved from 4.59758 to 4.56546, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6011 - mse: 2.9356 - val_loss: 4.5655 - val_mse: 0.1992\n","Epoch 32/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6177 - mse: 2.9239 \n","Epoch 32: val_loss improved from 4.56546 to 4.56135, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6151 - mse: 2.9197 - val_loss: 4.5613 - val_mse: 0.2400\n","Epoch 33/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5213 - mse: 2.9829 \n","Epoch 33: val_loss improved from 4.56135 to 4.44109, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5215 - mse: 2.9880 - val_loss: 4.4411 - val_mse: 0.1648\n","Epoch 34/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5511 - mse: 3.0572 \n","Epoch 34: val_loss improved from 4.44109 to 4.38775, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5457 - mse: 3.0505 - val_loss: 4.3878 - val_mse: 0.1538\n","Epoch 35/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3814 - mse: 2.6655 \n","Epoch 35: val_loss improved from 4.38775 to 4.30834, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3822 - mse: 2.6770 - val_loss: 4.3083 - val_mse: 0.1168\n","Epoch 36/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2966 - mse: 2.5765 \n","Epoch 36: val_loss did not improve from 4.30834\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2970 - mse: 2.5769 - val_loss: 4.3300 - val_mse: 0.1623\n","Epoch 37/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2751 - mse: 2.6569 \n","Epoch 37: val_loss did not improve from 4.30834\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2786 - mse: 2.6682 - val_loss: 4.3356 - val_mse: 0.2025\n","Epoch 38/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2473 - mse: 2.6880 \n","Epoch 38: val_loss improved from 4.30834 to 4.25443, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2472 - mse: 2.6894 - val_loss: 4.2544 - val_mse: 0.1675\n","Epoch 39/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2580 - mse: 2.8008 \n","Epoch 39: val_loss improved from 4.25443 to 4.17389, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2574 - mse: 2.8038 - val_loss: 4.1739 - val_mse: 0.1353\n","Epoch 40/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1408 - mse: 2.5722 \n","Epoch 40: val_loss improved from 4.17389 to 4.10295, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.1413 - mse: 2.5734 - val_loss: 4.1029 - val_mse: 0.1062\n","Epoch 41/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9761 - mse: 2.0292 \n","Epoch 41: val_loss improved from 4.10295 to 4.07501, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9789 - mse: 2.0441 - val_loss: 4.0750 - val_mse: 0.1218\n","Epoch 42/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0547 - mse: 2.6594 \n","Epoch 42: val_loss did not improve from 4.07501\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.0546 - mse: 2.6571 - val_loss: 4.1336 - val_mse: 0.2034\n","Epoch 43/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0426 - mse: 2.5450 \n","Epoch 43: val_loss improved from 4.07501 to 4.04346, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0414 - mse: 2.5409 - val_loss: 4.0435 - val_mse: 0.1624\n","Epoch 44/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9499 - mse: 2.3875  \n","Epoch 44: val_loss improved from 4.04346 to 3.96924, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.9436 - mse: 2.3684 - val_loss: 3.9692 - val_mse: 0.1281\n","Epoch 45/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8727 - mse: 2.1595 \n","Epoch 45: val_loss improved from 3.96924 to 3.91041, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8797 - mse: 2.2078 - val_loss: 3.9104 - val_mse: 0.1084\n","Epoch 46/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8999 - mse: 2.4020 \n","Epoch 46: val_loss did not improve from 3.91041\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.8842 - mse: 2.3628 - val_loss: 3.9524 - val_mse: 0.1648\n","Epoch 47/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8344 - mse: 2.3857 \n","Epoch 47: val_loss improved from 3.91041 to 3.89657, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8377 - mse: 2.4021 - val_loss: 3.8966 - val_mse: 0.1582\n","Epoch 48/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7346 - mse: 2.1064  \n","Epoch 48: val_loss improved from 3.89657 to 3.84601, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7339 - mse: 2.1044 - val_loss: 3.8460 - val_mse: 0.1407\n","Epoch 49/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7296 - mse: 2.2295 \n","Epoch 49: val_loss improved from 3.84601 to 3.77573, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7284 - mse: 2.2288 - val_loss: 3.7757 - val_mse: 0.1093\n","Epoch 50/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6465 - mse: 2.0391 \n","Epoch 50: val_loss improved from 3.77573 to 3.75741, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6458 - mse: 2.0406 - val_loss: 3.7574 - val_mse: 0.1206\n","Epoch 51/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6488 - mse: 2.2067 \n","Epoch 51: val_loss improved from 3.75741 to 3.66241, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6484 - mse: 2.2094 - val_loss: 3.6624 - val_mse: 0.0839\n","Epoch 52/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5144 - mse: 1.8980 \n","Epoch 52: val_loss improved from 3.66241 to 3.66060, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5141 - mse: 1.9013 - val_loss: 3.6606 - val_mse: 0.1171\n","Epoch 53/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4705 - mse: 1.8317 \n","Epoch 53: val_loss did not improve from 3.66060\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4677 - mse: 1.8262 - val_loss: 3.6612 - val_mse: 0.1247\n","Epoch 54/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5673 - mse: 2.2168 \n","Epoch 54: val_loss improved from 3.66060 to 3.62142, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5620 - mse: 2.2029 - val_loss: 3.6214 - val_mse: 0.1211\n","Epoch 55/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4983 - mse: 2.1143 \n","Epoch 55: val_loss improved from 3.62142 to 3.52267, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4950 - mse: 2.1043 - val_loss: 3.5227 - val_mse: 0.0824\n","Epoch 56/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3877 - mse: 1.8321 \n","Epoch 56: val_loss did not improve from 3.52267\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.3878 - mse: 1.8396 - val_loss: 3.5398 - val_mse: 0.1107\n","Epoch 57/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3558 - mse: 1.8944 \n","Epoch 57: val_loss improved from 3.52267 to 3.43277, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3548 - mse: 1.8929 - val_loss: 3.4328 - val_mse: 0.0664\n","Epoch 58/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2715 - mse: 1.7560 \n","Epoch 58: val_loss did not improve from 3.43277\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2763 - mse: 1.7723 - val_loss: 3.4429 - val_mse: 0.0925\n","Epoch 59/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3570 - mse: 2.1042 \n","Epoch 59: val_loss did not improve from 3.43277\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3514 - mse: 2.0885 - val_loss: 3.4787 - val_mse: 0.1337\n","Epoch 60/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1551 - mse: 1.5568 \n","Epoch 60: val_loss improved from 3.43277 to 3.40687, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1568 - mse: 1.5642 - val_loss: 3.4069 - val_mse: 0.1091\n","Epoch 61/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1698 - mse: 1.7476 \n","Epoch 61: val_loss improved from 3.40687 to 3.36949, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1716 - mse: 1.7500 - val_loss: 3.3695 - val_mse: 0.1012\n","Epoch 62/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2081 - mse: 1.9469 \n","Epoch 62: val_loss improved from 3.36949 to 3.32691, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2046 - mse: 1.9336 - val_loss: 3.3269 - val_mse: 0.0936\n","Epoch 63/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1208 - mse: 1.7747 \n","Epoch 63: val_loss did not improve from 3.32691\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1181 - mse: 1.7702 - val_loss: 3.4004 - val_mse: 0.1589\n","Epoch 64/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0775 - mse: 1.6948 \n","Epoch 64: val_loss improved from 3.32691 to 3.22385, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0807 - mse: 1.7091 - val_loss: 3.2238 - val_mse: 0.0709\n","Epoch 65/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1158 - mse: 1.8679 \n","Epoch 65: val_loss did not improve from 3.22385\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1064 - mse: 1.8449 - val_loss: 3.2717 - val_mse: 0.1187\n","Epoch 66/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0336 - mse: 1.7466 \n","Epoch 66: val_loss improved from 3.22385 to 3.13924, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0349 - mse: 1.7533 - val_loss: 3.1392 - val_mse: 0.0536\n","Epoch 67/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9422 - mse: 1.5676 \n","Epoch 67: val_loss did not improve from 3.13924\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9432 - mse: 1.5733 - val_loss: 3.1802 - val_mse: 0.1017\n","Epoch 68/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8937 - mse: 1.4675 \n","Epoch 68: val_loss improved from 3.13924 to 3.07105, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8939 - mse: 1.4687 - val_loss: 3.0710 - val_mse: 0.0517\n","Epoch 69/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9763 - mse: 1.8988 \n","Epoch 69: val_loss did not improve from 3.07105\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9706 - mse: 1.8819 - val_loss: 3.0824 - val_mse: 0.0747\n","Epoch 70/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8802 - mse: 1.6449 \n","Epoch 70: val_loss did not improve from 3.07105\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8814 - mse: 1.6495 - val_loss: 3.0860 - val_mse: 0.0946\n","Epoch 71/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8319 - mse: 1.5351 \n","Epoch 71: val_loss improved from 3.07105 to 2.98892, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8313 - mse: 1.5367 - val_loss: 2.9889 - val_mse: 0.0538\n","Epoch 72/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8033 - mse: 1.5895 \n","Epoch 72: val_loss did not improve from 2.98892\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8033 - mse: 1.5915 - val_loss: 3.0196 - val_mse: 0.0871\n","Epoch 73/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7668 - mse: 1.6423 \n","Epoch 73: val_loss did not improve from 2.98892\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7670 - mse: 1.6423 - val_loss: 3.0340 - val_mse: 0.1107\n","Epoch 74/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7339 - mse: 1.5754 \n","Epoch 74: val_loss improved from 2.98892 to 2.91287, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7316 - mse: 1.5698 - val_loss: 2.9129 - val_mse: 0.0607\n","Epoch 75/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7522 - mse: 1.6330 \n","Epoch 75: val_loss did not improve from 2.91287\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7537 - mse: 1.6419 - val_loss: 2.9172 - val_mse: 0.0825\n","Epoch 76/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6173 - mse: 1.3807 \n","Epoch 76: val_loss did not improve from 2.91287\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6208 - mse: 1.3920 - val_loss: 2.9367 - val_mse: 0.1022\n","Epoch 77/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6972 - mse: 1.6960 \n","Epoch 77: val_loss improved from 2.91287 to 2.83798, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6934 - mse: 1.6841 - val_loss: 2.8380 - val_mse: 0.0646\n","Epoch 78/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5823 - mse: 1.4164 \n","Epoch 78: val_loss did not improve from 2.83798\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5829 - mse: 1.4207 - val_loss: 2.9345 - val_mse: 0.1390\n","Epoch 79/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5596 - mse: 1.4929 \n","Epoch 79: val_loss improved from 2.83798 to 2.82915, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5607 - mse: 1.4922 - val_loss: 2.8292 - val_mse: 0.0912\n","Epoch 80/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5352 - mse: 1.3530 \n","Epoch 80: val_loss improved from 2.82915 to 2.75254, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5350 - mse: 1.3558 - val_loss: 2.7525 - val_mse: 0.0636\n","Epoch 81/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4888 - mse: 1.3136 \n","Epoch 81: val_loss improved from 2.75254 to 2.69999, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4887 - mse: 1.3187 - val_loss: 2.7000 - val_mse: 0.0467\n","Epoch 82/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4744 - mse: 1.4600 \n","Epoch 82: val_loss did not improve from 2.69999\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4742 - mse: 1.4630 - val_loss: 2.7107 - val_mse: 0.0665\n","Epoch 83/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4799 - mse: 1.4550 \n","Epoch 83: val_loss did not improve from 2.69999\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4782 - mse: 1.4500 - val_loss: 2.7083 - val_mse: 0.0805\n","Epoch 84/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4192 - mse: 1.3792 \n","Epoch 84: val_loss improved from 2.69999 to 2.68814, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4193 - mse: 1.3820 - val_loss: 2.6881 - val_mse: 0.0826\n","Epoch 85/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3747 - mse: 1.3211 \n","Epoch 85: val_loss improved from 2.68814 to 2.61881, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3726 - mse: 1.3153 - val_loss: 2.6188 - val_mse: 0.0610\n","Epoch 86/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3492 - mse: 1.3876 \n","Epoch 86: val_loss did not improve from 2.61881\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3517 - mse: 1.3966 - val_loss: 2.6342 - val_mse: 0.0776\n","Epoch 87/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3477 - mse: 1.3895 \n","Epoch 87: val_loss improved from 2.61881 to 2.54951, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3472 - mse: 1.3880 - val_loss: 2.5495 - val_mse: 0.0470\n","Epoch 88/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3490 - mse: 1.4470 \n","Epoch 88: val_loss did not improve from 2.54951\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3446 - mse: 1.4387 - val_loss: 2.6610 - val_mse: 0.1193\n","Epoch 89/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2705 - mse: 1.3732 \n","Epoch 89: val_loss did not improve from 2.54951\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2755 - mse: 1.3835 - val_loss: 2.5718 - val_mse: 0.0806\n","Epoch 90/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1870 - mse: 1.1802 \n","Epoch 90: val_loss improved from 2.54951 to 2.47501, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1925 - mse: 1.1947 - val_loss: 2.4750 - val_mse: 0.0434\n","Epoch 91/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2844 - mse: 1.4747 \n","Epoch 91: val_loss did not improve from 2.47501\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2791 - mse: 1.4615 - val_loss: 2.5140 - val_mse: 0.0738\n","Epoch 92/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2148 - mse: 1.3219 \n","Epoch 92: val_loss improved from 2.47501 to 2.43111, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2148 - mse: 1.3246 - val_loss: 2.4311 - val_mse: 0.0477\n","Epoch 93/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2011 - mse: 1.3935 \n","Epoch 93: val_loss improved from 2.43111 to 2.39478, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2010 - mse: 1.3940 - val_loss: 2.3948 - val_mse: 0.0398\n","Epoch 94/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1155 - mse: 1.1988 \n","Epoch 94: val_loss did not improve from 2.39478\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1165 - mse: 1.2028 - val_loss: 2.5191 - val_mse: 0.1156\n","Epoch 95/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0789 - mse: 1.1606 \n","Epoch 95: val_loss improved from 2.39478 to 2.37821, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0795 - mse: 1.1640 - val_loss: 2.3782 - val_mse: 0.0524\n","Epoch 96/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0178 - mse: 1.1352 \n","Epoch 96: val_loss did not improve from 2.37821\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0175 - mse: 1.1326 - val_loss: 2.4203 - val_mse: 0.0847\n","Epoch 97/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9965 - mse: 1.0762 \n","Epoch 97: val_loss improved from 2.37821 to 2.34581, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9994 - mse: 1.0861 - val_loss: 2.3458 - val_mse: 0.0604\n","Epoch 98/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0440 - mse: 1.2417 \n","Epoch 98: val_loss improved from 2.34581 to 2.34053, saving model to best_model_SGD_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0433 - mse: 1.2428 - val_loss: 2.3405 - val_mse: 0.0659\n","Epoch 99/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9566 - mse: 1.0842 \n","Epoch 99: val_loss did not improve from 2.34053\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9573 - mse: 1.0918 - val_loss: 2.3885 - val_mse: 0.1020\n","Epoch 100/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9786 - mse: 1.1672 \n","Epoch 100: val_loss did not improve from 2.34053\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9749 - mse: 1.1638 - val_loss: 2.3951 - val_mse: 0.1208\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: SGD, Loss Function: mae, Epochs: 100, Test MSE: 0.08966132446177055\n","Training with optimizer: SGD, loss function: huber, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 16.8927 - mse: 143.1647\n","Epoch 1: val_loss improved from inf to 16.27892, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 316ms/step - loss: 16.8871 - mse: 143.0458 - val_loss: 16.2789 - val_mse: 126.5414\n","Epoch 2/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.0498 - mse: 125.3168 \n","Epoch 2: val_loss improved from 16.27892 to 15.30585, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.0176 - mse: 124.6966 - val_loss: 15.3059 - val_mse: 106.8032\n","Epoch 3/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.0150 - mse: 105.2181 \n","Epoch 3: val_loss improved from 15.30585 to 14.01817, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.9973 - mse: 104.9318 - val_loss: 14.0182 - val_mse: 82.9557\n","Epoch 4/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.6616 - mse: 83.1169 \n","Epoch 4: val_loss improved from 14.01817 to 12.13536, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.6337 - mse: 82.7183 - val_loss: 12.1354 - val_mse: 53.2553\n","Epoch 5/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.8740 - mse: 57.8960 \n","Epoch 5: val_loss improved from 12.13536 to 9.59851, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.8333 - mse: 57.3893 - val_loss: 9.5985 - val_mse: 23.7844\n","Epoch 6/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4975 - mse: 31.3062  \n","Epoch 6: val_loss improved from 9.59851 to 7.10421, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.4690 - mse: 30.9917 - val_loss: 7.1042 - val_mse: 6.7734\n","Epoch 7/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1380 - mse: 17.2521 \n","Epoch 7: val_loss improved from 7.10421 to 6.02118, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1305 - mse: 17.2310 - val_loss: 6.0212 - val_mse: 2.4650\n","Epoch 8/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7446 - mse: 14.2728 \n","Epoch 8: val_loss improved from 6.02118 to 5.74828, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7295 - mse: 14.1006 - val_loss: 5.7483 - val_mse: 1.4953\n","Epoch 9/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2980 - mse: 10.5554\n","Epoch 9: val_loss improved from 5.74828 to 5.54574, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.2937 - mse: 10.5275 - val_loss: 5.5457 - val_mse: 0.9056\n","Epoch 10/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0354 - mse: 8.6426 \n","Epoch 10: val_loss improved from 5.54574 to 5.48171, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.0356 - mse: 8.6526 - val_loss: 5.4817 - val_mse: 0.8742\n","Epoch 11/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9473 - mse: 7.9964  \n","Epoch 11: val_loss improved from 5.48171 to 5.36214, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9329 - mse: 7.9224 - val_loss: 5.3621 - val_mse: 0.6383\n","Epoch 12/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7091 - mse: 6.9724 \n","Epoch 12: val_loss improved from 5.36214 to 5.32161, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7133 - mse: 6.9980 - val_loss: 5.3216 - val_mse: 0.6519\n","Epoch 13/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5671 - mse: 6.3204 \n","Epoch 13: val_loss improved from 5.32161 to 5.23050, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5580 - mse: 6.2677 - val_loss: 5.2305 - val_mse: 0.5436\n","Epoch 14/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4632 - mse: 6.0175 \n","Epoch 14: val_loss improved from 5.23050 to 5.17138, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4559 - mse: 5.9722 - val_loss: 5.1714 - val_mse: 0.5535\n","Epoch 15/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3584 - mse: 5.5241 \n","Epoch 15: val_loss improved from 5.17138 to 5.12840, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3530 - mse: 5.5057 - val_loss: 5.1284 - val_mse: 0.5483\n","Epoch 16/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3811 - mse: 5.8915 \n","Epoch 16: val_loss improved from 5.12840 to 5.04431, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.3688 - mse: 5.8175 - val_loss: 5.0443 - val_mse: 0.4598\n","Epoch 17/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1175 - mse: 4.7646 \n","Epoch 17: val_loss improved from 5.04431 to 4.97975, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.1210 - mse: 4.7813 - val_loss: 4.9798 - val_mse: 0.3643\n","Epoch 18/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1254 - mse: 5.0463 \n","Epoch 18: val_loss improved from 4.97975 to 4.94729, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.1203 - mse: 5.0205 - val_loss: 4.9473 - val_mse: 0.3930\n","Epoch 19/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0145 - mse: 4.5628 \n","Epoch 19: val_loss improved from 4.94729 to 4.87868, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0136 - mse: 4.5654 - val_loss: 4.8787 - val_mse: 0.3613\n","Epoch 20/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0376 - mse: 4.9902 \n","Epoch 20: val_loss improved from 4.87868 to 4.83579, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0301 - mse: 4.9625 - val_loss: 4.8358 - val_mse: 0.3598\n","Epoch 21/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8440 - mse: 4.2806 \n","Epoch 21: val_loss improved from 4.83579 to 4.76651, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8422 - mse: 4.2754 - val_loss: 4.7665 - val_mse: 0.3010\n","Epoch 22/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7850 - mse: 4.0460 \n","Epoch 22: val_loss improved from 4.76651 to 4.72057, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7814 - mse: 4.0413 - val_loss: 4.7206 - val_mse: 0.2892\n","Epoch 23/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7209 - mse: 3.9366 \n","Epoch 23: val_loss improved from 4.72057 to 4.66826, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7211 - mse: 3.9479 - val_loss: 4.6683 - val_mse: 0.2795\n","Epoch 24/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5875 - mse: 3.4782 \n","Epoch 24: val_loss improved from 4.66826 to 4.62277, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5914 - mse: 3.5026 - val_loss: 4.6228 - val_mse: 0.2757\n","Epoch 25/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6570 - mse: 4.2146 \n","Epoch 25: val_loss improved from 4.62277 to 4.54443, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6557 - mse: 4.2084 - val_loss: 4.5444 - val_mse: 0.1926\n","Epoch 26/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4677 - mse: 3.5358 \n","Epoch 26: val_loss improved from 4.54443 to 4.53756, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4690 - mse: 3.5460 - val_loss: 4.5376 - val_mse: 0.2689\n","Epoch 27/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3936 - mse: 3.3094 \n","Epoch 27: val_loss improved from 4.53756 to 4.46851, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3940 - mse: 3.3121 - val_loss: 4.4685 - val_mse: 0.2166\n","Epoch 28/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3947 - mse: 3.6062 \n","Epoch 28: val_loss improved from 4.46851 to 4.41133, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3938 - mse: 3.5971 - val_loss: 4.4113 - val_mse: 0.1889\n","Epoch 29/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3238 - mse: 3.2955 \n","Epoch 29: val_loss improved from 4.41133 to 4.37158, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3223 - mse: 3.2907 - val_loss: 4.3716 - val_mse: 0.1917\n","Epoch 30/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3056 - mse: 3.4770 \n","Epoch 30: val_loss improved from 4.37158 to 4.33129, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3041 - mse: 3.4687 - val_loss: 4.3313 - val_mse: 0.1962\n","Epoch 31/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2183 - mse: 3.1748 \n","Epoch 31: val_loss improved from 4.33129 to 4.33075, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2186 - mse: 3.1787 - val_loss: 4.3307 - val_mse: 0.2798\n","Epoch 32/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2031 - mse: 3.2238 \n","Epoch 32: val_loss improved from 4.33075 to 4.25799, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1892 - mse: 3.1701 - val_loss: 4.2580 - val_mse: 0.2194\n","Epoch 33/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0187 - mse: 2.7076 \n","Epoch 33: val_loss improved from 4.25799 to 4.19240, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0253 - mse: 2.7497 - val_loss: 4.1924 - val_mse: 0.1687\n","Epoch 34/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0667 - mse: 3.0355 \n","Epoch 34: val_loss improved from 4.19240 to 4.15932, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0634 - mse: 3.0314 - val_loss: 4.1593 - val_mse: 0.1854\n","Epoch 35/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0119 - mse: 3.0316 \n","Epoch 35: val_loss improved from 4.15932 to 4.09189, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0125 - mse: 3.0372 - val_loss: 4.0919 - val_mse: 0.1303\n","Epoch 36/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8982 - mse: 2.7571 \n","Epoch 36: val_loss improved from 4.09189 to 4.06875, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8986 - mse: 2.7581 - val_loss: 4.0687 - val_mse: 0.1660\n","Epoch 37/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8727 - mse: 2.8646 \n","Epoch 37: val_loss improved from 4.06875 to 4.04453, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8740 - mse: 2.8704 - val_loss: 4.0445 - val_mse: 0.1976\n","Epoch 38/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7426 - mse: 2.4094 \n","Epoch 38: val_loss improved from 4.04453 to 3.97636, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7414 - mse: 2.4080 - val_loss: 3.9764 - val_mse: 0.1402\n","Epoch 39/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8360 - mse: 2.8994 \n","Epoch 39: val_loss improved from 3.97636 to 3.92038, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8348 - mse: 2.9174 - val_loss: 3.9204 - val_mse: 0.1064\n","Epoch 40/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7154 - mse: 2.5547  \n","Epoch 40: val_loss improved from 3.92038 to 3.87982, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7154 - mse: 2.5660 - val_loss: 3.8798 - val_mse: 0.1033\n","Epoch 41/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6147 - mse: 2.3252 \n","Epoch 41: val_loss improved from 3.87982 to 3.85429, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6185 - mse: 2.3551 - val_loss: 3.8543 - val_mse: 0.1299\n","Epoch 42/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5962 - mse: 2.4406 \n","Epoch 42: val_loss improved from 3.85429 to 3.83465, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5882 - mse: 2.4145 - val_loss: 3.8346 - val_mse: 0.1693\n","Epoch 43/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5645 - mse: 2.3654 \n","Epoch 43: val_loss improved from 3.83465 to 3.77895, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5656 - mse: 2.3745 - val_loss: 3.7790 - val_mse: 0.1331\n","Epoch 44/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5381 - mse: 2.4776 \n","Epoch 44: val_loss improved from 3.77895 to 3.76950, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5348 - mse: 2.4663 - val_loss: 3.7695 - val_mse: 0.1914\n","Epoch 45/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4453 - mse: 2.2187 \n","Epoch 45: val_loss improved from 3.76950 to 3.69959, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4453 - mse: 2.2237 - val_loss: 3.6996 - val_mse: 0.1232\n","Epoch 46/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4220 - mse: 2.2924 \n","Epoch 46: val_loss improved from 3.69959 to 3.69200, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4207 - mse: 2.2918 - val_loss: 3.6920 - val_mse: 0.1798\n","Epoch 47/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4428 - mse: 2.5755 \n","Epoch 47: val_loss improved from 3.69200 to 3.64013, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4432 - mse: 2.5790 - val_loss: 3.6401 - val_mse: 0.1494\n","Epoch 48/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2793 - mse: 2.0094 \n","Epoch 48: val_loss improved from 3.64013 to 3.58863, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2807 - mse: 2.0168 - val_loss: 3.5886 - val_mse: 0.1173\n","Epoch 49/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3047 - mse: 2.2687 \n","Epoch 49: val_loss improved from 3.58863 to 3.54948, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3048 - mse: 2.2737 - val_loss: 3.5495 - val_mse: 0.1105\n","Epoch 50/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2222 - mse: 2.1026 \n","Epoch 50: val_loss improved from 3.54948 to 3.51055, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2191 - mse: 2.0952 - val_loss: 3.5106 - val_mse: 0.1038\n","Epoch 51/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2065 - mse: 2.1831 \n","Epoch 51: val_loss improved from 3.51055 to 3.48053, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2057 - mse: 2.1758 - val_loss: 3.4805 - val_mse: 0.1138\n","Epoch 52/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1275 - mse: 1.9513 \n","Epoch 52: val_loss improved from 3.48053 to 3.45123, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1265 - mse: 1.9541 - val_loss: 3.4512 - val_mse: 0.1247\n","Epoch 53/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0562 - mse: 1.8458 \n","Epoch 53: val_loss improved from 3.45123 to 3.40870, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0576 - mse: 1.8531 - val_loss: 3.4087 - val_mse: 0.1083\n","Epoch 54/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1349 - mse: 2.2531 \n","Epoch 54: val_loss improved from 3.40870 to 3.35489, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1315 - mse: 2.2428 - val_loss: 3.3549 - val_mse: 0.0687\n","Epoch 55/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0746 - mse: 2.1766 \n","Epoch 55: val_loss improved from 3.35489 to 3.33965, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0710 - mse: 2.1633 - val_loss: 3.3397 - val_mse: 0.1056\n","Epoch 56/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9689 - mse: 1.9043 \n","Epoch 56: val_loss improved from 3.33965 to 3.30090, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9690 - mse: 1.9071 - val_loss: 3.3009 - val_mse: 0.0948\n","Epoch 57/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9325 - mse: 1.9168 \n","Epoch 57: val_loss improved from 3.30090 to 3.26382, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9331 - mse: 1.9196 - val_loss: 3.2638 - val_mse: 0.0869\n","Epoch 58/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8631 - mse: 1.7571 \n","Epoch 58: val_loss improved from 3.26382 to 3.23117, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8649 - mse: 1.7658 - val_loss: 3.2312 - val_mse: 0.0871\n","Epoch 59/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9351 - mse: 2.1188 \n","Epoch 59: val_loss improved from 3.23117 to 3.20320, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9313 - mse: 2.1093 - val_loss: 3.2032 - val_mse: 0.0960\n","Epoch 60/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7340 - mse: 1.5225 \n","Epoch 60: val_loss improved from 3.20320 to 3.17532, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7390 - mse: 1.5440 - val_loss: 3.1753 - val_mse: 0.1044\n","Epoch 61/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7727 - mse: 1.9059 \n","Epoch 61: val_loss improved from 3.17532 to 3.12601, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7725 - mse: 1.8897 - val_loss: 3.1260 - val_mse: 0.0692\n","Epoch 62/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7782 - mse: 1.9404 \n","Epoch 62: val_loss improved from 3.12601 to 3.09466, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7751 - mse: 1.9278 - val_loss: 3.0947 - val_mse: 0.0695\n","Epoch 63/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6962 - mse: 1.7272 \n","Epoch 63: val_loss improved from 3.09466 to 3.08573, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6934 - mse: 1.7214 - val_loss: 3.0857 - val_mse: 0.1139\n","Epoch 64/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7227 - mse: 1.9286 \n","Epoch 64: val_loss improved from 3.08573 to 3.04244, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7209 - mse: 1.9276 - val_loss: 3.0424 - val_mse: 0.0890\n","Epoch 65/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6692 - mse: 1.8608 \n","Epoch 65: val_loss improved from 3.04244 to 3.00875, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6665 - mse: 1.8546 - val_loss: 3.0088 - val_mse: 0.0826\n","Epoch 66/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6082 - mse: 1.7470 \n","Epoch 66: val_loss improved from 3.00875 to 2.96646, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6104 - mse: 1.7573 - val_loss: 2.9665 - val_mse: 0.0585\n","Epoch 67/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5457 - mse: 1.6305 \n","Epoch 67: val_loss improved from 2.96646 to 2.96575, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5475 - mse: 1.6375 - val_loss: 2.9658 - val_mse: 0.1170\n","Epoch 68/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5823 - mse: 1.8417 \n","Epoch 68: val_loss improved from 2.96575 to 2.91628, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5772 - mse: 1.8301 - val_loss: 2.9163 - val_mse: 0.0772\n","Epoch 69/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5548 - mse: 1.8658 \n","Epoch 69: val_loss improved from 2.91628 to 2.87740, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5517 - mse: 1.8573 - val_loss: 2.8774 - val_mse: 0.0581\n","Epoch 70/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4576 - mse: 1.6579 \n","Epoch 70: val_loss improved from 2.87740 to 2.85523, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4631 - mse: 1.6807 - val_loss: 2.8552 - val_mse: 0.0718\n","Epoch 71/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4209 - mse: 1.5514 \n","Epoch 71: val_loss improved from 2.85523 to 2.81780, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4187 - mse: 1.5494 - val_loss: 2.8178 - val_mse: 0.0544\n","Epoch 72/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3716 - mse: 1.5695 \n","Epoch 72: val_loss improved from 2.81780 to 2.79176, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3730 - mse: 1.5771 - val_loss: 2.7918 - val_mse: 0.0593\n","Epoch 73/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3782 - mse: 1.6785 \n","Epoch 73: val_loss improved from 2.79176 to 2.78082, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3774 - mse: 1.6757 - val_loss: 2.7808 - val_mse: 0.0936\n","Epoch 74/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3423 - mse: 1.6506 \n","Epoch 74: val_loss improved from 2.78082 to 2.74365, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3411 - mse: 1.6478 - val_loss: 2.7437 - val_mse: 0.0751\n","Epoch 75/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3429 - mse: 1.7177 \n","Epoch 75: val_loss improved from 2.74365 to 2.70991, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3441 - mse: 1.7241 - val_loss: 2.7099 - val_mse: 0.0628\n","Epoch 76/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2326 - mse: 1.4487 \n","Epoch 76: val_loss did not improve from 2.70991\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2335 - mse: 1.4544 - val_loss: 2.7128 - val_mse: 0.1232\n","Epoch 77/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2596 - mse: 1.6069 \n","Epoch 77: val_loss improved from 2.70991 to 2.65104, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2585 - mse: 1.6033 - val_loss: 2.6510 - val_mse: 0.0538\n","Epoch 78/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1834 - mse: 1.4267 \n","Epoch 78: val_loss did not improve from 2.65104\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1853 - mse: 1.4361 - val_loss: 2.6519 - val_mse: 0.1091\n","Epoch 79/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1793 - mse: 1.6228 \n","Epoch 79: val_loss improved from 2.65104 to 2.60608, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1793 - mse: 1.6214 - val_loss: 2.6061 - val_mse: 0.0705\n","Epoch 80/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1073 - mse: 1.3743 \n","Epoch 80: val_loss improved from 2.60608 to 2.57505, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1084 - mse: 1.3795 - val_loss: 2.5750 - val_mse: 0.0610\n","Epoch 81/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1033 - mse: 1.4298 \n","Epoch 81: val_loss improved from 2.57505 to 2.54751, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1040 - mse: 1.4410 - val_loss: 2.5475 - val_mse: 0.0579\n","Epoch 82/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0965 - mse: 1.5473 \n","Epoch 82: val_loss improved from 2.54751 to 2.52319, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0944 - mse: 1.5422 - val_loss: 2.5232 - val_mse: 0.0607\n","Epoch 83/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0528 - mse: 1.4396 \n","Epoch 83: val_loss improved from 2.52319 to 2.49605, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0533 - mse: 1.4431 - val_loss: 2.4961 - val_mse: 0.0573\n","Epoch 84/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0453 - mse: 1.5146 \n","Epoch 84: val_loss improved from 2.49605 to 2.46685, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0445 - mse: 1.5118 - val_loss: 2.4669 - val_mse: 0.0493\n","Epoch 85/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9485 - mse: 1.2865 \n","Epoch 85: val_loss improved from 2.46685 to 2.45892, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9474 - mse: 1.2839 - val_loss: 2.4589 - val_mse: 0.0834\n","Epoch 86/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9575 - mse: 1.4366 \n","Epoch 86: val_loss improved from 2.45892 to 2.43717, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9605 - mse: 1.4483 - val_loss: 2.4372 - val_mse: 0.0892\n","Epoch 87/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9587 - mse: 1.4618 \n","Epoch 87: val_loss improved from 2.43717 to 2.39555, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9584 - mse: 1.4612 - val_loss: 2.3955 - val_mse: 0.0549\n","Epoch 88/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9460 - mse: 1.5208 \n","Epoch 88: val_loss improved from 2.39555 to 2.38195, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9447 - mse: 1.5190 - val_loss: 2.3819 - val_mse: 0.0762\n","Epoch 89/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9000 - mse: 1.4617 \n","Epoch 89: val_loss improved from 2.38195 to 2.34744, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9007 - mse: 1.4633 - val_loss: 2.3474 - val_mse: 0.0550\n","Epoch 90/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7991 - mse: 1.1880 \n","Epoch 90: val_loss improved from 2.34744 to 2.32292, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8018 - mse: 1.1979 - val_loss: 2.3229 - val_mse: 0.0534\n","Epoch 91/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8576 - mse: 1.4298 \n","Epoch 91: val_loss improved from 2.32292 to 2.29997, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8501 - mse: 1.4086 - val_loss: 2.3000 - val_mse: 0.0545\n","Epoch 92/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8275 - mse: 1.4331 \n","Epoch 92: val_loss improved from 2.29997 to 2.27408, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8241 - mse: 1.4258 - val_loss: 2.2741 - val_mse: 0.0492\n","Epoch 93/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7731 - mse: 1.3663 \n","Epoch 93: val_loss improved from 2.27408 to 2.25518, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7748 - mse: 1.3713 - val_loss: 2.2552 - val_mse: 0.0574\n","Epoch 94/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7175 - mse: 1.2343 \n","Epoch 94: val_loss improved from 2.25518 to 2.25090, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7219 - mse: 1.2509 - val_loss: 2.2509 - val_mse: 0.0943\n","Epoch 95/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7339 - mse: 1.3549 \n","Epoch 95: val_loss improved from 2.25090 to 2.21587, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7335 - mse: 1.3559 - val_loss: 2.2159 - val_mse: 0.0693\n","Epoch 96/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6551 - mse: 1.2074 \n","Epoch 96: val_loss improved from 2.21587 to 2.20462, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6552 - mse: 1.2091 - val_loss: 2.2046 - val_mse: 0.0915\n","Epoch 97/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6411 - mse: 1.2565 \n","Epoch 97: val_loss improved from 2.20462 to 2.17523, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6411 - mse: 1.2550 - val_loss: 2.1752 - val_mse: 0.0769\n","Epoch 98/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6782 - mse: 1.3839 \n","Epoch 98: val_loss improved from 2.17523 to 2.15438, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6774 - mse: 1.3832 - val_loss: 2.1544 - val_mse: 0.0789\n","Epoch 99/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5720 - mse: 1.1102 \n","Epoch 99: val_loss improved from 2.15438 to 2.13158, saving model to best_model_SGD_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5734 - mse: 1.1173 - val_loss: 2.1316 - val_mse: 0.0766\n","Epoch 100/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5739 - mse: 1.1985 \n","Epoch 100: val_loss did not improve from 2.13158\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5726 - mse: 1.1970 - val_loss: 2.1354 - val_mse: 0.1271\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: SGD, Loss Function: huber, Epochs: 100, Test MSE: 0.09111703221590442\n","Training with optimizer: RMSprop, loss function: mse, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 150.7485 - mse: 145.1640\n","Epoch 1: val_loss improved from inf to 142.87920, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 296ms/step - loss: 150.7119 - mse: 145.1277 - val_loss: 142.8792 - val_mse: 137.3174\n","Epoch 2/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 145.4148 - mse: 139.8567 \n","Epoch 2: val_loss improved from 142.87920 to 137.55307, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 145.1432 - mse: 139.5860 - val_loss: 137.5531 - val_mse: 132.0095\n","Epoch 3/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.7395 - mse: 133.1991 \n","Epoch 3: val_loss improved from 137.55307 to 131.90288, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 138.7160 - mse: 133.1763 - val_loss: 131.9029 - val_mse: 126.3718\n","Epoch 4/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 134.4144 - mse: 128.8848 \n","Epoch 4: val_loss improved from 131.90288 to 126.26064, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 134.2324 - mse: 128.7030 - val_loss: 126.2606 - val_mse: 120.7366\n","Epoch 5/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 130.1794 - mse: 124.6563 \n","Epoch 5: val_loss improved from 126.26064 to 120.95538, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 129.8726 - mse: 124.3497 - val_loss: 120.9554 - val_mse: 115.4332\n","Epoch 6/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 122.8023 - mse: 117.2824 \n","Epoch 6: val_loss improved from 120.95538 to 114.99468, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 122.6778 - mse: 117.1585 - val_loss: 114.9947 - val_mse: 109.4807\n","Epoch 7/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 112.9549 - mse: 107.4420 \n","Epoch 7: val_loss improved from 114.99468 to 107.25484, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 112.7152 - mse: 107.2027 - val_loss: 107.2548 - val_mse: 101.7483\n","Epoch 8/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 106.8859 - mse: 101.3806 \n","Epoch 8: val_loss improved from 107.25484 to 98.67563, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 106.6253 - mse: 101.1203 - val_loss: 98.6756 - val_mse: 93.1733\n","Epoch 9/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 102.4423 - mse: 96.9411  \n","Epoch 9: val_loss improved from 98.67563 to 89.85539, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 101.9776 - mse: 96.4766 - val_loss: 89.8554 - val_mse: 84.3593\n","Epoch 10/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90.5885 - mse: 85.0923 \n","Epoch 10: val_loss improved from 89.85539 to 80.43270, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 90.2706 - mse: 84.7746 - val_loss: 80.4327 - val_mse: 74.9384\n","Epoch 11/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83.1524 - mse: 77.6588 \n","Epoch 11: val_loss improved from 80.43270 to 71.45049, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 82.8148 - mse: 77.3213 - val_loss: 71.4505 - val_mse: 65.9581\n","Epoch 12/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.7647 - mse: 68.2728 \n","Epoch 12: val_loss improved from 71.45049 to 64.34290, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 73.3303 - mse: 67.8385 - val_loss: 64.3429 - val_mse: 58.8536\n","Epoch 13/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.0655 - mse: 61.5762 \n","Epoch 13: val_loss improved from 64.34290 to 55.41885, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 66.5691 - mse: 61.0799 - val_loss: 55.4188 - val_mse: 49.9301\n","Epoch 14/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55.7819 - mse: 50.2948 \n","Epoch 14: val_loss improved from 55.41885 to 46.09888, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 55.5787 - mse: 50.0919 - val_loss: 46.0989 - val_mse: 40.6169\n","Epoch 15/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47.7898 - mse: 42.3096 \n","Epoch 15: val_loss improved from 46.09888 to 38.32965, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 47.4998 - mse: 42.0199 - val_loss: 38.3297 - val_mse: 32.8525\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.6395 - mse: 35.1628 \n","Epoch 16: val_loss improved from 38.32965 to 31.33006, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 40.2503 - mse: 34.7739 - val_loss: 31.3301 - val_mse: 25.8567\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34.5954 - mse: 29.1241 \n","Epoch 17: val_loss improved from 31.33006 to 23.98330, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 34.0658 - mse: 28.5952 - val_loss: 23.9833 - val_mse: 18.5203\n","Epoch 18/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.0429 - mse: 21.5822 \n","Epoch 18: val_loss improved from 23.98330 to 17.20762, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.8888 - mse: 21.4284 - val_loss: 17.2076 - val_mse: 11.7552\n","Epoch 19/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.4135 - mse: 15.9639 \n","Epoch 19: val_loss improved from 17.20762 to 11.80914, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.3119 - mse: 15.8626 - val_loss: 11.8091 - val_mse: 6.3665\n","Epoch 20/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.8403 - mse: 12.4008 \n","Epoch 20: val_loss improved from 11.80914 to 9.01512, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.6767 - mse: 12.2378 - val_loss: 9.0151 - val_mse: 3.5878\n","Epoch 21/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.6606 - mse: 11.2365 \n","Epoch 21: val_loss improved from 9.01512 to 7.33739, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.2658 - mse: 10.8426 - val_loss: 7.3374 - val_mse: 1.9280\n","Epoch 22/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.8416 - mse: 9.4352  \n","Epoch 22: val_loss improved from 7.33739 to 6.45492, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.7023 - mse: 9.2966 - val_loss: 6.4549 - val_mse: 1.0633\n","Epoch 23/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.5679 - mse: 7.1801 \n","Epoch 23: val_loss improved from 6.45492 to 5.81348, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.4729 - mse: 7.0859 - val_loss: 5.8135 - val_mse: 0.4443\n","Epoch 24/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.7959 - mse: 6.4317 \n","Epoch 24: val_loss did not improve from 5.81348\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.7882 - mse: 6.4251 - val_loss: 5.8923 - val_mse: 0.5484\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.2370 - mse: 6.8984 \n","Epoch 25: val_loss improved from 5.81348 to 5.80205, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.1466 - mse: 6.8100 - val_loss: 5.8021 - val_mse: 0.4863\n","Epoch 26/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.5697 - mse: 6.2596 \n","Epoch 26: val_loss improved from 5.80205 to 5.63835, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.5465 - mse: 6.2380 - val_loss: 5.6384 - val_mse: 0.3516\n","Epoch 27/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.4680 - mse: 6.1879 \n","Epoch 27: val_loss improved from 5.63835 to 5.61434, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.4692 - mse: 6.1905 - val_loss: 5.6143 - val_mse: 0.3575\n","Epoch 28/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6778 - mse: 5.4288\n","Epoch 28: val_loss improved from 5.61434 to 5.46675, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6675 - mse: 5.4204 - val_loss: 5.4667 - val_mse: 0.2495\n","Epoch 29/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.1905 - mse: 5.9788 \n","Epoch 29: val_loss did not improve from 5.46675\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.9703 - mse: 5.7623 - val_loss: 5.5630 - val_mse: 0.3867\n","Epoch 30/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1434 - mse: 4.9752 \n","Epoch 30: val_loss improved from 5.46675 to 5.41746, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.1758 - mse: 5.0104 - val_loss: 5.4175 - val_mse: 0.2837\n","Epoch 31/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.7256 - mse: 4.6008  \n","Epoch 31: val_loss improved from 5.41746 to 5.30310, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.7125 - mse: 4.5905 - val_loss: 5.3031 - val_mse: 0.2162\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6363 - mse: 4.5580 \n","Epoch 32: val_loss did not improve from 5.30310\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.7258 - mse: 4.6507 - val_loss: 5.3727 - val_mse: 0.3340\n","Epoch 33/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2768 - mse: 4.2484 \n","Epoch 33: val_loss improved from 5.30310 to 5.27636, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.3125 - mse: 4.2869 - val_loss: 5.2764 - val_mse: 0.2880\n","Epoch 34/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2441 - mse: 4.2639  \n","Epoch 34: val_loss did not improve from 5.27636\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.2724 - mse: 4.2964 - val_loss: 5.3681 - val_mse: 0.4285\n","Epoch 35/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3708 - mse: 4.4395  \n","Epoch 35: val_loss improved from 5.27636 to 5.08601, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2958 - mse: 4.3690 - val_loss: 5.0860 - val_mse: 0.1991\n","Epoch 36/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.8445 - mse: 3.9687 \n","Epoch 36: val_loss improved from 5.08601 to 5.06004, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9747 - mse: 4.1026 - val_loss: 5.0600 - val_mse: 0.2314\n","Epoch 37/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0993 - mse: 4.2815  \n","Epoch 37: val_loss did not improve from 5.06004\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.0644 - mse: 4.2493 - val_loss: 5.0602 - val_mse: 0.2894\n","Epoch 38/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2000 - mse: 3.4441 \n","Epoch 38: val_loss improved from 5.06004 to 4.87917, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2343 - mse: 3.4811 - val_loss: 4.8792 - val_mse: 0.1778\n","Epoch 39/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9372 - mse: 4.2498 \n","Epoch 39: val_loss improved from 4.87917 to 4.75568, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9532 - mse: 4.2682 - val_loss: 4.7557 - val_mse: 0.1186\n","Epoch 40/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1712 - mse: 3.5487 \n","Epoch 40: val_loss did not improve from 4.75568\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1503 - mse: 3.5314 - val_loss: 4.7667 - val_mse: 0.2073\n","Epoch 41/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8981 - mse: 3.3556 \n","Epoch 41: val_loss improved from 4.75568 to 4.67162, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9270 - mse: 3.3874 - val_loss: 4.6716 - val_mse: 0.1893\n","Epoch 42/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3673 - mse: 3.9013 \n","Epoch 42: val_loss improved from 4.67162 to 4.53108, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2929 - mse: 3.8299 - val_loss: 4.5311 - val_mse: 0.1303\n","Epoch 43/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7733 - mse: 3.3903 \n","Epoch 43: val_loss improved from 4.53108 to 4.47162, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7659 - mse: 3.3861 - val_loss: 4.4716 - val_mse: 0.1570\n","Epoch 44/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5059 - mse: 3.2082 \n","Epoch 44: val_loss did not improve from 4.47162\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.5453 - mse: 3.2535 - val_loss: 4.5246 - val_mse: 0.2983\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3253 - mse: 3.1156 \n","Epoch 45: val_loss improved from 4.47162 to 4.30505, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3397 - mse: 3.1360 - val_loss: 4.3050 - val_mse: 0.1680\n","Epoch 46/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4740 - mse: 3.3557 \n","Epoch 46: val_loss improved from 4.30505 to 4.20577, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4545 - mse: 3.3405 - val_loss: 4.2058 - val_mse: 0.1605\n","Epoch 47/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1527 - mse: 3.1259 \n","Epoch 47: val_loss improved from 4.20577 to 4.12446, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1902 - mse: 3.1688 - val_loss: 4.1245 - val_mse: 0.1734\n","Epoch 48/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7001 - mse: 2.7695 \n","Epoch 48: val_loss improved from 4.12446 to 3.92867, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6835 - mse: 2.7577 - val_loss: 3.9287 - val_mse: 0.0835\n","Epoch 49/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0174 - mse: 3.1919 \n","Epoch 49: val_loss improved from 3.92867 to 3.85125, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0071 - mse: 3.1861 - val_loss: 3.8513 - val_mse: 0.1050\n","Epoch 50/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3260 - mse: 2.6013 \n","Epoch 50: val_loss improved from 3.85125 to 3.79212, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3262 - mse: 2.6064 - val_loss: 3.7921 - val_mse: 0.1511\n","Epoch 51/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5888 - mse: 2.9684 \n","Epoch 51: val_loss improved from 3.79212 to 3.62531, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5730 - mse: 2.9564 - val_loss: 3.6253 - val_mse: 0.0846\n","Epoch 52/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3005 - mse: 2.7818 \n","Epoch 52: val_loss improved from 3.62531 to 3.57615, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2774 - mse: 2.7627 - val_loss: 3.5762 - val_mse: 0.1445\n","Epoch 53/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1396 - mse: 2.7278 \n","Epoch 53: val_loss improved from 3.57615 to 3.41183, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1246 - mse: 2.7202 - val_loss: 3.4118 - val_mse: 0.0913\n","Epoch 54/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3941 - mse: 3.0928 \n","Epoch 54: val_loss improved from 3.41183 to 3.31755, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3780 - mse: 3.0836 - val_loss: 3.3175 - val_mse: 0.0988\n","Epoch 55/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9713 - mse: 2.7727 \n","Epoch 55: val_loss improved from 3.31755 to 3.23863, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9565 - mse: 2.7629 - val_loss: 3.2386 - val_mse: 0.1259\n","Epoch 56/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9804 - mse: 2.8878 \n","Epoch 56: val_loss improved from 3.23863 to 3.16756, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9799 - mse: 2.8920 - val_loss: 3.1676 - val_mse: 0.1555\n","Epoch 57/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7854 - mse: 2.7937 \n","Epoch 57: val_loss improved from 3.16756 to 2.98752, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7742 - mse: 2.7861 - val_loss: 2.9875 - val_mse: 0.0679\n","Epoch 58/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3594 - mse: 2.4600 \n","Epoch 58: val_loss improved from 2.98752 to 2.87286, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3779 - mse: 2.4831 - val_loss: 2.8729 - val_mse: 0.0515\n","Epoch 59/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7142 - mse: 2.9111  \n","Epoch 59: val_loss improved from 2.87286 to 2.78871, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6373 - mse: 2.8396 - val_loss: 2.7887 - val_mse: 0.0643\n","Epoch 60/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8779 - mse: 2.1739 \n","Epoch 60: val_loss improved from 2.78871 to 2.68424, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9083 - mse: 2.2079 - val_loss: 2.6842 - val_mse: 0.0556\n","Epoch 61/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9816 - mse: 2.3695 \n","Epoch 61: val_loss improved from 2.68424 to 2.58758, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9747 - mse: 2.3677 - val_loss: 2.5876 - val_mse: 0.0483\n","Epoch 62/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2838 - mse: 2.7607 \n","Epoch 62: val_loss did not improve from 2.58758\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2014 - mse: 2.6831 - val_loss: 2.6042 - val_mse: 0.1501\n","Epoch 63/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1189 - mse: 2.6813 \n","Epoch 63: val_loss improved from 2.58758 to 2.45673, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0852 - mse: 2.6512 - val_loss: 2.4567 - val_mse: 0.0844\n","Epoch 64/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7530 - mse: 2.3971 \n","Epoch 64: val_loss improved from 2.45673 to 2.40559, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7627 - mse: 2.4106 - val_loss: 2.4056 - val_mse: 0.1151\n","Epoch 65/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9382 - mse: 2.6628 \n","Epoch 65: val_loss improved from 2.40559 to 2.26613, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8962 - mse: 2.6243 - val_loss: 2.2661 - val_mse: 0.0562\n","Epoch 66/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9502 - mse: 2.7570 \n","Epoch 66: val_loss improved from 2.26613 to 2.17915, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9183 - mse: 2.7282 - val_loss: 2.1791 - val_mse: 0.0511\n","Epoch 67/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4435 - mse: 2.3329 \n","Epoch 67: val_loss improved from 2.17915 to 2.14709, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4369 - mse: 2.3295 - val_loss: 2.1471 - val_mse: 0.1014\n","Epoch 68/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0410 - mse: 2.0138 \n","Epoch 68: val_loss improved from 2.14709 to 2.06590, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0542 - mse: 2.0302 - val_loss: 2.0659 - val_mse: 0.1025\n","Epoch 69/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1927 - mse: 2.2440 \n","Epoch 69: val_loss improved from 2.06590 to 1.96012, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1777 - mse: 2.2317 - val_loss: 1.9601 - val_mse: 0.0686\n","Epoch 70/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2167 - mse: 2.3401 \n","Epoch 70: val_loss improved from 1.96012 to 1.88208, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2481 - mse: 2.3748 - val_loss: 1.8821 - val_mse: 0.0595\n","Epoch 71/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1926 - mse: 2.3826 \n","Epoch 71: val_loss improved from 1.88208 to 1.80350, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1483 - mse: 2.3421 - val_loss: 1.8035 - val_mse: 0.0476\n","Epoch 72/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8139 - mse: 2.0717 \n","Epoch 72: val_loss improved from 1.80350 to 1.72745, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8149 - mse: 2.0768 - val_loss: 1.7275 - val_mse: 0.0424\n","Epoch 73/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7616 - mse: 2.0900 \n","Epoch 73: val_loss improved from 1.72745 to 1.69304, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7812 - mse: 2.1126 - val_loss: 1.6930 - val_mse: 0.0715\n","Epoch 74/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2663 - mse: 2.6559 \n","Epoch 74: val_loss improved from 1.69304 to 1.64333, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2055 - mse: 2.5978 - val_loss: 1.6433 - val_mse: 0.0801\n","Epoch 75/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7703 - mse: 2.2198 \n","Epoch 75: val_loss improved from 1.64333 to 1.56234, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7615 - mse: 2.2133 - val_loss: 1.5623 - val_mse: 0.0598\n","Epoch 76/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4802 - mse: 1.9895 \n","Epoch 76: val_loss improved from 1.56234 to 1.50892, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5003 - mse: 2.0122 - val_loss: 1.5089 - val_mse: 0.0608\n","Epoch 77/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8184 - mse: 2.3813 \n","Epoch 77: val_loss improved from 1.50892 to 1.45783, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7773 - mse: 2.3423 - val_loss: 1.4578 - val_mse: 0.0649\n","Epoch 78/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5686 - mse: 2.1880 \n","Epoch 78: val_loss improved from 1.45783 to 1.41796, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5811 - mse: 2.2026 - val_loss: 1.4180 - val_mse: 0.0800\n","Epoch 79/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6499 - mse: 2.3220 \n","Epoch 79: val_loss improved from 1.41796 to 1.33243, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6175 - mse: 2.2920 - val_loss: 1.3324 - val_mse: 0.0455\n","Epoch 80/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2551 - mse: 1.9783 \n","Epoch 80: val_loss improved from 1.33243 to 1.30834, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2747 - mse: 2.0006 - val_loss: 1.3083 - val_mse: 0.0700\n","Epoch 81/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9865 - mse: 1.7580 \n","Epoch 81: val_loss improved from 1.30834 to 1.26246, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0287 - mse: 1.8027 - val_loss: 1.2625 - val_mse: 0.0683\n","Epoch 82/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3898 - mse: 2.2038 \n","Epoch 82: val_loss improved from 1.26246 to 1.20215, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3876 - mse: 2.2031 - val_loss: 1.2022 - val_mse: 0.0467\n","Epoch 83/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9985 - mse: 1.8515 \n","Epoch 83: val_loss improved from 1.20215 to 1.17702, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0148 - mse: 1.8691 - val_loss: 1.1770 - val_mse: 0.0590\n","Epoch 84/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0771 - mse: 1.9676 \n","Epoch 84: val_loss improved from 1.17702 to 1.12693, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0885 - mse: 1.9811 - val_loss: 1.1269 - val_mse: 0.0509\n","Epoch 85/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0951 - mse: 2.0271 \n","Epoch 85: val_loss improved from 1.12693 to 1.11967, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0533 - mse: 1.9873 - val_loss: 1.1197 - val_mse: 0.0878\n","Epoch 86/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8835 - mse: 1.8604 \n","Epoch 86: val_loss improved from 1.11967 to 1.06178, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8956 - mse: 1.8742 - val_loss: 1.0618 - val_mse: 0.0696\n","Epoch 87/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7818 - mse: 1.7982 \n","Epoch 87: val_loss improved from 1.06178 to 1.02623, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7811 - mse: 1.7989 - val_loss: 1.0262 - val_mse: 0.0732\n","Epoch 88/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7235 - mse: 1.7788 \n","Epoch 88: val_loss improved from 1.02623 to 0.98107, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7375 - mse: 1.7942 - val_loss: 0.9811 - val_mse: 0.0639\n","Epoch 89/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9162 - mse: 2.0059 \n","Epoch 89: val_loss improved from 0.98107 to 0.94074, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9299 - mse: 2.0213 - val_loss: 0.9407 - val_mse: 0.0545\n","Epoch 90/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5579 - mse: 1.6784 \n","Epoch 90: val_loss improved from 0.94074 to 0.92607, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6011 - mse: 1.7231 - val_loss: 0.9261 - val_mse: 0.0692\n","Epoch 91/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0356 - mse: 2.1841 \n","Epoch 91: val_loss improved from 0.92607 to 0.88053, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9819 - mse: 2.1314 - val_loss: 0.8805 - val_mse: 0.0509\n","Epoch 92/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6257 - mse: 1.8023 \n","Epoch 92: val_loss improved from 0.88053 to 0.85117, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6179 - mse: 1.7959 - val_loss: 0.8512 - val_mse: 0.0496\n","Epoch 93/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7119 - mse: 1.9153 \n","Epoch 93: val_loss did not improve from 0.85117\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7013 - mse: 1.9055 - val_loss: 0.8568 - val_mse: 0.0799\n","Epoch 94/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5555 - mse: 1.7843 \n","Epoch 94: val_loss improved from 0.85117 to 0.81362, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5665 - mse: 1.7960 - val_loss: 0.8136 - val_mse: 0.0640\n","Epoch 95/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4406 - mse: 1.6972 \n","Epoch 95: val_loss improved from 0.81362 to 0.79751, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4482 - mse: 1.7058 - val_loss: 0.7975 - val_mse: 0.0761\n","Epoch 96/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3504 - mse: 1.6356 \n","Epoch 96: val_loss improved from 0.79751 to 0.76112, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3599 - mse: 1.6462 - val_loss: 0.7611 - val_mse: 0.0665\n","Epoch 97/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5941 - mse: 1.9047 \n","Epoch 97: val_loss improved from 0.76112 to 0.72237, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5801 - mse: 1.8915 - val_loss: 0.7224 - val_mse: 0.0503\n","Epoch 98/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5000 - mse: 1.8328 \n","Epoch 98: val_loss improved from 0.72237 to 0.71652, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4989 - mse: 1.8329 - val_loss: 0.7165 - val_mse: 0.0664\n","Epoch 99/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2863 - mse: 1.6416 \n","Epoch 99: val_loss improved from 0.71652 to 0.70477, saving model to best_model_RMSprop_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2967 - mse: 1.6529 - val_loss: 0.7048 - val_mse: 0.0791\n","Epoch 100/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3231 - mse: 1.7026 \n","Epoch 100: val_loss did not improve from 0.70477\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3192 - mse: 1.7001 - val_loss: 0.7221 - val_mse: 0.1200\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: RMSprop, Loss Function: mse, Epochs: 100, Test MSE: 0.09795026393054462\n","Training with optimizer: RMSprop, loss function: mae, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 17.2367 - mse: 145.3325\n","Epoch 1: val_loss improved from inf to 16.59195, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 327ms/step - loss: 17.2297 - mse: 145.3028 - val_loss: 16.5920 - val_mse: 136.0696\n","Epoch 2/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.5716 - mse: 140.4886 \n","Epoch 2: val_loss improved from 16.59195 to 16.10756, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.5472 - mse: 140.2270 - val_loss: 16.1076 - val_mse: 131.4574\n","Epoch 3/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.0080 - mse: 133.5835 \n","Epoch 3: val_loss improved from 16.10756 to 15.62973, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.9981 - mse: 133.5799 - val_loss: 15.6297 - val_mse: 125.6365\n","Epoch 4/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.5680 - mse: 128.6069 \n","Epoch 4: val_loss improved from 15.62973 to 15.18069, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.5515 - mse: 128.4869 - val_loss: 15.1807 - val_mse: 119.9016\n","Epoch 5/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.1862 - mse: 125.1365 \n","Epoch 5: val_loss improved from 15.18069 to 14.74269, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.1674 - mse: 124.8843 - val_loss: 14.7427 - val_mse: 114.1057\n","Epoch 6/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.6475 - mse: 118.0093 \n","Epoch 6: val_loss improved from 14.74269 to 14.34904, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.6353 - mse: 117.9659 - val_loss: 14.3490 - val_mse: 109.7310\n","Epoch 7/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.1124 - mse: 110.9582 \n","Epoch 7: val_loss improved from 14.34904 to 13.83514, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.0976 - mse: 110.8055 - val_loss: 13.8351 - val_mse: 102.2055\n","Epoch 8/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.6809 - mse: 106.0837 \n","Epoch 8: val_loss improved from 13.83514 to 13.35690, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.6571 - mse: 105.7084 - val_loss: 13.3569 - val_mse: 95.0301\n","Epoch 9/100\n","\u001b[1m14/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.1924 - mse: 98.3909  \n","Epoch 9: val_loss improved from 13.35690 to 12.73911, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.0741 - mse: 96.6558 - val_loss: 12.7391 - val_mse: 85.7803\n","Epoch 10/100\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.4604 - mse: 88.2076 \n","Epoch 10: val_loss improved from 12.73911 to 12.03943, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.4355 - mse: 88.1095 - val_loss: 12.0394 - val_mse: 74.4098\n","Epoch 11/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.9431 - mse: 79.3280  \n","Epoch 11: val_loss improved from 12.03943 to 11.15127, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.8910 - mse: 78.6278 - val_loss: 11.1513 - val_mse: 61.0340\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.2752 - mse: 69.9277 \n","Epoch 12: val_loss improved from 11.15127 to 10.42959, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.2149 - mse: 69.2175 - val_loss: 10.4296 - val_mse: 51.3146\n","Epoch 13/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6618 - mse: 63.2102 \n","Epoch 13: val_loss improved from 10.42959 to 9.77650, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.5816 - mse: 61.9843 - val_loss: 9.7765 - val_mse: 43.5162\n","Epoch 14/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6349 - mse: 49.7896   \n","Epoch 14: val_loss improved from 9.77650 to 9.18160, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6209 - mse: 49.5154 - val_loss: 9.1816 - val_mse: 37.6272\n","Epoch 15/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3055 - mse: 44.9361 \n","Epoch 15: val_loss improved from 9.18160 to 8.07072, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.2645 - mse: 44.5608 - val_loss: 8.0707 - val_mse: 26.4596\n","Epoch 16/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3127 - mse: 34.4476 \n","Epoch 16: val_loss improved from 8.07072 to 7.21892, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2805 - mse: 34.1245 - val_loss: 7.2189 - val_mse: 19.4412\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5730 - mse: 27.3718 \n","Epoch 17: val_loss improved from 7.21892 to 6.37049, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5285 - mse: 26.9480 - val_loss: 6.3705 - val_mse: 12.5975\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7860 - mse: 19.6969 \n","Epoch 18: val_loss improved from 6.37049 to 5.30443, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7550 - mse: 19.4833 - val_loss: 5.3044 - val_mse: 8.1499\n","Epoch 19/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3556 - mse: 17.5833 \n","Epoch 19: val_loss improved from 5.30443 to 4.69602, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3289 - mse: 17.3527 - val_loss: 4.6960 - val_mse: 4.2992\n","Epoch 20/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7701 - mse: 12.3823 \n","Epoch 20: val_loss improved from 4.69602 to 4.39909, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7471 - mse: 12.2802 - val_loss: 4.3991 - val_mse: 3.1331\n","Epoch 21/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5420 - mse: 11.4277\n","Epoch 21: val_loss improved from 4.39909 to 4.07911, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5242 - mse: 11.3251 - val_loss: 4.0791 - val_mse: 2.0859\n","Epoch 22/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1644 - mse: 8.9367  \n","Epoch 22: val_loss improved from 4.07911 to 3.56039, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1677 - mse: 8.9686 - val_loss: 3.5604 - val_mse: 0.8614\n","Epoch 23/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1076 - mse: 8.6996 \n","Epoch 23: val_loss improved from 3.56039 to 3.43868, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1028 - mse: 8.6915 - val_loss: 3.4387 - val_mse: 0.7384\n","Epoch 24/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9619 - mse: 8.4800 \n","Epoch 24: val_loss improved from 3.43868 to 3.38275, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9605 - mse: 8.4098 - val_loss: 3.3827 - val_mse: 0.6575\n","Epoch 25/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8799 - mse: 7.7784 \n","Epoch 25: val_loss improved from 3.38275 to 3.24557, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8729 - mse: 7.6969 - val_loss: 3.2456 - val_mse: 0.5184\n","Epoch 26/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8501 - mse: 7.5937  \n","Epoch 26: val_loss did not improve from 3.24557\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.8283 - mse: 7.5245 - val_loss: 3.3839 - val_mse: 0.8243\n","Epoch 27/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6594 - mse: 7.5745 \n","Epoch 27: val_loss improved from 3.24557 to 3.17428, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6399 - mse: 7.4255 - val_loss: 3.1743 - val_mse: 0.5577\n","Epoch 28/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6219 - mse: 7.2894 \n","Epoch 28: val_loss improved from 3.17428 to 3.01033, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6069 - mse: 7.2621 - val_loss: 3.0103 - val_mse: 0.3931\n","Epoch 29/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4686 - mse: 6.5283 \n","Epoch 29: val_loss improved from 3.01033 to 2.96766, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4443 - mse: 6.4484 - val_loss: 2.9677 - val_mse: 0.4036\n","Epoch 30/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3355 - mse: 5.6526 \n","Epoch 30: val_loss improved from 2.96766 to 2.93503, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3314 - mse: 5.6959 - val_loss: 2.9350 - val_mse: 0.4749\n","Epoch 31/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1930 - mse: 6.0895 \n","Epoch 31: val_loss improved from 2.93503 to 2.85953, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1917 - mse: 6.0816 - val_loss: 2.8595 - val_mse: 0.4962\n","Epoch 32/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0863 - mse: 6.2514 \n","Epoch 32: val_loss improved from 2.85953 to 2.66308, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0924 - mse: 6.4180 - val_loss: 2.6631 - val_mse: 0.2641\n","Epoch 33/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8570 - mse: 4.2782 \n","Epoch 33: val_loss improved from 2.66308 to 2.64301, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8612 - mse: 4.3483 - val_loss: 2.6430 - val_mse: 0.2811\n","Epoch 34/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8009 - mse: 4.4463 \n","Epoch 34: val_loss improved from 2.64301 to 2.58498, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8034 - mse: 4.5439 - val_loss: 2.5850 - val_mse: 0.3313\n","Epoch 35/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8251 - mse: 5.4426 \n","Epoch 35: val_loss improved from 2.58498 to 2.49572, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8146 - mse: 5.4104 - val_loss: 2.4957 - val_mse: 0.2968\n","Epoch 36/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6614 - mse: 4.4564 \n","Epoch 36: val_loss improved from 2.49572 to 2.34902, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6648 - mse: 4.6826 - val_loss: 2.3490 - val_mse: 0.1798\n","Epoch 37/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4491 - mse: 4.0323 \n","Epoch 37: val_loss did not improve from 2.34902\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4536 - mse: 4.1258 - val_loss: 2.4063 - val_mse: 0.3060\n","Epoch 38/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4862 - mse: 4.3245 \n","Epoch 38: val_loss improved from 2.34902 to 2.23636, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4722 - mse: 4.2759 - val_loss: 2.2364 - val_mse: 0.2120\n","Epoch 39/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4693 - mse: 5.1532 \n","Epoch 39: val_loss improved from 2.23636 to 2.14841, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4595 - mse: 5.1271 - val_loss: 2.1484 - val_mse: 0.1994\n","Epoch 40/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2751 - mse: 4.6383 \n","Epoch 40: val_loss improved from 2.14841 to 2.02563, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2699 - mse: 4.6341 - val_loss: 2.0256 - val_mse: 0.1862\n","Epoch 41/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1118 - mse: 3.8924 \n","Epoch 41: val_loss did not improve from 2.02563\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1099 - mse: 3.9784 - val_loss: 2.0578 - val_mse: 0.2861\n","Epoch 42/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0728 - mse: 4.2929 \n","Epoch 42: val_loss improved from 2.02563 to 1.83669, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0690 - mse: 4.3819 - val_loss: 1.8367 - val_mse: 0.1194\n","Epoch 43/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9691 - mse: 4.0255 \n","Epoch 43: val_loss improved from 1.83669 to 1.79943, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9683 - mse: 4.0722 - val_loss: 1.7994 - val_mse: 0.1629\n","Epoch 44/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7689 - mse: 3.6794 \n","Epoch 44: val_loss improved from 1.79943 to 1.75092, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7793 - mse: 3.7759 - val_loss: 1.7509 - val_mse: 0.2333\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7706 - mse: 3.5886 \n","Epoch 45: val_loss improved from 1.75092 to 1.68429, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7706 - mse: 3.7463 - val_loss: 1.6843 - val_mse: 0.1907\n","Epoch 46/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6858 - mse: 3.6513 \n","Epoch 46: val_loss improved from 1.68429 to 1.52618, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6734 - mse: 3.6509 - val_loss: 1.5262 - val_mse: 0.1609\n","Epoch 47/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6897 - mse: 4.0880 \n","Epoch 47: val_loss improved from 1.52618 to 1.48618, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6867 - mse: 4.1168 - val_loss: 1.4862 - val_mse: 0.1663\n","Epoch 48/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4486 - mse: 3.5361 \n","Epoch 48: val_loss improved from 1.48618 to 1.41920, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4433 - mse: 3.5137 - val_loss: 1.4192 - val_mse: 0.1725\n","Epoch 49/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5605 - mse: 5.1597 \n","Epoch 49: val_loss improved from 1.41920 to 1.32379, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5524 - mse: 5.1043 - val_loss: 1.3238 - val_mse: 0.1307\n","Epoch 50/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3458 - mse: 3.2588 \n","Epoch 50: val_loss improved from 1.32379 to 1.25764, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3434 - mse: 3.2587 - val_loss: 1.2576 - val_mse: 0.1261\n","Epoch 51/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4144 - mse: 4.3050 \n","Epoch 51: val_loss improved from 1.25764 to 1.21727, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4006 - mse: 4.2190 - val_loss: 1.2173 - val_mse: 0.1234\n","Epoch 52/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2152 - mse: 3.8133 \n","Epoch 52: val_loss did not improve from 1.21727\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2097 - mse: 3.8120 - val_loss: 1.2322 - val_mse: 0.1652\n","Epoch 53/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2283 - mse: 3.7695 \n","Epoch 53: val_loss improved from 1.21727 to 1.14382, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2227 - mse: 3.7734 - val_loss: 1.1438 - val_mse: 0.1420\n","Epoch 54/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1778 - mse: 3.4331 \n","Epoch 54: val_loss improved from 1.14382 to 0.97972, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1813 - mse: 3.5732 - val_loss: 0.9797 - val_mse: 0.0653\n","Epoch 55/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1485 - mse: 4.6300 \n","Epoch 55: val_loss improved from 0.97972 to 0.97411, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1258 - mse: 4.4566 - val_loss: 0.9741 - val_mse: 0.0919\n","Epoch 56/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0056 - mse: 3.4739 \n","Epoch 56: val_loss improved from 0.97411 to 0.93549, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0075 - mse: 3.5224 - val_loss: 0.9355 - val_mse: 0.0885\n","Epoch 57/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9597 - mse: 3.0970 \n","Epoch 57: val_loss did not improve from 0.93549\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9553 - mse: 3.1254 - val_loss: 0.9488 - val_mse: 0.1438\n","Epoch 58/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9319 - mse: 3.8918 \n","Epoch 58: val_loss improved from 0.93549 to 0.83596, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9334 - mse: 3.8666 - val_loss: 0.8360 - val_mse: 0.0720\n","Epoch 59/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9852 - mse: 3.7208 \n","Epoch 59: val_loss improved from 0.83596 to 0.82956, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9759 - mse: 3.7008 - val_loss: 0.8296 - val_mse: 0.0972\n","Epoch 60/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7654 - mse: 3.3065 \n","Epoch 60: val_loss improved from 0.82956 to 0.82075, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7761 - mse: 3.3572 - val_loss: 0.8207 - val_mse: 0.1133\n","Epoch 61/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7911 - mse: 2.8547 \n","Epoch 61: val_loss improved from 0.82075 to 0.74526, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7867 - mse: 2.8360 - val_loss: 0.7453 - val_mse: 0.0903\n","Epoch 62/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8595 - mse: 3.8646 \n","Epoch 62: val_loss did not improve from 0.74526\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8526 - mse: 3.8345 - val_loss: 0.7811 - val_mse: 0.1268\n","Epoch 63/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7854 - mse: 3.5170 \n","Epoch 63: val_loss did not improve from 0.74526\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7766 - mse: 3.4715 - val_loss: 0.8465 - val_mse: 0.2019\n","Epoch 64/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8135 - mse: 3.9883 \n","Epoch 64: val_loss improved from 0.74526 to 0.64734, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8028 - mse: 4.0068 - val_loss: 0.6473 - val_mse: 0.0705\n","Epoch 65/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7163 - mse: 2.7583 \n","Epoch 65: val_loss did not improve from 0.64734\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7098 - mse: 2.8561 - val_loss: 0.6894 - val_mse: 0.1052\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7301 - mse: 3.4411 \n","Epoch 66: val_loss improved from 0.64734 to 0.61056, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7248 - mse: 3.4218 - val_loss: 0.6106 - val_mse: 0.0648\n","Epoch 67/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6019 - mse: 2.8640 \n","Epoch 67: val_loss did not improve from 0.61056\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6101 - mse: 2.9699 - val_loss: 0.6312 - val_mse: 0.0997\n","Epoch 68/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5657 - mse: 2.9967 \n","Epoch 68: val_loss did not improve from 0.61056\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5647 - mse: 2.9667 - val_loss: 0.6202 - val_mse: 0.0933\n","Epoch 69/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6074 - mse: 2.8112 \n","Epoch 69: val_loss improved from 0.61056 to 0.58735, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5982 - mse: 2.7865 - val_loss: 0.5873 - val_mse: 0.0843\n","Epoch 70/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6181 - mse: 3.2574 \n","Epoch 70: val_loss did not improve from 0.58735\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6274 - mse: 3.3547 - val_loss: 0.6269 - val_mse: 0.1190\n","Epoch 71/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5669 - mse: 3.3995 \n","Epoch 71: val_loss improved from 0.58735 to 0.55110, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5645 - mse: 3.3598 - val_loss: 0.5511 - val_mse: 0.0880\n","Epoch 72/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4771 - mse: 2.9030  \n","Epoch 72: val_loss improved from 0.55110 to 0.52817, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4807 - mse: 2.9022 - val_loss: 0.5282 - val_mse: 0.0681\n","Epoch 73/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5208 - mse: 2.6122 \n","Epoch 73: val_loss did not improve from 0.52817\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5186 - mse: 2.6143 - val_loss: 0.5613 - val_mse: 0.0965\n","Epoch 74/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4503 - mse: 2.6238 \n","Epoch 74: val_loss did not improve from 0.52817\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4575 - mse: 2.8095 - val_loss: 0.6011 - val_mse: 0.1484\n","Epoch 75/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5254 - mse: 3.1190 \n","Epoch 75: val_loss did not improve from 0.52817\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5224 - mse: 3.1299 - val_loss: 0.5958 - val_mse: 0.1417\n","Epoch 76/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4646 - mse: 2.7987 \n","Epoch 76: val_loss did not improve from 0.52817\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4601 - mse: 2.7786 - val_loss: 0.5919 - val_mse: 0.1431\n","Epoch 77/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5186 - mse: 3.3473 \n","Epoch 77: val_loss improved from 0.52817 to 0.48712, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5122 - mse: 3.3561 - val_loss: 0.4871 - val_mse: 0.0908\n","Epoch 78/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4515 - mse: 3.3813 \n","Epoch 78: val_loss did not improve from 0.48712\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4606 - mse: 3.3976 - val_loss: 0.5311 - val_mse: 0.1205\n","Epoch 79/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4666 - mse: 3.1342 \n","Epoch 79: val_loss improved from 0.48712 to 0.45921, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4714 - mse: 3.1883 - val_loss: 0.4592 - val_mse: 0.0968\n","Epoch 80/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4593 - mse: 2.8463 \n","Epoch 80: val_loss improved from 0.45921 to 0.44043, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4597 - mse: 2.8934 - val_loss: 0.4404 - val_mse: 0.0870\n","Epoch 81/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3257 - mse: 2.8532 \n","Epoch 81: val_loss did not improve from 0.44043\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3336 - mse: 2.8648 - val_loss: 0.4456 - val_mse: 0.0732\n","Epoch 82/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4361 - mse: 2.9029 \n","Epoch 82: val_loss improved from 0.44043 to 0.40735, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4328 - mse: 2.9502 - val_loss: 0.4073 - val_mse: 0.0521\n","Epoch 83/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3695 - mse: 2.5642 \n","Epoch 83: val_loss did not improve from 0.40735\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3773 - mse: 2.6307 - val_loss: 0.4654 - val_mse: 0.0904\n","Epoch 84/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3641 - mse: 2.3446 \n","Epoch 84: val_loss improved from 0.40735 to 0.39574, saving model to best_model_RMSprop_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3730 - mse: 2.3846 - val_loss: 0.3957 - val_mse: 0.0558\n","Epoch 85/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4008 - mse: 2.7200 \n","Epoch 85: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3833 - mse: 2.7397 - val_loss: 0.5124 - val_mse: 0.1369\n","Epoch 86/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3612 - mse: 2.7948 \n","Epoch 86: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3633 - mse: 2.8204 - val_loss: 0.4822 - val_mse: 0.1371\n","Epoch 87/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3399 - mse: 2.6131 \n","Epoch 87: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3405 - mse: 2.6372 - val_loss: 0.4408 - val_mse: 0.1219\n","Epoch 88/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3629 - mse: 2.7119 \n","Epoch 88: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3682 - mse: 2.7541 - val_loss: 0.4318 - val_mse: 0.0997\n","Epoch 89/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4194 - mse: 2.6607 \n","Epoch 89: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4234 - mse: 2.7035 - val_loss: 0.4448 - val_mse: 0.1039\n","Epoch 90/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2849 - mse: 2.4081 \n","Epoch 90: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3012 - mse: 2.4976 - val_loss: 0.4319 - val_mse: 0.0923\n","Epoch 91/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3356 - mse: 2.3342 \n","Epoch 91: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3307 - mse: 2.3273 - val_loss: 0.4044 - val_mse: 0.0726\n","Epoch 92/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3286 - mse: 2.2970 \n","Epoch 92: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3420 - mse: 2.3936 - val_loss: 0.4161 - val_mse: 0.1605\n","Epoch 93/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3054 - mse: 2.2962 \n","Epoch 93: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3106 - mse: 2.3407 - val_loss: 0.4142 - val_mse: 0.0917\n","Epoch 94/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3596 - mse: 3.0369 \n","Epoch 94: val_loss did not improve from 0.39574\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3647 - mse: 3.0266 - val_loss: 0.4022 - val_mse: 0.0776\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: RMSprop, Loss Function: mae, Epochs: 100, Test MSE: 0.08164798103735256\n","Training with optimizer: RMSprop, loss function: huber, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 16.7367 - mse: 145.3325\n","Epoch 1: val_loss improved from inf to 16.09195, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 332ms/step - loss: 16.7297 - mse: 145.3028 - val_loss: 16.0920 - val_mse: 136.0696\n","Epoch 2/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.0770 - mse: 140.5510 \n","Epoch 2: val_loss improved from 16.09195 to 15.60756, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.0472 - mse: 140.2270 - val_loss: 15.6076 - val_mse: 131.4574\n","Epoch 3/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.5135 - mse: 133.5862 \n","Epoch 3: val_loss improved from 15.60756 to 15.12462, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.4983 - mse: 133.5805 - val_loss: 15.1246 - val_mse: 125.5144\n","Epoch 4/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.0672 - mse: 128.4929 \n","Epoch 4: val_loss improved from 15.12462 to 14.66644, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.0518 - mse: 128.3981 - val_loss: 14.6664 - val_mse: 119.6091\n","Epoch 5/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.6912 - mse: 125.2850 \n","Epoch 5: val_loss improved from 14.66644 to 14.26779, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.6697 - mse: 124.9878 - val_loss: 14.2678 - val_mse: 114.7855\n","Epoch 6/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.1142 - mse: 117.0947 \n","Epoch 6: val_loss improved from 14.26779 to 13.77545, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.1048 - mse: 117.0408 - val_loss: 13.7755 - val_mse: 108.0498\n","Epoch 7/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.6103 - mse: 110.5700 \n","Epoch 7: val_loss improved from 13.77545 to 13.28240, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.5948 - mse: 110.3097 - val_loss: 13.2824 - val_mse: 100.5042\n","Epoch 8/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.1384 - mse: 104.3323 \n","Epoch 8: val_loss improved from 13.28240 to 12.69116, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.1013 - mse: 103.6670 - val_loss: 12.6912 - val_mse: 91.5137\n","Epoch 9/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.7112 - mse: 98.1997  \n","Epoch 9: val_loss improved from 12.69116 to 12.06449, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.6488 - mse: 97.0871 - val_loss: 12.0645 - val_mse: 82.1983\n","Epoch 10/100\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.9391 - mse: 86.8731 \n","Epoch 10: val_loss improved from 12.06449 to 11.54705, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.8829 - mse: 85.8217 - val_loss: 11.5471 - val_mse: 74.8918\n","Epoch 11/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.3700 - mse: 77.9939 \n","Epoch 11: val_loss improved from 11.54705 to 10.57047, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3371 - mse: 77.5719 - val_loss: 10.5705 - val_mse: 59.9841\n","Epoch 12/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6554 - mse: 67.9112 \n","Epoch 12: val_loss improved from 10.57047 to 9.74897, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6273 - mse: 67.5091 - val_loss: 9.7490 - val_mse: 49.7421\n","Epoch 13/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.0147 - mse: 60.5810 \n","Epoch 13: val_loss improved from 9.74897 to 9.18260, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.9847 - mse: 60.1065 - val_loss: 9.1826 - val_mse: 42.9441\n","Epoch 14/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1018 - mse: 48.6340 \n","Epoch 14: val_loss improved from 9.18260 to 8.35826, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.0765 - mse: 48.3240 - val_loss: 8.3583 - val_mse: 33.6436\n","Epoch 15/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7431 - mse: 44.1949 \n","Epoch 15: val_loss improved from 8.35826 to 7.38894, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7044 - mse: 43.7871 - val_loss: 7.3889 - val_mse: 24.9986\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9642 - mse: 37.4165 \n","Epoch 16: val_loss improved from 7.38894 to 6.59551, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.8961 - mse: 36.6349 - val_loss: 6.5955 - val_mse: 17.8817\n","Epoch 17/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1961 - mse: 28.9117 \n","Epoch 17: val_loss improved from 6.59551 to 5.50700, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1134 - mse: 27.9987 - val_loss: 5.5070 - val_mse: 10.8571\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3592 - mse: 22.0145 \n","Epoch 18: val_loss improved from 5.50700 to 4.48627, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3336 - mse: 21.7793 - val_loss: 4.4863 - val_mse: 5.4086\n","Epoch 19/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8184 - mse: 16.9015 \n","Epoch 19: val_loss improved from 4.48627 to 3.98233, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7891 - mse: 16.6206 - val_loss: 3.9823 - val_mse: 3.1425\n","Epoch 20/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3865 - mse: 12.7992 \n","Epoch 20: val_loss improved from 3.98233 to 3.67929, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3706 - mse: 12.7068 - val_loss: 3.6793 - val_mse: 2.1462\n","Epoch 21/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1814 - mse: 11.9577 \n","Epoch 21: val_loss improved from 3.67929 to 3.39698, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1554 - mse: 11.7930 - val_loss: 3.3970 - val_mse: 1.6037\n","Epoch 22/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7538 - mse: 9.2364 \n","Epoch 22: val_loss improved from 3.39698 to 3.35670, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7499 - mse: 9.1951 - val_loss: 3.3567 - val_mse: 1.4419\n","Epoch 23/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6571 - mse: 8.8488 \n","Epoch 23: val_loss improved from 3.35670 to 3.27949, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6538 - mse: 8.7879 - val_loss: 3.2795 - val_mse: 1.1259\n","Epoch 24/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6199 - mse: 8.2898 \n","Epoch 24: val_loss improved from 3.27949 to 3.10009, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6166 - mse: 8.3046 - val_loss: 3.1001 - val_mse: 0.7463\n","Epoch 25/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4818 - mse: 8.0626 \n","Epoch 25: val_loss improved from 3.10009 to 3.07630, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4707 - mse: 7.9284 - val_loss: 3.0763 - val_mse: 1.1773\n","Epoch 26/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2751 - mse: 6.7735 \n","Epoch 26: val_loss improved from 3.07630 to 3.03106, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2844 - mse: 6.8366 - val_loss: 3.0311 - val_mse: 1.3015\n","Epoch 27/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1625 - mse: 6.2393 \n","Epoch 27: val_loss improved from 3.03106 to 2.93447, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1770 - mse: 6.4463 - val_loss: 2.9345 - val_mse: 0.8085\n","Epoch 28/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2051 - mse: 7.1788 \n","Epoch 28: val_loss improved from 2.93447 to 2.81438, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1950 - mse: 7.0946 - val_loss: 2.8144 - val_mse: 0.7594\n","Epoch 29/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0874 - mse: 6.3896 \n","Epoch 29: val_loss improved from 2.81438 to 2.78915, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0676 - mse: 6.3349 - val_loss: 2.7892 - val_mse: 0.8755\n","Epoch 30/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9431 - mse: 5.9875 \n","Epoch 30: val_loss improved from 2.78915 to 2.64443, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9314 - mse: 5.9293 - val_loss: 2.6444 - val_mse: 0.4122\n","Epoch 31/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8403 - mse: 6.3154 \n","Epoch 31: val_loss improved from 2.64443 to 2.62685, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8325 - mse: 6.2197 - val_loss: 2.6268 - val_mse: 0.8909\n","Epoch 32/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7167 - mse: 6.0332 \n","Epoch 32: val_loss improved from 2.62685 to 2.52657, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7053 - mse: 5.9657 - val_loss: 2.5266 - val_mse: 0.6173\n","Epoch 33/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5961 - mse: 5.4749 \n","Epoch 33: val_loss improved from 2.52657 to 2.43161, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5903 - mse: 5.4762 - val_loss: 2.4316 - val_mse: 0.4038\n","Epoch 34/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3981 - mse: 4.5284 \n","Epoch 34: val_loss improved from 2.43161 to 2.33173, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4018 - mse: 4.6090 - val_loss: 2.3317 - val_mse: 0.4267\n","Epoch 35/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4324 - mse: 5.5081 \n","Epoch 35: val_loss improved from 2.33173 to 2.27582, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4253 - mse: 5.4656 - val_loss: 2.2758 - val_mse: 0.4336\n","Epoch 36/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3147 - mse: 4.8328 \n","Epoch 36: val_loss improved from 2.27582 to 2.16527, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3091 - mse: 4.9121 - val_loss: 2.1653 - val_mse: 0.3084\n","Epoch 37/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1620 - mse: 4.8170 \n","Epoch 37: val_loss improved from 2.16527 to 2.06508, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1658 - mse: 4.8829 - val_loss: 2.0651 - val_mse: 0.2565\n","Epoch 38/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0904 - mse: 4.4256 \n","Epoch 38: val_loss improved from 2.06508 to 1.99162, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0837 - mse: 4.4350 - val_loss: 1.9916 - val_mse: 0.2853\n","Epoch 39/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0135 - mse: 4.7198 \n","Epoch 39: val_loss did not improve from 1.99162\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0136 - mse: 4.7665 - val_loss: 2.0248 - val_mse: 1.7267\n","Epoch 40/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7928 - mse: 3.8541 \n","Epoch 40: val_loss improved from 1.99162 to 1.74863, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7939 - mse: 3.8953 - val_loss: 1.7486 - val_mse: 0.0855\n","Epoch 41/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6455 - mse: 3.9261 \n","Epoch 41: val_loss improved from 1.74863 to 1.74568, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6582 - mse: 4.0580 - val_loss: 1.7457 - val_mse: 0.3100\n","Epoch 42/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7267 - mse: 4.5393 \n","Epoch 42: val_loss improved from 1.74568 to 1.61717, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7175 - mse: 4.5723 - val_loss: 1.6172 - val_mse: 0.1529\n","Epoch 43/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5899 - mse: 5.0155 \n","Epoch 43: val_loss improved from 1.61717 to 1.60179, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5729 - mse: 4.8945 - val_loss: 1.6018 - val_mse: 0.5208\n","Epoch 44/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3358 - mse: 3.1280 \n","Epoch 44: val_loss improved from 1.60179 to 1.59750, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3395 - mse: 3.1692 - val_loss: 1.5975 - val_mse: 1.1438\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4032 - mse: 4.1665 \n","Epoch 45: val_loss improved from 1.59750 to 1.45391, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3954 - mse: 4.1756 - val_loss: 1.4539 - val_mse: 0.3685\n","Epoch 46/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2575 - mse: 3.9435 \n","Epoch 46: val_loss improved from 1.45391 to 1.31799, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2455 - mse: 3.8772 - val_loss: 1.3180 - val_mse: 0.2378\n","Epoch 47/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2671 - mse: 4.4038 \n","Epoch 47: val_loss improved from 1.31799 to 1.23404, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2612 - mse: 4.3720 - val_loss: 1.2340 - val_mse: 0.1727\n","Epoch 48/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1139 - mse: 4.4618 \n","Epoch 48: val_loss improved from 1.23404 to 1.13646, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0983 - mse: 4.3502 - val_loss: 1.1365 - val_mse: 0.1166\n","Epoch 49/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0161 - mse: 3.6411 \n","Epoch 49: val_loss improved from 1.13646 to 1.08647, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0124 - mse: 3.6447 - val_loss: 1.0865 - val_mse: 0.1582\n","Epoch 50/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8776 - mse: 3.4213 \n","Epoch 50: val_loss improved from 1.08647 to 1.03603, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8708 - mse: 3.4088 - val_loss: 1.0360 - val_mse: 0.2221\n","Epoch 51/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7851 - mse: 2.8455 \n","Epoch 51: val_loss improved from 1.03603 to 1.03465, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7881 - mse: 2.9277 - val_loss: 1.0347 - val_mse: 0.5439\n","Epoch 52/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7293 - mse: 3.4052 \n","Epoch 52: val_loss improved from 1.03465 to 0.90360, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7251 - mse: 3.4372 - val_loss: 0.9036 - val_mse: 0.1686\n","Epoch 53/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6553 - mse: 3.4060 \n","Epoch 53: val_loss improved from 0.90360 to 0.83175, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6631 - mse: 3.5762 - val_loss: 0.8318 - val_mse: 0.1326\n","Epoch 54/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6595 - mse: 3.6870 \n","Epoch 54: val_loss improved from 0.83175 to 0.75409, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6528 - mse: 3.7004 - val_loss: 0.7541 - val_mse: 0.0904\n","Epoch 55/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4960 - mse: 2.7657 \n","Epoch 55: val_loss improved from 0.75409 to 0.72028, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4976 - mse: 2.7941 - val_loss: 0.7203 - val_mse: 0.1303\n","Epoch 56/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5167 - mse: 3.7555 \n","Epoch 56: val_loss improved from 0.72028 to 0.68720, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5148 - mse: 3.7359 - val_loss: 0.6872 - val_mse: 0.1543\n","Epoch 57/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4427 - mse: 2.6988 \n","Epoch 57: val_loss improved from 0.68720 to 0.63071, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4377 - mse: 2.7415 - val_loss: 0.6307 - val_mse: 0.1227\n","Epoch 58/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4264 - mse: 3.8555 \n","Epoch 58: val_loss improved from 0.63071 to 0.57075, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4216 - mse: 3.8200 - val_loss: 0.5707 - val_mse: 0.0754\n","Epoch 59/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4611 - mse: 3.7850 \n","Epoch 59: val_loss improved from 0.57075 to 0.53796, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4425 - mse: 3.7005 - val_loss: 0.5380 - val_mse: 0.0714\n","Epoch 60/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2858 - mse: 3.1878 \n","Epoch 60: val_loss improved from 0.53796 to 0.51261, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2854 - mse: 3.1458 - val_loss: 0.5126 - val_mse: 0.0825\n","Epoch 61/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2333 - mse: 3.0871 \n","Epoch 61: val_loss improved from 0.51261 to 0.47905, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2417 - mse: 3.1848 - val_loss: 0.4790 - val_mse: 0.0720\n","Epoch 62/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3046 - mse: 3.5939 \n","Epoch 62: val_loss did not improve from 0.47905\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3028 - mse: 3.6330 - val_loss: 0.4863 - val_mse: 0.1427\n","Epoch 63/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2572 - mse: 3.6890 \n","Epoch 63: val_loss improved from 0.47905 to 0.44407, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2497 - mse: 3.5744 - val_loss: 0.4441 - val_mse: 0.0953\n","Epoch 64/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2615 - mse: 3.3679 \n","Epoch 64: val_loss did not improve from 0.44407\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2479 - mse: 3.2924 - val_loss: 0.4611 - val_mse: 0.2373\n","Epoch 65/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3023 - mse: 3.9867 \n","Epoch 65: val_loss improved from 0.44407 to 0.40755, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2909 - mse: 4.0360 - val_loss: 0.4075 - val_mse: 0.1272\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1899 - mse: 3.0346 \n","Epoch 66: val_loss improved from 0.40755 to 0.39461, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1842 - mse: 3.0255 - val_loss: 0.3946 - val_mse: 0.1459\n","Epoch 67/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1198 - mse: 2.9688 \n","Epoch 67: val_loss did not improve from 0.39461\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1278 - mse: 3.1393 - val_loss: 0.4021 - val_mse: 0.2297\n","Epoch 68/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9936 - mse: 2.3043 \n","Epoch 68: val_loss improved from 0.39461 to 0.37074, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9980 - mse: 2.3403 - val_loss: 0.3707 - val_mse: 0.1834\n","Epoch 69/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0370 - mse: 2.6051 \n","Epoch 69: val_loss improved from 0.37074 to 0.33858, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0353 - mse: 2.6170 - val_loss: 0.3386 - val_mse: 0.1263\n","Epoch 70/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0626 - mse: 2.6206 \n","Epoch 70: val_loss did not improve from 0.33858\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0675 - mse: 2.6699 - val_loss: 0.3573 - val_mse: 0.2348\n","Epoch 71/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0544 - mse: 3.0783 \n","Epoch 71: val_loss improved from 0.33858 to 0.31571, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0579 - mse: 3.1115 - val_loss: 0.3157 - val_mse: 0.1379\n","Epoch 72/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9907 - mse: 3.0629 \n","Epoch 72: val_loss did not improve from 0.31571\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9891 - mse: 2.9999 - val_loss: 0.3213 - val_mse: 0.2588\n","Epoch 73/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0540 - mse: 3.5569 \n","Epoch 73: val_loss improved from 0.31571 to 0.26689, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0439 - mse: 3.4327 - val_loss: 0.2669 - val_mse: 0.0913\n","Epoch 74/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0405 - mse: 3.4610 \n","Epoch 74: val_loss improved from 0.26689 to 0.24451, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0246 - mse: 3.3189 - val_loss: 0.2445 - val_mse: 0.0784\n","Epoch 75/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0188 - mse: 3.1624 \n","Epoch 75: val_loss did not improve from 0.24451\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0116 - mse: 3.1333 - val_loss: 0.2906 - val_mse: 0.2459\n","Epoch 76/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9529 - mse: 2.8631 \n","Epoch 76: val_loss did not improve from 0.24451\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9577 - mse: 2.9025 - val_loss: 0.2543 - val_mse: 0.1387\n","Epoch 77/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0872 - mse: 3.9183 \n","Epoch 77: val_loss did not improve from 0.24451\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0770 - mse: 3.8419 - val_loss: 0.4221 - val_mse: 1.5882\n","Epoch 78/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0270 - mse: 3.1344 \n","Epoch 78: val_loss did not improve from 0.24451\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0248 - mse: 3.1351 - val_loss: 0.2611 - val_mse: 0.1710\n","Epoch 79/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9686 - mse: 2.9784 \n","Epoch 79: val_loss improved from 0.24451 to 0.22466, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9697 - mse: 2.9872 - val_loss: 0.2247 - val_mse: 0.1001\n","Epoch 80/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0102 - mse: 3.4938 \n","Epoch 80: val_loss did not improve from 0.22466\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0048 - mse: 3.4743 - val_loss: 0.2574 - val_mse: 0.2501\n","Epoch 81/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8482 - mse: 2.5067 \n","Epoch 81: val_loss did not improve from 0.22466\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8511 - mse: 2.4787 - val_loss: 0.2604 - val_mse: 0.2854\n","Epoch 82/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9657 - mse: 2.8281 \n","Epoch 82: val_loss improved from 0.22466 to 0.20669, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9604 - mse: 2.8053 - val_loss: 0.2067 - val_mse: 0.0872\n","Epoch 83/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9200 - mse: 2.7419 \n","Epoch 83: val_loss did not improve from 0.20669\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9251 - mse: 2.7938 - val_loss: 0.2589 - val_mse: 0.3036\n","Epoch 84/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9748 - mse: 3.4763 \n","Epoch 84: val_loss did not improve from 0.20669\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9730 - mse: 3.4072 - val_loss: 0.2083 - val_mse: 0.1063\n","Epoch 85/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9893 - mse: 3.4238 \n","Epoch 85: val_loss improved from 0.20669 to 0.20537, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9652 - mse: 3.2439 - val_loss: 0.2054 - val_mse: 0.1052\n","Epoch 86/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8657 - mse: 2.6078 \n","Epoch 86: val_loss did not improve from 0.20537\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8761 - mse: 2.6992 - val_loss: 0.2063 - val_mse: 0.1145\n","Epoch 87/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8747 - mse: 2.7745 \n","Epoch 87: val_loss improved from 0.20537 to 0.20322, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8711 - mse: 2.7407 - val_loss: 0.2032 - val_mse: 0.1211\n","Epoch 88/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8941 - mse: 2.6166 \n","Epoch 88: val_loss did not improve from 0.20322\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8916 - mse: 2.6372 - val_loss: 0.2272 - val_mse: 0.2625\n","Epoch 89/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9217 - mse: 2.9026 \n","Epoch 89: val_loss did not improve from 0.20322\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9207 - mse: 2.8704 - val_loss: 0.2441 - val_mse: 0.3503\n","Epoch 90/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7551 - mse: 1.7974 \n","Epoch 90: val_loss did not improve from 0.20322\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7641 - mse: 1.8269 - val_loss: 0.2435 - val_mse: 0.3375\n","Epoch 91/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8752 - mse: 2.5524 \n","Epoch 91: val_loss improved from 0.20322 to 0.19423, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8672 - mse: 2.5041 - val_loss: 0.1942 - val_mse: 0.1484\n","Epoch 92/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8461 - mse: 2.6181 \n","Epoch 92: val_loss improved from 0.19423 to 0.16731, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8492 - mse: 2.6648 - val_loss: 0.1673 - val_mse: 0.0833\n","Epoch 93/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8443 - mse: 2.4631 \n","Epoch 93: val_loss did not improve from 0.16731\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8420 - mse: 2.4444 - val_loss: 0.2282 - val_mse: 0.3381\n","Epoch 94/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8501 - mse: 2.5965 \n","Epoch 94: val_loss improved from 0.16731 to 0.15681, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8511 - mse: 2.5820 - val_loss: 0.1568 - val_mse: 0.0920\n","Epoch 95/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9038 - mse: 3.4995 \n","Epoch 95: val_loss did not improve from 0.15681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8857 - mse: 3.2480 - val_loss: 0.1632 - val_mse: 0.1129\n","Epoch 96/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8077 - mse: 2.3465 \n","Epoch 96: val_loss did not improve from 0.15681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8089 - mse: 2.3580 - val_loss: 0.1675 - val_mse: 0.1177\n","Epoch 97/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8620 - mse: 2.8565 \n","Epoch 97: val_loss did not improve from 0.15681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8615 - mse: 2.8789 - val_loss: 0.1720 - val_mse: 0.1409\n","Epoch 98/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8971 - mse: 3.5620 \n","Epoch 98: val_loss did not improve from 0.15681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8941 - mse: 3.5099 - val_loss: 0.1750 - val_mse: 0.1489\n","Epoch 99/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7942 - mse: 2.6844 \n","Epoch 99: val_loss did not improve from 0.15681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7934 - mse: 2.6744 - val_loss: 0.1709 - val_mse: 0.1461\n","Epoch 100/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7948 - mse: 2.5668 \n","Epoch 100: val_loss improved from 0.15681 to 0.14801, saving model to best_model_RMSprop_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7930 - mse: 2.5603 - val_loss: 0.1480 - val_mse: 0.1033\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: RMSprop, Loss Function: huber, Epochs: 100, Test MSE: 0.16322850515719267\n","Training with optimizer: Nadam, loss function: mse, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - loss: 151.9328 - mse: 146.3506\n","Epoch 1: val_loss improved from inf to 144.73209, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 321ms/step - loss: 151.9179 - mse: 146.3361 - val_loss: 144.7321 - val_mse: 139.1732\n","Epoch 2/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.3450 - mse: 141.7882 \n","Epoch 2: val_loss improved from 144.73209 to 139.77116, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 147.0267 - mse: 141.4704 - val_loss: 139.7712 - val_mse: 134.2240\n","Epoch 3/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.5417 - mse: 134.9966 \n","Epoch 3: val_loss improved from 139.77116 to 135.27309, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 140.5960 - mse: 135.0514 - val_loss: 135.2731 - val_mse: 129.7337\n","Epoch 4/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 137.2823 - mse: 131.7431 \n","Epoch 4: val_loss improved from 135.27309 to 128.01175, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 137.0519 - mse: 131.5129 - val_loss: 128.0117 - val_mse: 122.4752\n","Epoch 5/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 134.3633 - mse: 128.8273 \n","Epoch 5: val_loss improved from 128.01175 to 121.07597, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 133.7816 - mse: 128.2456 - val_loss: 121.0760 - val_mse: 115.5393\n","Epoch 6/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.7839 - mse: 119.2467 \n","Epoch 6: val_loss improved from 121.07597 to 114.25851, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 124.6145 - mse: 119.0772 - val_loss: 114.2585 - val_mse: 108.7205\n","Epoch 7/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.3487 - mse: 110.8102 \n","Epoch 7: val_loss improved from 114.25851 to 106.70404, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 116.3340 - mse: 110.7954 - val_loss: 106.7040 - val_mse: 101.1648\n","Epoch 8/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109.9653 - mse: 104.4254 \n","Epoch 8: val_loss improved from 106.70404 to 96.87434, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 109.3334 - mse: 103.7933 - val_loss: 96.8743 - val_mse: 91.3312\n","Epoch 9/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.6215 - mse: 97.0776  \n","Epoch 9: val_loss improved from 96.87434 to 86.63572, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 101.9048 - mse: 96.3606 - val_loss: 86.6357 - val_mse: 81.0893\n","Epoch 10/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.8786 - mse: 84.3312 \n","Epoch 10: val_loss improved from 86.63572 to 77.38144, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 89.3636 - mse: 83.8159 - val_loss: 77.3814 - val_mse: 71.8308\n","Epoch 11/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.3343 - mse: 74.7823 \n","Epoch 11: val_loss improved from 77.38144 to 66.53938, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 79.6368 - mse: 74.0844 - val_loss: 66.5394 - val_mse: 60.9817\n","Epoch 12/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.5326 - mse: 63.9744 \n","Epoch 12: val_loss improved from 66.53938 to 54.66636, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 68.4995 - mse: 62.9411 - val_loss: 54.6664 - val_mse: 49.1055\n","Epoch 13/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.6168 - mse: 57.0554 \n","Epoch 13: val_loss improved from 54.66636 to 45.20464, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 61.2696 - mse: 55.7080 - val_loss: 45.2046 - val_mse: 39.6400\n","Epoch 14/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.0663 - mse: 42.5009 \n","Epoch 14: val_loss improved from 45.20464 to 37.22106, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 47.7416 - mse: 42.1760 - val_loss: 37.2211 - val_mse: 31.6539\n","Epoch 15/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.3385 - mse: 35.7711 \n","Epoch 15: val_loss improved from 37.22106 to 30.05518, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 40.7432 - mse: 35.1757 - val_loss: 30.0552 - val_mse: 24.4869\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33.6045 - mse: 28.0364 \n","Epoch 16: val_loss improved from 30.05518 to 23.42591, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 33.2465 - mse: 27.6783 - val_loss: 23.4259 - val_mse: 17.8580\n","Epoch 17/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.1937 - mse: 23.6262 \n","Epoch 17: val_loss improved from 23.42591 to 17.97111, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 28.6556 - mse: 23.0882 - val_loss: 17.9711 - val_mse: 12.4065\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.9417 - mse: 18.3780 \n","Epoch 18: val_loss improved from 17.97111 to 13.78129, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.6917 - mse: 18.1283 - val_loss: 13.7813 - val_mse: 8.2219\n","Epoch 19/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.4672 - mse: 13.9088 \n","Epoch 19: val_loss improved from 13.78129 to 11.04125, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.3984 - mse: 13.8403 - val_loss: 11.0413 - val_mse: 5.4874\n","Epoch 20/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.1099 - mse: 13.5570 \n","Epoch 20: val_loss improved from 11.04125 to 9.42590, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.7973 - mse: 13.2447 - val_loss: 9.4259 - val_mse: 3.8775\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.0199 - mse: 11.4726 \n","Epoch 21: val_loss improved from 9.42590 to 7.95141, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.5545 - mse: 11.0077 - val_loss: 7.9514 - val_mse: 2.4099\n","Epoch 22/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.0758 - mse: 8.5357 \n","Epoch 22: val_loss improved from 7.95141 to 7.29224, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.9816 - mse: 8.4419 - val_loss: 7.2922 - val_mse: 1.7587\n","Epoch 23/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2423 - mse: 7.7105 \n","Epoch 23: val_loss improved from 7.29224 to 6.80147, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.1854 - mse: 7.6542 - val_loss: 6.8015 - val_mse: 1.2773\n","Epoch 24/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.6375 - mse: 7.1151 \n","Epoch 24: val_loss improved from 6.80147 to 6.36367, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.5819 - mse: 7.0600 - val_loss: 6.3637 - val_mse: 0.8487\n","Epoch 25/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.1090 - mse: 7.5960 \n","Epoch 25: val_loss improved from 6.36367 to 6.28293, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.0914 - mse: 7.5789 - val_loss: 6.2829 - val_mse: 0.7777\n","Epoch 26/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.3277 - mse: 5.8244 \n","Epoch 26: val_loss improved from 6.28293 to 6.27224, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3952 - mse: 5.8924 - val_loss: 6.2722 - val_mse: 0.7763\n","Epoch 27/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.3198 - mse: 6.8255 \n","Epoch 27: val_loss improved from 6.27224 to 6.09478, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.3050 - mse: 6.8113 - val_loss: 6.0948 - val_mse: 0.6082\n","Epoch 28/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.0047 - mse: 6.5199\n","Epoch 28: val_loss improved from 6.09478 to 5.97888, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.0348 - mse: 6.5505 - val_loss: 5.9789 - val_mse: 0.5018\n","Epoch 29/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.0292 - mse: 6.5536 \n","Epoch 29: val_loss did not improve from 5.97888\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.9579 - mse: 6.4830 - val_loss: 5.9838 - val_mse: 0.5158\n","Epoch 30/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7579 - mse: 5.2917 \n","Epoch 30: val_loss improved from 5.97888 to 5.79692, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.7787 - mse: 5.3133 - val_loss: 5.7969 - val_mse: 0.3399\n","Epoch 31/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.5124 - mse: 6.0575 \n","Epoch 31: val_loss improved from 5.79692 to 5.75873, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3683 - mse: 5.9143 - val_loss: 5.7587 - val_mse: 0.3138\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.4477 - mse: 6.0049 \n","Epoch 32: val_loss improved from 5.75873 to 5.70470, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.3533 - mse: 5.9113 - val_loss: 5.7047 - val_mse: 0.2713\n","Epoch 33/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0777 - mse: 4.6468\n","Epoch 33: val_loss improved from 5.70470 to 5.68813, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.1314 - mse: 4.7012 - val_loss: 5.6881 - val_mse: 0.2682\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3789 - mse: 4.9614 \n","Epoch 34: val_loss improved from 5.68813 to 5.66300, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.4240 - mse: 5.0073 - val_loss: 5.6630 - val_mse: 0.2561\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6814 - mse: 5.2768 \n","Epoch 35: val_loss did not improve from 5.66300\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.6899 - mse: 5.2862 - val_loss: 5.6657 - val_mse: 0.2717\n","Epoch 36/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7491 - mse: 5.3576\n","Epoch 36: val_loss improved from 5.66300 to 5.63333, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.7014 - mse: 5.3107 - val_loss: 5.6333 - val_mse: 0.2527\n","Epoch 37/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9688 - mse: 4.5910  \n","Epoch 37: val_loss did not improve from 5.63333\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.9586 - mse: 4.5815 - val_loss: 5.6422 - val_mse: 0.2758\n","Epoch 38/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5565 - mse: 4.1924 \n","Epoch 38: val_loss improved from 5.63333 to 5.60023, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.5127 - mse: 4.1499 - val_loss: 5.6002 - val_mse: 0.2487\n","Epoch 39/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0865 - mse: 4.7377\n","Epoch 39: val_loss improved from 5.60023 to 5.51472, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.1917 - mse: 4.8440 - val_loss: 5.5147 - val_mse: 0.1784\n","Epoch 40/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9475 - mse: 3.6138 \n","Epoch 40: val_loss did not improve from 5.51472\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.0555 - mse: 3.7228 - val_loss: 5.5155 - val_mse: 0.1944\n","Epoch 41/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3767 - mse: 4.0588 \n","Epoch 41: val_loss improved from 5.51472 to 5.44432, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.4256 - mse: 4.1085 - val_loss: 5.4443 - val_mse: 0.1394\n","Epoch 42/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.7681 - mse: 4.4663  \n","Epoch 42: val_loss improved from 5.44432 to 5.42671, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6932 - mse: 4.3923 - val_loss: 5.4267 - val_mse: 0.1380\n","Epoch 43/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8085 - mse: 4.5229 \n","Epoch 43: val_loss improved from 5.42671 to 5.41563, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.7119 - mse: 4.4272 - val_loss: 5.4156 - val_mse: 0.1431\n","Epoch 44/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1903 - mse: 3.9209 \n","Epoch 44: val_loss improved from 5.41563 to 5.38453, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.2327 - mse: 3.9644 - val_loss: 5.3845 - val_mse: 0.1293\n","Epoch 45/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0083 - mse: 4.7563\n","Epoch 45: val_loss did not improve from 5.38453\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.9683 - mse: 4.7173 - val_loss: 5.3848 - val_mse: 0.1467\n","Epoch 46/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3003 - mse: 4.0655  \n","Epoch 46: val_loss improved from 5.38453 to 5.35565, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.2358 - mse: 4.0021 - val_loss: 5.3556 - val_mse: 0.1353\n","Epoch 47/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9824 - mse: 4.7652  \n","Epoch 47: val_loss improved from 5.35565 to 5.32723, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.8674 - mse: 4.6515 - val_loss: 5.3272 - val_mse: 0.1250\n","Epoch 48/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7803 - mse: 3.5814 \n","Epoch 48: val_loss improved from 5.32723 to 5.31627, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7803 - mse: 3.5827 - val_loss: 5.3163 - val_mse: 0.1327\n","Epoch 49/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1629 - mse: 3.9826 \n","Epoch 49: val_loss improved from 5.31627 to 5.28505, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.1818 - mse: 4.0027 - val_loss: 5.2851 - val_mse: 0.1196\n","Epoch 50/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7477 - mse: 3.5855 \n","Epoch 50: val_loss improved from 5.28505 to 5.25892, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7652 - mse: 3.6043 - val_loss: 5.2589 - val_mse: 0.1117\n","Epoch 51/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7833 - mse: 3.6394 \n","Epoch 51: val_loss improved from 5.25892 to 5.24122, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.8274 - mse: 3.6848 - val_loss: 5.2412 - val_mse: 0.1130\n","Epoch 52/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3814 - mse: 3.2569 \n","Epoch 52: val_loss did not improve from 5.24122\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.3992 - mse: 3.2758 - val_loss: 5.2538 - val_mse: 0.1451\n","Epoch 53/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2593 - mse: 3.1546 \n","Epoch 53: val_loss improved from 5.24122 to 5.21256, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.3115 - mse: 3.2077 - val_loss: 5.2126 - val_mse: 0.1237\n","Epoch 54/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0919 - mse: 4.0066  \n","Epoch 54: val_loss improved from 5.21256 to 5.18182, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.0107 - mse: 3.9267 - val_loss: 5.1818 - val_mse: 0.1129\n","Epoch 55/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5236 - mse: 3.4586 \n","Epoch 55: val_loss improved from 5.18182 to 5.16190, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5151 - mse: 3.4511 - val_loss: 5.1619 - val_mse: 0.1132\n","Epoch 56/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6810 - mse: 3.6360 \n","Epoch 56: val_loss improved from 5.16190 to 5.14868, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.6972 - mse: 3.6535 - val_loss: 5.1487 - val_mse: 0.1203\n","Epoch 57/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1511 - mse: 3.1266 \n","Epoch 57: val_loss improved from 5.14868 to 5.13697, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1606 - mse: 3.1374 - val_loss: 5.1370 - val_mse: 0.1294\n","Epoch 58/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0782 - mse: 3.0747 \n","Epoch 58: val_loss improved from 5.13697 to 5.09011, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0967 - mse: 3.0943 - val_loss: 5.0901 - val_mse: 0.1036\n","Epoch 59/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6779 - mse: 3.6955 \n","Epoch 59: val_loss improved from 5.09011 to 5.06720, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5653 - mse: 3.5841 - val_loss: 5.0672 - val_mse: 0.1020\n","Epoch 60/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0198 - mse: 3.0586 \n","Epoch 60: val_loss improved from 5.06720 to 5.04661, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0411 - mse: 3.0814 - val_loss: 5.0466 - val_mse: 0.1033\n","Epoch 61/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9571 - mse: 3.0179 \n","Epoch 61: val_loss improved from 5.04661 to 5.02793, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.9957 - mse: 3.0579 - val_loss: 5.0279 - val_mse: 0.1069\n","Epoch 62/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6801 - mse: 2.7632 \n","Epoch 62: val_loss improved from 5.02793 to 5.00571, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7403 - mse: 2.8249 - val_loss: 5.0057 - val_mse: 0.1071\n","Epoch 63/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2141 - mse: 3.3198 \n","Epoch 63: val_loss improved from 5.00571 to 4.95860, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2060 - mse: 3.3130 - val_loss: 4.9586 - val_mse: 0.0826\n","Epoch 64/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1480 - mse: 3.2762 \n","Epoch 64: val_loss did not improve from 4.95860\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1249 - mse: 3.2546 - val_loss: 4.9691 - val_mse: 0.1160\n","Epoch 65/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8735 - mse: 3.0247 \n","Epoch 65: val_loss improved from 4.95860 to 4.92443, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.8528 - mse: 3.0053 - val_loss: 4.9244 - val_mse: 0.0943\n","Epoch 66/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0762 - mse: 3.2503 \n","Epoch 66: val_loss improved from 4.92443 to 4.90024, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0390 - mse: 3.2147 - val_loss: 4.9002 - val_mse: 0.0935\n","Epoch 67/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0621 - mse: 3.2596 \n","Epoch 67: val_loss improved from 4.90024 to 4.85993, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0366 - mse: 3.2357 - val_loss: 4.8599 - val_mse: 0.0764\n","Epoch 68/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3884 - mse: 2.6092 \n","Epoch 68: val_loss improved from 4.85993 to 4.83968, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3791 - mse: 2.6014 - val_loss: 4.8397 - val_mse: 0.0799\n","Epoch 69/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6619 - mse: 2.9062 \n","Epoch 69: val_loss improved from 4.83968 to 4.83384, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.6568 - mse: 2.9029 - val_loss: 4.8338 - val_mse: 0.0980\n","Epoch 70/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6993 - mse: 2.9677 \n","Epoch 70: val_loss improved from 4.83384 to 4.80700, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7529 - mse: 3.0231 - val_loss: 4.8070 - val_mse: 0.0956\n","Epoch 71/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1417 - mse: 2.4349 \n","Epoch 71: val_loss improved from 4.80700 to 4.77734, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1706 - mse: 2.4653 - val_loss: 4.7773 - val_mse: 0.0908\n","Epoch 72/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1358 - mse: 2.4538 \n","Epoch 72: val_loss improved from 4.77734 to 4.75672, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1822 - mse: 2.5018 - val_loss: 4.7567 - val_mse: 0.0948\n","Epoch 73/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4416 - mse: 2.7845 \n","Epoch 73: val_loss improved from 4.75672 to 4.72514, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4303 - mse: 2.7745 - val_loss: 4.7251 - val_mse: 0.0880\n","Epoch 74/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5026 - mse: 2.8704 \n","Epoch 74: val_loss improved from 4.72514 to 4.70224, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4858 - mse: 2.8549 - val_loss: 4.7022 - val_mse: 0.0904\n","Epoch 75/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4106 - mse: 2.8034 \n","Epoch 75: val_loss did not improve from 4.70224\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.4009 - mse: 2.7954 - val_loss: 4.7041 - val_mse: 0.1178\n","Epoch 76/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2765 - mse: 2.6953 \n","Epoch 76: val_loss improved from 4.70224 to 4.66060, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2784 - mse: 2.6986 - val_loss: 4.6606 - val_mse: 0.1005\n","Epoch 77/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4019 - mse: 2.8468 \n","Epoch 77: val_loss improved from 4.66060 to 4.62465, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3839 - mse: 2.8303 - val_loss: 4.6247 - val_mse: 0.0908\n","Epoch 78/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1829 - mse: 2.6536 \n","Epoch 78: val_loss improved from 4.62465 to 4.59339, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2026 - mse: 2.6753 - val_loss: 4.5934 - val_mse: 0.0859\n","Epoch 79/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1785 - mse: 2.6756 \n","Epoch 79: val_loss improved from 4.59339 to 4.54801, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1750 - mse: 2.6741 - val_loss: 4.5480 - val_mse: 0.0672\n","Epoch 80/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2309 - mse: 2.7550 \n","Epoch 80: val_loss improved from 4.54801 to 4.51895, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2255 - mse: 2.7513 - val_loss: 4.5189 - val_mse: 0.0647\n","Epoch 81/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6846 - mse: 2.2350 \n","Epoch 81: val_loss improved from 4.51895 to 4.49889, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7266 - mse: 2.2790 - val_loss: 4.4989 - val_mse: 0.0716\n","Epoch 82/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1764 - mse: 2.7540 \n","Epoch 82: val_loss improved from 4.49889 to 4.46215, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1201 - mse: 2.6995 - val_loss: 4.4621 - val_mse: 0.0620\n","Epoch 83/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5791 - mse: 2.1840 \n","Epoch 83: val_loss improved from 4.46215 to 4.43713, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6453 - mse: 2.2520 - val_loss: 4.4371 - val_mse: 0.0645\n","Epoch 84/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7396 - mse: 2.3720 \n","Epoch 84: val_loss improved from 4.43713 to 4.40253, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7704 - mse: 2.4046 - val_loss: 4.4025 - val_mse: 0.0575\n","Epoch 85/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9327 - mse: 2.5927 \n","Epoch 85: val_loss improved from 4.40253 to 4.38745, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8701 - mse: 2.5319 - val_loss: 4.3874 - val_mse: 0.0698\n","Epoch 86/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6583 - mse: 2.3458 \n","Epoch 86: val_loss improved from 4.38745 to 4.37093, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6802 - mse: 2.3695 - val_loss: 4.3709 - val_mse: 0.0812\n","Epoch 87/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4990 - mse: 2.2144 \n","Epoch 87: val_loss improved from 4.37093 to 4.34864, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4933 - mse: 2.2106 - val_loss: 4.3486 - val_mse: 0.0872\n","Epoch 88/100\n","\u001b[1m14/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.7926 - mse: 2.5347 \n","Epoch 88: val_loss improved from 4.34864 to 4.30229, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8024 - mse: 2.5480 - val_loss: 4.3023 - val_mse: 0.0691\n","Epoch 89/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7253 - mse: 2.4973 \n","Epoch 89: val_loss improved from 4.30229 to 4.27041, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7243 - mse: 2.4981 - val_loss: 4.2704 - val_mse: 0.0656\n","Epoch 90/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4949 - mse: 2.2953 \n","Epoch 90: val_loss improved from 4.27041 to 4.23439, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5519 - mse: 2.3542 - val_loss: 4.2344 - val_mse: 0.0583\n","Epoch 91/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5994 - mse: 2.4285 \n","Epoch 91: val_loss improved from 4.23439 to 4.21944, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5407 - mse: 2.3716 - val_loss: 4.2194 - val_mse: 0.0718\n","Epoch 92/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4233 - mse: 2.2810 \n","Epoch 92: val_loss improved from 4.21944 to 4.17363, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4385 - mse: 2.2981 - val_loss: 4.1736 - val_mse: 0.0553\n","Epoch 93/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5417 - mse: 2.4291 \n","Epoch 93: val_loss improved from 4.17363 to 4.14623, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5298 - mse: 2.4188 - val_loss: 4.1462 - val_mse: 0.0573\n","Epoch 94/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3184 - mse: 2.2352 \n","Epoch 94: val_loss improved from 4.14623 to 4.12052, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3568 - mse: 2.2751 - val_loss: 4.1205 - val_mse: 0.0609\n","Epoch 95/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2791 - mse: 2.2249 \n","Epoch 95: val_loss improved from 4.12052 to 4.10496, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3082 - mse: 2.2559 - val_loss: 4.1050 - val_mse: 0.0749\n","Epoch 96/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2266 - mse: 2.2023 \n","Epoch 96: val_loss improved from 4.10496 to 4.06184, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2357 - mse: 2.2130 - val_loss: 4.0618 - val_mse: 0.0616\n","Epoch 97/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2995 - mse: 2.3048 \n","Epoch 97: val_loss improved from 4.06184 to 4.02503, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2769 - mse: 2.2842 - val_loss: 4.0250 - val_mse: 0.0550\n","Epoch 98/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4910 - mse: 2.5264 \n","Epoch 98: val_loss improved from 4.02503 to 4.00116, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4712 - mse: 2.5086 - val_loss: 4.0012 - val_mse: 0.0613\n","Epoch 99/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8766 - mse: 1.9422 \n","Epoch 99: val_loss improved from 4.00116 to 3.98344, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9232 - mse: 1.9908 - val_loss: 3.9834 - val_mse: 0.0740\n","Epoch 100/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2597 - mse: 2.3561 \n","Epoch 100: val_loss improved from 3.98344 to 3.95714, saving model to best_model_Nadam_loss_mse_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2413 - mse: 2.3394 - val_loss: 3.9571 - val_mse: 0.0783\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: Nadam, Loss Function: mse, Epochs: 100, Test MSE: 0.10984575434078131\n","Training with optimizer: Nadam, loss function: mae, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - loss: 17.4114 - mse: 146.4489\n","Epoch 1: val_loss improved from inf to 16.91347, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 346ms/step - loss: 17.4069 - mse: 146.4346 - val_loss: 16.9135 - val_mse: 138.6498\n","Epoch 2/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.8847 - mse: 142.8017 \n","Epoch 2: val_loss improved from 16.91347 to 16.40023, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.8390 - mse: 142.3331 - val_loss: 16.4002 - val_mse: 133.7806\n","Epoch 3/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.2955 - mse: 135.3810 \n","Epoch 3: val_loss improved from 16.40023 to 15.93610, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.2800 - mse: 135.5483 - val_loss: 15.9361 - val_mse: 128.4976\n","Epoch 4/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.8859 - mse: 131.3040 \n","Epoch 4: val_loss improved from 15.93610 to 15.46726, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.8659 - mse: 131.2246 - val_loss: 15.4673 - val_mse: 122.2406\n","Epoch 5/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.5053 - mse: 127.7461 \n","Epoch 5: val_loss improved from 15.46726 to 14.95883, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.4764 - mse: 127.3791 - val_loss: 14.9588 - val_mse: 114.8023\n","Epoch 6/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.0135 - mse: 120.7836 \n","Epoch 6: val_loss improved from 14.95883 to 14.49851, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.9771 - mse: 120.2942 - val_loss: 14.4985 - val_mse: 108.3367\n","Epoch 7/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.3244 - mse: 110.8296 \n","Epoch 7: val_loss improved from 14.49851 to 13.94903, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.2920 - mse: 110.3138 - val_loss: 13.9490 - val_mse: 100.0426\n","Epoch 8/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.7165 - mse: 101.7990 \n","Epoch 8: val_loss improved from 13.94903 to 13.17154, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.6694 - mse: 100.9711 - val_loss: 13.1715 - val_mse: 87.3805\n","Epoch 9/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.8992 - mse: 89.3341 \n","Epoch 9: val_loss improved from 13.17154 to 12.50260, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.8678 - mse: 88.8859 - val_loss: 12.5026 - val_mse: 77.8039\n","Epoch 10/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.2839 - mse: 81.2787 \n","Epoch 10: val_loss improved from 12.50260 to 11.60003, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.1990 - mse: 80.0317 - val_loss: 11.6000 - val_mse: 64.1596\n","Epoch 11/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.3676 - mse: 67.6957 \n","Epoch 11: val_loss improved from 11.60003 to 10.56576, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.2966 - mse: 66.9763 - val_loss: 10.5658 - val_mse: 51.9010\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.4398 - mse: 57.3965 \n","Epoch 12: val_loss improved from 10.56576 to 9.16629, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.3581 - mse: 56.7040 - val_loss: 9.1663 - val_mse: 36.8520\n","Epoch 13/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3462 - mse: 46.5449 \n","Epoch 13: val_loss improved from 9.16629 to 7.67342, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2801 - mse: 45.9880 - val_loss: 7.6734 - val_mse: 21.9092\n","Epoch 14/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2152 - mse: 33.3183 \n","Epoch 14: val_loss improved from 7.67342 to 6.56723, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1689 - mse: 32.7278 - val_loss: 6.5672 - val_mse: 11.7983\n","Epoch 15/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3403 - mse: 22.8844 \n","Epoch 15: val_loss improved from 6.56723 to 5.54614, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2667 - mse: 22.2558 - val_loss: 5.5461 - val_mse: 6.1809\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4974 - mse: 16.4303 \n","Epoch 16: val_loss improved from 5.54614 to 4.73852, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4708 - mse: 16.2114 - val_loss: 4.7385 - val_mse: 3.2243\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0521 - mse: 12.0855 \n","Epoch 17: val_loss improved from 4.73852 to 4.30466, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0085 - mse: 11.7952 - val_loss: 4.3047 - val_mse: 1.9687\n","Epoch 18/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8537 - mse: 10.9853 \n","Epoch 18: val_loss improved from 4.30466 to 4.28526, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.8313 - mse: 10.9358 - val_loss: 4.2853 - val_mse: 2.0219\n","Epoch 19/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3933 - mse: 8.1172 \n","Epoch 19: val_loss improved from 4.28526 to 4.01386, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.3944 - mse: 8.0708 - val_loss: 4.0139 - val_mse: 1.1986\n","Epoch 20/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3300 - mse: 7.4914 \n","Epoch 20: val_loss improved from 4.01386 to 3.76363, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3289 - mse: 7.6580 - val_loss: 3.7636 - val_mse: 0.7478\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2343 - mse: 7.6044 \n","Epoch 21: val_loss improved from 3.76363 to 3.68263, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2232 - mse: 7.6663 - val_loss: 3.6826 - val_mse: 0.7072\n","Epoch 22/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0824 - mse: 6.9076 \n","Epoch 22: val_loss did not improve from 3.68263\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.0722 - mse: 6.8632 - val_loss: 3.7408 - val_mse: 1.7168\n","Epoch 23/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9560 - mse: 6.2215 \n","Epoch 23: val_loss improved from 3.68263 to 3.43978, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9535 - mse: 6.2088 - val_loss: 3.4398 - val_mse: 0.3507\n","Epoch 24/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8666 - mse: 6.3714 \n","Epoch 24: val_loss improved from 3.43978 to 3.40139, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8672 - mse: 6.3653 - val_loss: 3.4014 - val_mse: 0.4851\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7839 - mse: 6.4247 \n","Epoch 25: val_loss improved from 3.40139 to 3.37376, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7821 - mse: 6.3834 - val_loss: 3.3738 - val_mse: 0.7168\n","Epoch 26/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6602 - mse: 7.1240 \n","Epoch 26: val_loss improved from 3.37376 to 3.31147, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6640 - mse: 7.0059 - val_loss: 3.3115 - val_mse: 0.3446\n","Epoch 27/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5541 - mse: 5.0914 \n","Epoch 27: val_loss improved from 3.31147 to 3.18569, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5589 - mse: 5.1917 - val_loss: 3.1857 - val_mse: 0.3020\n","Epoch 28/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5033 - mse: 6.0396 \n","Epoch 28: val_loss improved from 3.18569 to 3.01371, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4900 - mse: 5.9466 - val_loss: 3.0137 - val_mse: 0.1579\n","Epoch 29/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3329 - mse: 4.6825 \n","Epoch 29: val_loss improved from 3.01371 to 3.00709, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3268 - mse: 4.7004 - val_loss: 3.0071 - val_mse: 0.2837\n","Epoch 30/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1914 - mse: 4.7274 \n","Epoch 30: val_loss improved from 3.00709 to 2.95292, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.1990 - mse: 4.8321 - val_loss: 2.9529 - val_mse: 0.3178\n","Epoch 31/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2564 - mse: 6.3158 \n","Epoch 31: val_loss improved from 2.95292 to 2.88926, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2481 - mse: 6.1118 - val_loss: 2.8893 - val_mse: 0.2519\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0661 - mse: 5.0055 \n","Epoch 32: val_loss improved from 2.88926 to 2.74553, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0555 - mse: 4.9719 - val_loss: 2.7455 - val_mse: 0.1236\n","Epoch 33/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9177 - mse: 4.0827 \n","Epoch 33: val_loss improved from 2.74553 to 2.68572, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9170 - mse: 4.1063 - val_loss: 2.6857 - val_mse: 0.1321\n","Epoch 34/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0355 - mse: 5.5698 \n","Epoch 34: val_loss improved from 2.68572 to 2.62975, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0252 - mse: 5.3860 - val_loss: 2.6298 - val_mse: 0.1423\n","Epoch 35/100\n","\u001b[1m13/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9282 - mse: 4.3582 \n","Epoch 35: val_loss improved from 2.62975 to 2.59940, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.9098 - mse: 4.6574 - val_loss: 2.5994 - val_mse: 0.3947\n","Epoch 36/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7008 - mse: 3.8135 \n","Epoch 36: val_loss improved from 2.59940 to 2.50568, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.7128 - mse: 3.9179 - val_loss: 2.5057 - val_mse: 0.1467\n","Epoch 37/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7493 - mse: 4.7975 \n","Epoch 37: val_loss did not improve from 2.50568\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7485 - mse: 4.8207 - val_loss: 2.5062 - val_mse: 0.2003\n","Epoch 38/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4732 - mse: 3.2619 \n","Epoch 38: val_loss improved from 2.50568 to 2.40085, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4960 - mse: 3.4351 - val_loss: 2.4009 - val_mse: 0.1318\n","Epoch 39/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5626 - mse: 4.2509 \n","Epoch 39: val_loss improved from 2.40085 to 2.37977, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5656 - mse: 4.3291 - val_loss: 2.3798 - val_mse: 0.2520\n","Epoch 40/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4237 - mse: 3.5231 \n","Epoch 40: val_loss improved from 2.37977 to 2.35146, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4377 - mse: 3.6731 - val_loss: 2.3515 - val_mse: 0.4021\n","Epoch 41/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4467 - mse: 4.8886 \n","Epoch 41: val_loss improved from 2.35146 to 2.22974, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4525 - mse: 4.9435 - val_loss: 2.2297 - val_mse: 0.1449\n","Epoch 42/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3763 - mse: 4.5449 \n","Epoch 42: val_loss improved from 2.22974 to 2.19361, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3657 - mse: 4.5026 - val_loss: 2.1936 - val_mse: 0.1377\n","Epoch 43/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2084 - mse: 4.0138 \n","Epoch 43: val_loss improved from 2.19361 to 2.14784, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2138 - mse: 4.1734 - val_loss: 2.1478 - val_mse: 0.1566\n","Epoch 44/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0949 - mse: 2.9154 \n","Epoch 44: val_loss improved from 2.14784 to 2.07929, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1084 - mse: 3.0518 - val_loss: 2.0793 - val_mse: 0.1374\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2289 - mse: 4.2395 \n","Epoch 45: val_loss improved from 2.07929 to 2.02147, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2213 - mse: 4.3285 - val_loss: 2.0215 - val_mse: 0.1287\n","Epoch 46/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0836 - mse: 3.5219 \n","Epoch 46: val_loss did not improve from 2.02147\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0715 - mse: 3.5205 - val_loss: 2.0380 - val_mse: 0.3974\n","Epoch 47/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0301 - mse: 3.5533 \n","Epoch 47: val_loss improved from 2.02147 to 1.95021, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0406 - mse: 3.6457 - val_loss: 1.9502 - val_mse: 0.2045\n","Epoch 48/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9197 - mse: 3.3925 \n","Epoch 48: val_loss improved from 1.95021 to 1.87573, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9112 - mse: 3.3757 - val_loss: 1.8757 - val_mse: 0.1336\n","Epoch 49/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9816 - mse: 4.0570 \n","Epoch 49: val_loss improved from 1.87573 to 1.81327, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9809 - mse: 4.1307 - val_loss: 1.8133 - val_mse: 0.1184\n","Epoch 50/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8127 - mse: 3.6117 \n","Epoch 50: val_loss improved from 1.81327 to 1.72823, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8054 - mse: 3.6422 - val_loss: 1.7282 - val_mse: 0.1001\n","Epoch 51/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7530 - mse: 3.4483 \n","Epoch 51: val_loss improved from 1.72823 to 1.66396, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7580 - mse: 3.5444 - val_loss: 1.6640 - val_mse: 0.0827\n","Epoch 52/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7451 - mse: 3.3828 \n","Epoch 52: val_loss did not improve from 1.66396\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7267 - mse: 3.4310 - val_loss: 1.6839 - val_mse: 0.1443\n","Epoch 53/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6014 - mse: 2.9526 \n","Epoch 53: val_loss improved from 1.66396 to 1.66106, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6101 - mse: 3.0333 - val_loss: 1.6611 - val_mse: 0.1421\n","Epoch 54/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7055 - mse: 3.7674 \n","Epoch 54: val_loss improved from 1.66106 to 1.55284, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6953 - mse: 3.7211 - val_loss: 1.5528 - val_mse: 0.1141\n","Epoch 55/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6097 - mse: 4.0880 \n","Epoch 55: val_loss improved from 1.55284 to 1.51440, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5949 - mse: 3.9337 - val_loss: 1.5144 - val_mse: 0.1472\n","Epoch 56/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4807 - mse: 3.3351 \n","Epoch 56: val_loss improved from 1.51440 to 1.50563, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4956 - mse: 3.4672 - val_loss: 1.5056 - val_mse: 0.1991\n","Epoch 57/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4703 - mse: 2.9831 \n","Epoch 57: val_loss improved from 1.50563 to 1.44191, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4616 - mse: 3.0220 - val_loss: 1.4419 - val_mse: 0.1186\n","Epoch 58/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4816 - mse: 3.9008 \n","Epoch 58: val_loss improved from 1.44191 to 1.38714, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4774 - mse: 3.8879 - val_loss: 1.3871 - val_mse: 0.1324\n","Epoch 59/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4726 - mse: 3.6078 \n","Epoch 59: val_loss improved from 1.38714 to 1.34532, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4553 - mse: 3.6436 - val_loss: 1.3453 - val_mse: 0.1267\n","Epoch 60/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3426 - mse: 3.3590 \n","Epoch 60: val_loss improved from 1.34532 to 1.32369, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3314 - mse: 3.2789 - val_loss: 1.3237 - val_mse: 0.1134\n","Epoch 61/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2762 - mse: 2.9792 \n","Epoch 61: val_loss improved from 1.32369 to 1.26122, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2890 - mse: 3.1568 - val_loss: 1.2612 - val_mse: 0.0956\n","Epoch 62/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2947 - mse: 3.1576 \n","Epoch 62: val_loss improved from 1.26122 to 1.22635, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2968 - mse: 3.3130 - val_loss: 1.2263 - val_mse: 0.0939\n","Epoch 63/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2975 - mse: 4.5729 \n","Epoch 63: val_loss improved from 1.22635 to 1.20125, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2821 - mse: 4.3608 - val_loss: 1.2012 - val_mse: 0.0998\n","Epoch 64/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2893 - mse: 3.7318 \n","Epoch 64: val_loss improved from 1.20125 to 1.17696, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2753 - mse: 3.6946 - val_loss: 1.1770 - val_mse: 0.1052\n","Epoch 65/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3408 - mse: 4.5484 \n","Epoch 65: val_loss improved from 1.17696 to 1.11730, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3132 - mse: 4.4855 - val_loss: 1.1173 - val_mse: 0.0882\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2034 - mse: 3.5704 \n","Epoch 66: val_loss improved from 1.11730 to 1.08061, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1984 - mse: 3.5998 - val_loss: 1.0806 - val_mse: 0.0883\n","Epoch 67/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0717 - mse: 3.0989 \n","Epoch 67: val_loss improved from 1.08061 to 1.05749, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0719 - mse: 3.1717 - val_loss: 1.0575 - val_mse: 0.0934\n","Epoch 68/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9553 - mse: 2.6481 \n","Epoch 68: val_loss improved from 1.05749 to 1.03231, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9490 - mse: 2.6423 - val_loss: 1.0323 - val_mse: 0.0998\n","Epoch 69/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0789 - mse: 3.2385 \n","Epoch 69: val_loss improved from 1.03231 to 1.02143, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0573 - mse: 3.2447 - val_loss: 1.0214 - val_mse: 0.1141\n","Epoch 70/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0020 - mse: 3.0639 \n","Epoch 70: val_loss improved from 1.02143 to 1.00200, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0159 - mse: 3.1922 - val_loss: 1.0020 - val_mse: 0.1108\n","Epoch 71/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9378 - mse: 2.8617 \n","Epoch 71: val_loss improved from 1.00200 to 0.91471, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9384 - mse: 2.9172 - val_loss: 0.9147 - val_mse: 0.0724\n","Epoch 72/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8815 - mse: 2.9491 \n","Epoch 72: val_loss improved from 0.91471 to 0.89749, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8819 - mse: 2.9565 - val_loss: 0.8975 - val_mse: 0.0773\n","Epoch 73/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8991 - mse: 3.0828 \n","Epoch 73: val_loss improved from 0.89749 to 0.86890, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8956 - mse: 3.0880 - val_loss: 0.8689 - val_mse: 0.0762\n","Epoch 74/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9295 - mse: 4.0807 \n","Epoch 74: val_loss did not improve from 0.86890\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9120 - mse: 3.8754 - val_loss: 0.9095 - val_mse: 0.1140\n","Epoch 75/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9186 - mse: 3.9689 \n","Epoch 75: val_loss did not improve from 0.86890\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9018 - mse: 3.8008 - val_loss: 0.8840 - val_mse: 0.1141\n","Epoch 76/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8328 - mse: 3.3764 \n","Epoch 76: val_loss improved from 0.86890 to 0.82957, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8253 - mse: 3.3016 - val_loss: 0.8296 - val_mse: 0.1087\n","Epoch 77/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9190 - mse: 4.0328 \n","Epoch 77: val_loss improved from 0.82957 to 0.81830, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8946 - mse: 3.8223 - val_loss: 0.8183 - val_mse: 0.1018\n","Epoch 78/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7638 - mse: 2.8926 \n","Epoch 78: val_loss improved from 0.81830 to 0.81232, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7715 - mse: 2.8872 - val_loss: 0.8123 - val_mse: 0.1207\n","Epoch 79/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7355 - mse: 3.0856 \n","Epoch 79: val_loss improved from 0.81232 to 0.75126, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7498 - mse: 3.2391 - val_loss: 0.7513 - val_mse: 0.0908\n","Epoch 80/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8360 - mse: 4.1587 \n","Epoch 80: val_loss did not improve from 0.75126\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8205 - mse: 4.0161 - val_loss: 0.7538 - val_mse: 0.1132\n","Epoch 81/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5619 - mse: 2.1225 \n","Epoch 81: val_loss improved from 0.75126 to 0.72657, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5755 - mse: 2.2079 - val_loss: 0.7266 - val_mse: 0.0984\n","Epoch 82/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6985 - mse: 2.7832 \n","Epoch 82: val_loss improved from 0.72657 to 0.69163, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6869 - mse: 2.7706 - val_loss: 0.6916 - val_mse: 0.0796\n","Epoch 83/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6257 - mse: 2.8254 \n","Epoch 83: val_loss did not improve from 0.69163\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6393 - mse: 2.8943 - val_loss: 0.7169 - val_mse: 0.1208\n","Epoch 84/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7513 - mse: 4.1401 \n","Epoch 84: val_loss improved from 0.69163 to 0.66031, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7457 - mse: 3.9894 - val_loss: 0.6603 - val_mse: 0.0894\n","Epoch 85/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7170 - mse: 4.1391 \n","Epoch 85: val_loss did not improve from 0.66031\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6869 - mse: 3.8268 - val_loss: 0.7005 - val_mse: 0.1411\n","Epoch 86/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6007 - mse: 2.9350 \n","Epoch 86: val_loss did not improve from 0.66031\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6171 - mse: 3.1424 - val_loss: 0.6651 - val_mse: 0.1195\n","Epoch 87/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5659 - mse: 2.7271 \n","Epoch 87: val_loss did not improve from 0.66031\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5550 - mse: 2.7249 - val_loss: 0.6703 - val_mse: 0.1977\n","Epoch 88/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6189 - mse: 3.3385 \n","Epoch 88: val_loss improved from 0.66031 to 0.61687, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6057 - mse: 3.2618 - val_loss: 0.6169 - val_mse: 0.1490\n","Epoch 89/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5908 - mse: 2.7611 \n","Epoch 89: val_loss improved from 0.61687 to 0.59235, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5873 - mse: 2.7450 - val_loss: 0.5923 - val_mse: 0.1201\n","Epoch 90/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4340 - mse: 2.1893 \n","Epoch 90: val_loss improved from 0.59235 to 0.57792, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4519 - mse: 2.2256 - val_loss: 0.5779 - val_mse: 0.1068\n","Epoch 91/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5632 - mse: 2.7265 \n","Epoch 91: val_loss did not improve from 0.57792\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5400 - mse: 2.6468 - val_loss: 0.5882 - val_mse: 0.1082\n","Epoch 92/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4970 - mse: 2.7353 \n","Epoch 92: val_loss improved from 0.57792 to 0.52474, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5026 - mse: 2.8077 - val_loss: 0.5247 - val_mse: 0.0711\n","Epoch 93/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4415 - mse: 2.4371 \n","Epoch 93: val_loss did not improve from 0.52474\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4471 - mse: 2.4708 - val_loss: 0.5298 - val_mse: 0.0831\n","Epoch 94/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4681 - mse: 2.4462 \n","Epoch 94: val_loss did not improve from 0.52474\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4756 - mse: 2.4609 - val_loss: 0.5392 - val_mse: 0.1574\n","Epoch 95/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5504 - mse: 3.8283 \n","Epoch 95: val_loss did not improve from 0.52474\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5358 - mse: 3.5896 - val_loss: 0.6163 - val_mse: 0.3505\n","Epoch 96/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4748 - mse: 3.2065 \n","Epoch 96: val_loss did not improve from 0.52474\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4746 - mse: 3.2178 - val_loss: 0.5326 - val_mse: 0.2096\n","Epoch 97/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4417 - mse: 2.7039 \n","Epoch 97: val_loss improved from 0.52474 to 0.50147, saving model to best_model_Nadam_loss_mae_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4504 - mse: 2.9042 - val_loss: 0.5015 - val_mse: 0.1850\n","Epoch 98/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4741 - mse: 3.1305 \n","Epoch 98: val_loss did not improve from 0.50147\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4819 - mse: 3.2139 - val_loss: 0.5462 - val_mse: 0.1931\n","Epoch 99/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3584 - mse: 2.2853 \n","Epoch 99: val_loss did not improve from 0.50147\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3660 - mse: 2.4240 - val_loss: 0.5188 - val_mse: 0.1153\n","Epoch 100/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4025 - mse: 2.9070 \n","Epoch 100: val_loss did not improve from 0.50147\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4012 - mse: 2.9033 - val_loss: 0.5055 - val_mse: 0.1126\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: Nadam, Loss Function: mae, Epochs: 100, Test MSE: 0.24551022721097354\n","Training with optimizer: Nadam, loss function: huber, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - loss: 16.9114 - mse: 146.4489\n","Epoch 1: val_loss improved from inf to 16.41347, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 345ms/step - loss: 16.9069 - mse: 146.4346 - val_loss: 16.4135 - val_mse: 138.6498\n","Epoch 2/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.3741 - mse: 142.6924 \n","Epoch 2: val_loss improved from 16.41347 to 15.90023, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.3390 - mse: 142.3331 - val_loss: 15.9002 - val_mse: 133.7806\n","Epoch 3/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.7926 - mse: 135.3875 \n","Epoch 3: val_loss improved from 15.90023 to 15.43610, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.7800 - mse: 135.5483 - val_loss: 15.4361 - val_mse: 128.4976\n","Epoch 4/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.3836 - mse: 131.3048 \n","Epoch 4: val_loss improved from 15.43610 to 14.96726, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.3659 - mse: 131.2246 - val_loss: 14.9673 - val_mse: 122.2406\n","Epoch 5/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.0053 - mse: 127.7461 \n","Epoch 5: val_loss improved from 14.96726 to 14.45883, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.9764 - mse: 127.3791 - val_loss: 14.4588 - val_mse: 114.8023\n","Epoch 6/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.5088 - mse: 120.7245 \n","Epoch 6: val_loss improved from 14.45883 to 13.99851, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.4771 - mse: 120.2942 - val_loss: 13.9985 - val_mse: 108.3367\n","Epoch 7/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.8195 - mse: 110.7464 \n","Epoch 7: val_loss improved from 13.99851 to 13.44719, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.7920 - mse: 110.3138 - val_loss: 13.4472 - val_mse: 99.9967\n","Epoch 8/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2103 - mse: 101.6400 \n","Epoch 8: val_loss improved from 13.44719 to 12.69207, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.1704 - mse: 100.9453 - val_loss: 12.6921 - val_mse: 87.7749\n","Epoch 9/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.3972 - mse: 89.2864 \n","Epoch 9: val_loss improved from 12.69207 to 12.07260, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.3602 - mse: 88.7198 - val_loss: 12.0726 - val_mse: 79.0486\n","Epoch 10/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.7756 - mse: 81.4609 \n","Epoch 10: val_loss improved from 12.07260 to 11.13077, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.7090 - mse: 80.4202 - val_loss: 11.1308 - val_mse: 65.3235\n","Epoch 11/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.9283 - mse: 68.6144 \n","Epoch 11: val_loss improved from 11.13077 to 10.08628, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.8398 - mse: 67.7023 - val_loss: 10.0863 - val_mse: 53.3741\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9954 - mse: 58.1991  \n","Epoch 12: val_loss improved from 10.08628 to 8.80575, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.8922 - mse: 56.9794 - val_loss: 8.8058 - val_mse: 39.0996\n","Epoch 13/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7946 - mse: 45.3003 \n","Epoch 13: val_loss improved from 8.80575 to 7.33195, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7344 - mse: 44.6970 - val_loss: 7.3319 - val_mse: 23.6875\n","Epoch 14/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6761 - mse: 32.0548 \n","Epoch 14: val_loss improved from 7.33195 to 6.17999, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6322 - mse: 31.7666 - val_loss: 6.1800 - val_mse: 13.2950\n","Epoch 15/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9028 - mse: 23.8976 \n","Epoch 15: val_loss improved from 6.17999 to 5.30198, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8312 - mse: 23.3676 - val_loss: 5.3020 - val_mse: 9.2195\n","Epoch 16/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0973 - mse: 17.4508 \n","Epoch 16: val_loss improved from 5.30198 to 4.47785, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0338 - mse: 17.0599 - val_loss: 4.4779 - val_mse: 4.1864\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6281 - mse: 13.1755 \n","Epoch 17: val_loss improved from 4.47785 to 4.16865, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5770 - mse: 12.7027 - val_loss: 4.1687 - val_mse: 4.3662\n","Epoch 18/100\n","\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2649 - mse: 10.1485 \n","Epoch 18: val_loss improved from 4.16865 to 3.74709, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.2035 - mse: 9.7885 - val_loss: 3.7471 - val_mse: 1.4461\n","Epoch 19/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0029 - mse: 8.9701  \n","Epoch 19: val_loss improved from 3.74709 to 3.56355, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9873 - mse: 8.8290 - val_loss: 3.5636 - val_mse: 0.9714\n","Epoch 20/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8344 - mse: 7.6663 \n","Epoch 20: val_loss improved from 3.56355 to 3.33808, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8288 - mse: 7.7297 - val_loss: 3.3381 - val_mse: 0.5391\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7035 - mse: 7.5604 \n","Epoch 21: val_loss improved from 3.33808 to 3.19876, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6794 - mse: 7.4070 - val_loss: 3.1988 - val_mse: 0.4098\n","Epoch 22/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5743 - mse: 7.2882 \n","Epoch 22: val_loss improved from 3.19876 to 3.13437, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5709 - mse: 7.3254 - val_loss: 3.1344 - val_mse: 0.3755\n","Epoch 23/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3778 - mse: 6.2112 \n","Epoch 23: val_loss improved from 3.13437 to 3.05074, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3690 - mse: 6.2082 - val_loss: 3.0507 - val_mse: 0.3260\n","Epoch 24/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2406 - mse: 5.8027 \n","Epoch 24: val_loss improved from 3.05074 to 2.98700, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2595 - mse: 5.9131 - val_loss: 2.9870 - val_mse: 0.4429\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2820 - mse: 7.3256 \n","Epoch 25: val_loss improved from 2.98700 to 2.97202, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2618 - mse: 7.1187 - val_loss: 2.9720 - val_mse: 0.7710\n","Epoch 26/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0766 - mse: 6.8115  \n","Epoch 26: val_loss improved from 2.97202 to 2.95218, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0651 - mse: 6.6376 - val_loss: 2.9522 - val_mse: 0.8639\n","Epoch 27/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9918 - mse: 5.3374 \n","Epoch 27: val_loss improved from 2.95218 to 2.81533, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9902 - mse: 5.3831 - val_loss: 2.8153 - val_mse: 0.5640\n","Epoch 28/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0547 - mse: 6.1342 \n","Epoch 28: val_loss improved from 2.81533 to 2.68953, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0313 - mse: 6.0472 - val_loss: 2.6895 - val_mse: 0.2833\n","Epoch 29/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6615 - mse: 4.2351 \n","Epoch 29: val_loss improved from 2.68953 to 2.57403, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6676 - mse: 4.2780 - val_loss: 2.5740 - val_mse: 0.1505\n","Epoch 30/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7032 - mse: 5.6948 \n","Epoch 30: val_loss did not improve from 2.57403\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7000 - mse: 5.6804 - val_loss: 2.5850 - val_mse: 0.4111\n","Epoch 31/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6481 - mse: 5.3980 \n","Epoch 31: val_loss improved from 2.57403 to 2.46923, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6433 - mse: 5.3409 - val_loss: 2.4692 - val_mse: 0.2040\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5463 - mse: 5.0780 \n","Epoch 32: val_loss improved from 2.46923 to 2.39194, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5383 - mse: 5.0661 - val_loss: 2.3919 - val_mse: 0.1849\n","Epoch 33/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2420 - mse: 3.8669 \n","Epoch 33: val_loss improved from 2.39194 to 2.34304, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2553 - mse: 3.9517 - val_loss: 2.3430 - val_mse: 0.2184\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3958 - mse: 4.9292 \n","Epoch 34: val_loss improved from 2.34304 to 2.27593, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3828 - mse: 4.8435 - val_loss: 2.2759 - val_mse: 0.2423\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3518 - mse: 5.4516 \n","Epoch 35: val_loss improved from 2.27593 to 2.20100, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3375 - mse: 5.4277 - val_loss: 2.2010 - val_mse: 0.2260\n","Epoch 36/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1626 - mse: 3.8104 \n","Epoch 36: val_loss improved from 2.20100 to 2.16179, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1630 - mse: 3.9203 - val_loss: 2.1618 - val_mse: 0.3572\n","Epoch 37/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1424 - mse: 4.9034 \n","Epoch 37: val_loss improved from 2.16179 to 2.09079, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1343 - mse: 4.8619 - val_loss: 2.0908 - val_mse: 0.2227\n","Epoch 38/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9175 - mse: 3.4446 \n","Epoch 38: val_loss improved from 2.09079 to 2.03955, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9227 - mse: 3.5259 - val_loss: 2.0395 - val_mse: 0.2992\n","Epoch 39/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9628 - mse: 4.1516 \n","Epoch 39: val_loss improved from 2.03955 to 2.00654, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9682 - mse: 4.2169 - val_loss: 2.0065 - val_mse: 0.5739\n","Epoch 40/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7596 - mse: 3.3165 \n","Epoch 40: val_loss improved from 2.00654 to 1.94669, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7716 - mse: 3.4683 - val_loss: 1.9467 - val_mse: 0.5346\n","Epoch 41/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8466 - mse: 4.8732 \n","Epoch 41: val_loss improved from 1.94669 to 1.80330, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8450 - mse: 4.7544 - val_loss: 1.8033 - val_mse: 0.1014\n","Epoch 42/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7357 - mse: 3.5435 \n","Epoch 42: val_loss improved from 1.80330 to 1.76515, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7186 - mse: 3.5340 - val_loss: 1.7651 - val_mse: 0.1367\n","Epoch 43/100\n","\u001b[1m13/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7121 - mse: 4.3998 \n","Epoch 43: val_loss improved from 1.76515 to 1.69757, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6922 - mse: 4.4214 - val_loss: 1.6976 - val_mse: 0.1116\n","Epoch 44/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5547 - mse: 3.3094 \n","Epoch 44: val_loss improved from 1.69757 to 1.65304, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5591 - mse: 3.3643 - val_loss: 1.6530 - val_mse: 0.1291\n","Epoch 45/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5488 - mse: 3.6091 \n","Epoch 45: val_loss improved from 1.65304 to 1.62438, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5404 - mse: 3.7416 - val_loss: 1.6244 - val_mse: 0.1852\n","Epoch 46/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4272 - mse: 3.2254 \n","Epoch 46: val_loss improved from 1.62438 to 1.53305, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4235 - mse: 3.3378 - val_loss: 1.5331 - val_mse: 0.0985\n","Epoch 47/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4811 - mse: 3.8384 \n","Epoch 47: val_loss improved from 1.53305 to 1.48053, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4840 - mse: 3.8561 - val_loss: 1.4805 - val_mse: 0.0952\n","Epoch 48/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3394 - mse: 3.2060 \n","Epoch 48: val_loss improved from 1.48053 to 1.43338, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3296 - mse: 3.2697 - val_loss: 1.4334 - val_mse: 0.0992\n","Epoch 49/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3030 - mse: 3.4824 \n","Epoch 49: val_loss improved from 1.43338 to 1.39454, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3090 - mse: 3.5772 - val_loss: 1.3945 - val_mse: 0.1174\n","Epoch 50/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1976 - mse: 3.2833 \n","Epoch 50: val_loss improved from 1.39454 to 1.37784, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2038 - mse: 3.3680 - val_loss: 1.3778 - val_mse: 0.2123\n","Epoch 51/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2295 - mse: 3.7929 \n","Epoch 51: val_loss improved from 1.37784 to 1.33577, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2311 - mse: 3.8235 - val_loss: 1.3358 - val_mse: 0.2627\n","Epoch 52/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1395 - mse: 3.3101 \n","Epoch 52: val_loss improved from 1.33577 to 1.26497, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1313 - mse: 3.3650 - val_loss: 1.2650 - val_mse: 0.1303\n","Epoch 53/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0391 - mse: 2.9995 \n","Epoch 53: val_loss improved from 1.26497 to 1.23736, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0484 - mse: 3.1229 - val_loss: 1.2374 - val_mse: 0.1633\n","Epoch 54/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0728 - mse: 3.4048 \n","Epoch 54: val_loss improved from 1.23736 to 1.16817, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0788 - mse: 3.5359 - val_loss: 1.1682 - val_mse: 0.1085\n","Epoch 55/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9992 - mse: 3.2951 \n","Epoch 55: val_loss improved from 1.16817 to 1.11871, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9936 - mse: 3.2933 - val_loss: 1.1187 - val_mse: 0.0920\n","Epoch 56/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9112 - mse: 2.8872 \n","Epoch 56: val_loss improved from 1.11871 to 1.09541, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9228 - mse: 3.0433 - val_loss: 1.0954 - val_mse: 0.1242\n","Epoch 57/100\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9624 - mse: 3.4715 \n","Epoch 57: val_loss improved from 1.09541 to 1.06937, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.9151 - mse: 3.3017 - val_loss: 1.0694 - val_mse: 0.1515\n","Epoch 58/100\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8651 - mse: 3.3621 \n","Epoch 58: val_loss improved from 1.06937 to 1.02983, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8706 - mse: 3.4490 - val_loss: 1.0298 - val_mse: 0.1762\n","Epoch 59/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9280 - mse: 3.8317 \n","Epoch 59: val_loss improved from 1.02983 to 0.99934, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8985 - mse: 3.7585 - val_loss: 0.9993 - val_mse: 0.1814\n","Epoch 60/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8004 - mse: 3.5195 \n","Epoch 60: val_loss improved from 0.99934 to 0.93281, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.7770 - mse: 3.3879 - val_loss: 0.9328 - val_mse: 0.0973\n","Epoch 61/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7459 - mse: 3.4116 \n","Epoch 61: val_loss improved from 0.93281 to 0.88930, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7460 - mse: 3.3697 - val_loss: 0.8893 - val_mse: 0.0788\n","Epoch 62/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7472 - mse: 3.6093 \n","Epoch 62: val_loss improved from 0.88930 to 0.85552, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7431 - mse: 3.6764 - val_loss: 0.8555 - val_mse: 0.0783\n","Epoch 63/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7158 - mse: 3.5171 \n","Epoch 63: val_loss improved from 0.85552 to 0.83629, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6958 - mse: 3.4075 - val_loss: 0.8363 - val_mse: 0.1087\n","Epoch 64/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6982 - mse: 3.3550 \n","Epoch 64: val_loss improved from 0.83629 to 0.80766, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6805 - mse: 3.2985 - val_loss: 0.8077 - val_mse: 0.1092\n","Epoch 65/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7778 - mse: 4.8544 \n","Epoch 65: val_loss improved from 0.80766 to 0.76287, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7494 - mse: 4.7309 - val_loss: 0.7629 - val_mse: 0.0776\n","Epoch 66/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6720 - mse: 4.0712 \n","Epoch 66: val_loss improved from 0.76287 to 0.74165, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6611 - mse: 3.9920 - val_loss: 0.7417 - val_mse: 0.0923\n","Epoch 67/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5694 - mse: 3.3045 \n","Epoch 67: val_loss improved from 0.74165 to 0.73254, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5669 - mse: 3.3681 - val_loss: 0.7325 - val_mse: 0.1352\n","Epoch 68/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4325 - mse: 2.8782 \n","Epoch 68: val_loss improved from 0.73254 to 0.70288, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4227 - mse: 2.8475 - val_loss: 0.7029 - val_mse: 0.1229\n","Epoch 69/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5155 - mse: 3.2682 \n","Epoch 69: val_loss improved from 0.70288 to 0.66594, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5037 - mse: 3.2581 - val_loss: 0.6659 - val_mse: 0.1001\n","Epoch 70/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4555 - mse: 3.1564 \n","Epoch 70: val_loss improved from 0.66594 to 0.63535, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4688 - mse: 3.2411 - val_loss: 0.6354 - val_mse: 0.0891\n","Epoch 71/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4133 - mse: 3.0161 \n","Epoch 71: val_loss improved from 0.63535 to 0.60382, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4106 - mse: 3.0109 - val_loss: 0.6038 - val_mse: 0.0725\n","Epoch 72/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3611 - mse: 3.4401 \n","Epoch 72: val_loss improved from 0.60382 to 0.58550, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3612 - mse: 3.3404 - val_loss: 0.5855 - val_mse: 0.0809\n","Epoch 73/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3871 - mse: 2.8572 \n","Epoch 73: val_loss improved from 0.58550 to 0.56038, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3812 - mse: 2.8680 - val_loss: 0.5604 - val_mse: 0.0735\n","Epoch 74/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4505 - mse: 4.4301 \n","Epoch 74: val_loss improved from 0.56038 to 0.54803, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.4221 - mse: 4.1203 - val_loss: 0.5480 - val_mse: 0.0903\n","Epoch 75/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3258 - mse: 3.2576 \n","Epoch 75: val_loss improved from 0.54803 to 0.52631, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3170 - mse: 3.2028 - val_loss: 0.5263 - val_mse: 0.0869\n","Epoch 76/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2618 - mse: 2.8712 \n","Epoch 76: val_loss improved from 0.52631 to 0.51814, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2622 - mse: 2.9053 - val_loss: 0.5181 - val_mse: 0.1122\n","Epoch 77/100\n","\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3772 - mse: 3.3887 \n","Epoch 77: val_loss improved from 0.51814 to 0.49230, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3474 - mse: 3.2595 - val_loss: 0.4923 - val_mse: 0.0934\n","Epoch 78/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2685 - mse: 3.2679 \n","Epoch 78: val_loss improved from 0.49230 to 0.48173, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2642 - mse: 3.0954 - val_loss: 0.4817 - val_mse: 0.1072\n","Epoch 79/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2489 - mse: 3.0997 \n","Epoch 79: val_loss improved from 0.48173 to 0.44494, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2568 - mse: 3.1917 - val_loss: 0.4449 - val_mse: 0.0647\n","Epoch 80/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2337 - mse: 2.8712 \n","Epoch 80: val_loss improved from 0.44494 to 0.43819, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2222 - mse: 2.8111 - val_loss: 0.4382 - val_mse: 0.0814\n","Epoch 81/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0917 - mse: 2.9069 \n","Epoch 81: val_loss improved from 0.43819 to 0.42251, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1012 - mse: 2.8757 - val_loss: 0.4225 - val_mse: 0.0825\n","Epoch 82/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2010 - mse: 2.7826 \n","Epoch 82: val_loss improved from 0.42251 to 0.39609, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1931 - mse: 2.7668 - val_loss: 0.3961 - val_mse: 0.0622\n","Epoch 83/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1715 - mse: 2.8762 \n","Epoch 83: val_loss improved from 0.39609 to 0.39164, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1837 - mse: 2.9992 - val_loss: 0.3916 - val_mse: 0.0830\n","Epoch 84/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2333 - mse: 3.7477 \n","Epoch 84: val_loss improved from 0.39164 to 0.38947, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2242 - mse: 3.5969 - val_loss: 0.3895 - val_mse: 0.1068\n","Epoch 85/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2038 - mse: 3.0920 \n","Epoch 85: val_loss improved from 0.38947 to 0.38878, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1711 - mse: 2.9104 - val_loss: 0.3888 - val_mse: 0.1311\n","Epoch 86/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0336 - mse: 2.5647 \n","Epoch 86: val_loss improved from 0.38878 to 0.35662, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0541 - mse: 2.7279 - val_loss: 0.3566 - val_mse: 0.0943\n","Epoch 87/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0546 - mse: 2.5939 \n","Epoch 87: val_loss improved from 0.35662 to 0.34888, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0516 - mse: 2.5949 - val_loss: 0.3489 - val_mse: 0.1049\n","Epoch 88/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0963 - mse: 2.6231 \n","Epoch 88: val_loss improved from 0.34888 to 0.31471, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0878 - mse: 2.5877 - val_loss: 0.3147 - val_mse: 0.0591\n","Epoch 89/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1015 - mse: 2.7750 \n","Epoch 89: val_loss improved from 0.31471 to 0.30839, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0965 - mse: 2.7624 - val_loss: 0.3084 - val_mse: 0.0667\n","Epoch 90/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9675 - mse: 2.1804 \n","Epoch 90: val_loss improved from 0.30839 to 0.30260, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9794 - mse: 2.2333 - val_loss: 0.3026 - val_mse: 0.0724\n","Epoch 91/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1191 - mse: 3.3647 \n","Epoch 91: val_loss did not improve from 0.30260\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0942 - mse: 3.1795 - val_loss: 0.3064 - val_mse: 0.0984\n","Epoch 92/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0098 - mse: 2.4336 \n","Epoch 92: val_loss improved from 0.30260 to 0.27504, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0150 - mse: 2.4643 - val_loss: 0.2750 - val_mse: 0.0558\n","Epoch 93/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0028 - mse: 2.4886 \n","Epoch 93: val_loss improved from 0.27504 to 0.26429, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9987 - mse: 2.4756 - val_loss: 0.2643 - val_mse: 0.0516\n","Epoch 94/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0025 - mse: 2.6875 \n","Epoch 94: val_loss did not improve from 0.26429\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0134 - mse: 2.7006 - val_loss: 0.2656 - val_mse: 0.0708\n","Epoch 95/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0677 - mse: 3.4243 \n","Epoch 95: val_loss did not improve from 0.26429\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0546 - mse: 3.2446 - val_loss: 0.3004 - val_mse: 0.1803\n","Epoch 96/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9585 - mse: 2.5185 \n","Epoch 96: val_loss improved from 0.26429 to 0.26208, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9622 - mse: 2.5449 - val_loss: 0.2621 - val_mse: 0.1040\n","Epoch 97/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9475 - mse: 2.5265 \n","Epoch 97: val_loss improved from 0.26208 to 0.23083, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9615 - mse: 2.6767 - val_loss: 0.2308 - val_mse: 0.0486\n","Epoch 98/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0033 - mse: 2.8696 \n","Epoch 98: val_loss did not improve from 0.23083\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0062 - mse: 2.9372 - val_loss: 0.2409 - val_mse: 0.0845\n","Epoch 99/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8850 - mse: 2.2201 \n","Epoch 99: val_loss did not improve from 0.23083\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8932 - mse: 2.3058 - val_loss: 0.2341 - val_mse: 0.0858\n","Epoch 100/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9352 - mse: 2.5462 \n","Epoch 100: val_loss improved from 0.23083 to 0.22590, saving model to best_model_Nadam_loss_huber_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9342 - mse: 2.5283 - val_loss: 0.2259 - val_mse: 0.0804\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: Nadam, Loss Function: huber, Epochs: 100, Test MSE: 0.1070295830127915\n"]}],"source":["# Additional training with different loss functions\n","for opt in optimizers:\n","    opt_name = opt.__name__  # Access the class name directly\n","    for loss in loss_functions:\n","        for epochs in additional_epochs:\n","            print(f\"Training with optimizer: {opt_name}, loss function: {loss}, epochs: {epochs}\")\n","            \n","            set_seed(seed_value)  # Reinitialize the seed before creating the model\n","            optimizer_instance = opt()  # Create a new instance of the optimizer\n","            model = create_model(optimizer=optimizer_instance, loss=loss)\n","            \n","            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","            model_checkpoint = ModelCheckpoint(f'best_model_{opt_name}_loss_{loss}_epochs_{epochs}.keras', monitor='val_loss', save_best_only=True, verbose=1)\n","            \n","            history = model.fit(X_train_scaled, y_train_log, validation_split=0.2, epochs=epochs, batch_size=32, \n","                                callbacks=[early_stopping, model_checkpoint], verbose=1)\n","            \n","            # Load the best model saved by ModelCheckpoint\n","            model.load_weights(f'best_model_{opt_name}_loss_{loss}_epochs_{epochs}.keras')\n","            \n","            y_pred_log = model.predict(X_val_scaled)\n","            \n","            mse = mean_squared_error(y_val_log, y_pred_log.flatten())\n","            \n","            results[opt_name][f'epochs_{epochs}_loss_{loss}'] = {'MSE': mse, 'history': history.history}\n","            \n","            print(f\"Optimizer: {opt_name}, Loss Function: {loss}, Epochs: {epochs}, Test MSE: {mse}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### test leanring rate"]},{"cell_type":"markdown","metadata":{},"source":["We perform additional training with different learning rates for each optimizer in the optimizers list. For each combination of optimizer and learning rate in learning_rates, we display a message indicating the optimizer, learning rate, and the number of epochs being trained (100 epochs).\n","\n","We reset the random seed to ensure reproducibility (set_seed(seed_value)). Next, we create an instance of the optimizer specifying the learning rate (learning_rate=lr) and a model using the create_model function with the optimizer and the default loss function 'mse'.\n","\n","We define callbacks for early stopping (EarlyStopping) and saving the best model (ModelCheckpoint). We train the model on normalized and log-transformed training data (X_train_scaled and y_train_log), using 20% cross-validation of the data, for 100 epochs.\n","\n","Next, we check if the training produced NaN values in losses (loss) or validation losses (val_loss). If NaN values are detected, we display a message and move on to the next combination.\n","\n","If no NaN value is detected, we load the weights of the best model saved by ModelCheckpoint. We make predictions on the normalized validation data (X_val_scaled), calculate the mean square error (MSE) between the predicted and actual log-transformed values, and store the results (MSE and training history) in the results dictionary.\n","\n","Finally, we display the MSE for the optimizer, the learning rate, and the 100 epochs currently being trained."]},{"cell_type":"markdown","metadata":{},"source":["The loss (loss) and mean square error (mse) values became NaN during training with a high learning rate (0.1). This often happens because the learning rate is too high, causing the model weights to oscillate unsteadily.\n","To resolve this issue, we can add a check to ignore hyperparameter combinations that result in NaN values during training."]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T13:04:11.967273Z","iopub.status.busy":"2024-06-02T13:04:11.966459Z","iopub.status.idle":"2024-06-02T13:10:20.489620Z","shell.execute_reply":"2024-06-02T13:10:20.488635Z","shell.execute_reply.started":"2024-06-02T13:04:11.967238Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training with optimizer: Adam, learning rate: 0.001, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - loss: 151.7273 - mse: 146.1441\n","Epoch 1: val_loss improved from inf to 144.39944, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 346ms/step - loss: 151.7009 - mse: 146.1179 - val_loss: 144.3994 - val_mse: 138.8348\n","Epoch 2/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.4527 - mse: 141.8902 \n","Epoch 2: val_loss improved from 144.39944 to 139.98276, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 146.9201 - mse: 141.3585 - val_loss: 139.9828 - val_mse: 134.4307\n","Epoch 3/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.4168 - mse: 134.8662 \n","Epoch 3: val_loss improved from 139.98276 to 135.39253, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 140.3515 - mse: 134.8015 - val_loss: 135.3925 - val_mse: 129.8475\n","Epoch 4/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 135.8861 - mse: 130.3417 \n","Epoch 4: val_loss improved from 135.39253 to 129.22597, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 135.6197 - mse: 130.0756 - val_loss: 129.2260 - val_mse: 123.6852\n","Epoch 5/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 131.5241 - mse: 125.9838 \n","Epoch 5: val_loss improved from 129.22597 to 123.52956, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 131.3670 - mse: 125.8267 - val_loss: 123.5296 - val_mse: 117.9900\n","Epoch 6/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.2742 - mse: 118.7348 \n","Epoch 6: val_loss improved from 123.52956 to 116.66747, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 123.9950 - mse: 118.4557 - val_loss: 116.6675 - val_mse: 111.1295\n","Epoch 7/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.0698 - mse: 110.5313 \n","Epoch 7: val_loss improved from 116.66747 to 107.28171, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 115.7553 - mse: 110.2166 - val_loss: 107.2817 - val_mse: 101.7410\n","Epoch 8/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 110.6011 - mse: 105.0600 \n","Epoch 8: val_loss improved from 107.28171 to 97.49488, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 109.8402 - mse: 104.2990 - val_loss: 97.4949 - val_mse: 91.9523\n","Epoch 9/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.8980 - mse: 95.3548 \n","Epoch 9: val_loss improved from 97.49488 to 88.04699, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 100.1450 - mse: 94.6014 - val_loss: 88.0470 - val_mse: 82.4999\n","Epoch 10/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 91.6700 - mse: 86.1222 \n","Epoch 10: val_loss improved from 88.04699 to 78.06598, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 90.8450 - mse: 85.2969 - val_loss: 78.0660 - val_mse: 72.5153\n","Epoch 11/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78.4792 - mse: 72.9278 \n","Epoch 11: val_loss improved from 78.06598 to 67.31535, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 77.7782 - mse: 72.2264 - val_loss: 67.3153 - val_mse: 61.7604\n","Epoch 12/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.0871 - mse: 62.5316 \n","Epoch 12: val_loss improved from 67.31535 to 56.45268, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 67.3573 - mse: 61.8016 - val_loss: 56.4527 - val_mse: 50.8937\n","Epoch 13/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58.4403 - mse: 52.8808 \n","Epoch 13: val_loss improved from 56.45268 to 46.37225, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 57.4853 - mse: 51.9254 - val_loss: 46.3722 - val_mse: 40.8097\n","Epoch 14/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.4688 - mse: 42.9057 \n","Epoch 14: val_loss improved from 46.37225 to 37.29024, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 48.1428 - mse: 42.5797 - val_loss: 37.2902 - val_mse: 31.7267\n","Epoch 15/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.7089 - mse: 35.1451 \n","Epoch 15: val_loss improved from 37.29024 to 29.86378, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 39.9165 - mse: 34.3527 - val_loss: 29.8638 - val_mse: 24.3005\n","Epoch 16/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.6242 - mse: 27.0613 \n","Epoch 16: val_loss improved from 29.86378 to 22.87077, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32.1138 - mse: 26.5511 - val_loss: 22.8708 - val_mse: 17.3102\n","Epoch 17/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.4506 - mse: 22.8908 \n","Epoch 17: val_loss improved from 22.87077 to 17.74915, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27.8346 - mse: 22.2751 - val_loss: 17.7491 - val_mse: 12.1928\n","Epoch 18/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.0191 - mse: 17.4636 \n","Epoch 18: val_loss improved from 17.74915 to 14.01122, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22.7588 - mse: 17.2037 - val_loss: 14.0112 - val_mse: 8.4600\n","Epoch 19/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18.5111 - mse: 12.9609 \n","Epoch 19: val_loss improved from 14.01122 to 11.48183, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.6108 - mse: 13.0610 - val_loss: 11.4818 - val_mse: 5.9360\n","Epoch 20/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.5421 - mse: 11.9972 \n","Epoch 20: val_loss improved from 11.48183 to 9.26194, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.2922 - mse: 11.7477 - val_loss: 9.2619 - val_mse: 3.7214\n","Epoch 21/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.6067 - mse: 10.0671 \n","Epoch 21: val_loss improved from 9.26194 to 7.97892, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.5803 - mse: 10.0412 - val_loss: 7.9789 - val_mse: 2.4447\n","Epoch 22/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.8757 - mse: 9.3428  \n","Epoch 22: val_loss improved from 7.97892 to 7.18055, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.6779 - mse: 9.1455 - val_loss: 7.1806 - val_mse: 1.6531\n","Epoch 23/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.8208 - mse: 7.2947 \n","Epoch 23: val_loss improved from 7.18055 to 6.63913, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.7614 - mse: 7.2359 - val_loss: 6.6391 - val_mse: 1.1205\n","Epoch 24/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.0252 - mse: 7.5084 \n","Epoch 24: val_loss improved from 6.63913 to 6.41648, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.9834 - mse: 7.4673 - val_loss: 6.4165 - val_mse: 0.9064\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.5363 - mse: 7.0277 \n","Epoch 25: val_loss improved from 6.41648 to 6.17031, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.5881 - mse: 7.0800 - val_loss: 6.1703 - val_mse: 0.6686\n","Epoch 26/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.0338 - mse: 6.5337 \n","Epoch 26: val_loss improved from 6.17031 to 6.10852, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.0157 - mse: 6.5163 - val_loss: 6.1085 - val_mse: 0.6163\n","Epoch 27/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.9274 - mse: 6.4369 \n","Epoch 27: val_loss improved from 6.10852 to 6.04258, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.8970 - mse: 6.4072 - val_loss: 6.0426 - val_mse: 0.5606\n","Epoch 28/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.0947 - mse: 5.6144\n","Epoch 28: val_loss improved from 6.04258 to 5.91700, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 11.2197 - mse: 5.7405 - val_loss: 5.9170 - val_mse: 0.4460\n","Epoch 29/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.3813 - mse: 5.9119\n","Epoch 29: val_loss improved from 5.91700 to 5.82837, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.3608 - mse: 5.8925 - val_loss: 5.8284 - val_mse: 0.3687\n","Epoch 30/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.3561 - mse: 6.8983 \n","Epoch 30: val_loss improved from 5.82837 to 5.79699, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.1112 - mse: 6.6542 - val_loss: 5.7970 - val_mse: 0.3477\n","Epoch 31/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.0570 - mse: 5.6096 \n","Epoch 31: val_loss did not improve from 5.79699\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.9580 - mse: 5.5115 - val_loss: 5.8048 - val_mse: 0.3670\n","Epoch 32/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7741 - mse: 5.3382 \n","Epoch 32: val_loss improved from 5.79699 to 5.74989, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.6560 - mse: 5.2209 - val_loss: 5.7499 - val_mse: 0.3237\n","Epoch 33/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6612 - mse: 5.2371 \n","Epoch 33: val_loss improved from 5.74989 to 5.74659, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.6357 - mse: 5.2127 - val_loss: 5.7466 - val_mse: 0.3331\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.8117 - mse: 5.4005 \n","Epoch 34: val_loss improved from 5.74659 to 5.67390, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.7743 - mse: 5.3638 - val_loss: 5.6739 - val_mse: 0.2728\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.4803 - mse: 5.0816 \n","Epoch 35: val_loss did not improve from 5.67390\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.4753 - mse: 5.0774 - val_loss: 5.6852 - val_mse: 0.2969\n","Epoch 36/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6741 - mse: 5.2881 \n","Epoch 36: val_loss did not improve from 5.67390\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.6231 - mse: 5.2378 - val_loss: 5.6782 - val_mse: 0.3022\n","Epoch 37/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8658 - mse: 4.4921 \n","Epoch 37: val_loss improved from 5.67390 to 5.64751, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.0101 - mse: 4.6376 - val_loss: 5.6475 - val_mse: 0.2857\n","Epoch 38/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3989 - mse: 4.0397 \n","Epoch 38: val_loss improved from 5.64751 to 5.64236, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.5037 - mse: 4.1456 - val_loss: 5.6424 - val_mse: 0.2953\n","Epoch 39/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1026 - mse: 4.7582\n","Epoch 39: val_loss improved from 5.64236 to 5.57151, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.2100 - mse: 4.8666 - val_loss: 5.5715 - val_mse: 0.2389\n","Epoch 40/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6429 - mse: 4.3130 \n","Epoch 40: val_loss improved from 5.57151 to 5.54069, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.6956 - mse: 4.3667 - val_loss: 5.5407 - val_mse: 0.2225\n","Epoch 41/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4158 - mse: 4.1002 \n","Epoch 41: val_loss did not improve from 5.54069\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.4708 - mse: 4.1562 - val_loss: 5.5625 - val_mse: 0.2590\n","Epoch 42/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1686 - mse: 3.8679  \n","Epoch 42: val_loss improved from 5.54069 to 5.52765, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2044 - mse: 3.9048 - val_loss: 5.5277 - val_mse: 0.2398\n","Epoch 43/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5550 - mse: 4.2698 \n","Epoch 43: val_loss improved from 5.52765 to 5.49161, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.4847 - mse: 4.2006 - val_loss: 5.4916 - val_mse: 0.2193\n","Epoch 44/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2191 - mse: 3.9496 \n","Epoch 44: val_loss improved from 5.49161 to 5.45722, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.1685 - mse: 3.9003 - val_loss: 5.4572 - val_mse: 0.2016\n","Epoch 45/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4901 - mse: 4.2372 \n","Epoch 45: val_loss improved from 5.45722 to 5.45486, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.5190 - mse: 4.2675 - val_loss: 5.4549 - val_mse: 0.2160\n","Epoch 46/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9519 - mse: 3.7160 \n","Epoch 46: val_loss improved from 5.45486 to 5.40670, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.9436 - mse: 3.7091 - val_loss: 5.4067 - val_mse: 0.1855\n","Epoch 47/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5277 - mse: 4.3096 \n","Epoch 47: val_loss improved from 5.40670 to 5.38643, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.5854 - mse: 4.3685 - val_loss: 5.3864 - val_mse: 0.1827\n","Epoch 48/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5505 - mse: 3.3496 \n","Epoch 48: val_loss did not improve from 5.38643\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6680 - mse: 3.4686 - val_loss: 5.4034 - val_mse: 0.2175\n","Epoch 49/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7383 - mse: 3.5558 \n","Epoch 49: val_loss improved from 5.38643 to 5.35821, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7792 - mse: 3.5977 - val_loss: 5.3582 - val_mse: 0.1899\n","Epoch 50/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5801 - mse: 3.4151 \n","Epoch 50: val_loss improved from 5.35821 to 5.33854, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5627 - mse: 3.3989 - val_loss: 5.3385 - val_mse: 0.1888\n","Epoch 51/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7143 - mse: 3.5679 \n","Epoch 51: val_loss improved from 5.33854 to 5.27145, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7939 - mse: 3.6488 - val_loss: 5.2714 - val_mse: 0.1407\n","Epoch 52/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7216 - mse: 3.5941  \n","Epoch 52: val_loss improved from 5.27145 to 5.25533, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.6636 - mse: 3.5375 - val_loss: 5.2553 - val_mse: 0.1437\n","Epoch 53/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0092 - mse: 3.9007 \n","Epoch 53: val_loss did not improve from 5.25533\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.8717 - mse: 3.7647 - val_loss: 5.2708 - val_mse: 0.1780\n","Epoch 54/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0543 - mse: 3.9648 \n","Epoch 54: val_loss improved from 5.25533 to 5.20475, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.9615 - mse: 3.8735 - val_loss: 5.2047 - val_mse: 0.1316\n","Epoch 55/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3787 - mse: 3.3090 \n","Epoch 55: val_loss improved from 5.20475 to 5.19456, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.4115 - mse: 3.3433 - val_loss: 5.1946 - val_mse: 0.1414\n","Epoch 56/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3564 - mse: 3.3068 \n","Epoch 56: val_loss improved from 5.19456 to 5.18873, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3940 - mse: 3.3458 - val_loss: 5.1887 - val_mse: 0.1557\n","Epoch 57/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3504 - mse: 3.3208 \n","Epoch 57: val_loss improved from 5.18873 to 5.15845, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3097 - mse: 3.2816 - val_loss: 5.1585 - val_mse: 0.1456\n","Epoch 58/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1425 - mse: 3.1333 \n","Epoch 58: val_loss improved from 5.15845 to 5.15418, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2190 - mse: 3.2113 - val_loss: 5.1542 - val_mse: 0.1621\n","Epoch 59/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9292 - mse: 3.9408 \n","Epoch 59: val_loss improved from 5.15418 to 5.09189, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7991 - mse: 3.8120 - val_loss: 5.0919 - val_mse: 0.1205\n","Epoch 60/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0103 - mse: 3.0427 \n","Epoch 60: val_loss improved from 5.09189 to 5.06318, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0073 - mse: 3.0413 - val_loss: 5.0632 - val_mse: 0.1137\n","Epoch 61/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9177 - mse: 2.9719 \n","Epoch 61: val_loss improved from 5.06318 to 5.04612, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.9505 - mse: 3.0065 - val_loss: 5.0461 - val_mse: 0.1185\n","Epoch 62/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1703 - mse: 3.2467 \n","Epoch 62: val_loss improved from 5.04612 to 5.02270, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1463 - mse: 3.2242 - val_loss: 5.0227 - val_mse: 0.1173\n","Epoch 63/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9867 - mse: 3.0851 \n","Epoch 63: val_loss improved from 5.02270 to 4.98946, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0034 - mse: 3.1034 - val_loss: 4.9895 - val_mse: 0.1061\n","Epoch 64/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1760 - mse: 3.2965 \n","Epoch 64: val_loss improved from 4.98946 to 4.97769, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1513 - mse: 3.2735 - val_loss: 4.9777 - val_mse: 0.1169\n","Epoch 65/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3578 - mse: 3.5008  \n","Epoch 65: val_loss improved from 4.97769 to 4.94402, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2843 - mse: 3.4290 - val_loss: 4.9440 - val_mse: 0.1056\n","Epoch 66/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1587 - mse: 3.3242 \n","Epoch 66: val_loss improved from 4.94402 to 4.91745, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1333 - mse: 3.3005 - val_loss: 4.9174 - val_mse: 0.1018\n","Epoch 67/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0007 - mse: 3.1891 \n","Epoch 67: val_loss improved from 4.91745 to 4.88479, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0091 - mse: 3.1990 - val_loss: 4.8848 - val_mse: 0.0917\n","Epoch 68/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9643 - mse: 3.1748 \n","Epoch 68: val_loss improved from 4.88479 to 4.86526, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.8264 - mse: 3.0389 - val_loss: 4.8653 - val_mse: 0.0952\n","Epoch 69/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7831 - mse: 3.0172 \n","Epoch 69: val_loss improved from 4.86526 to 4.84919, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7598 - mse: 2.9957 - val_loss: 4.8492 - val_mse: 0.1028\n","Epoch 70/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4524 - mse: 2.7102 \n","Epoch 70: val_loss improved from 4.84919 to 4.82795, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.4954 - mse: 2.7551 - val_loss: 4.8280 - val_mse: 0.1060\n","Epoch 71/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0368 - mse: 2.3192 \n","Epoch 71: val_loss improved from 4.82795 to 4.79638, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.0996 - mse: 2.3839 - val_loss: 4.7964 - val_mse: 0.0993\n","Epoch 72/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1941 - mse: 2.5014 \n","Epoch 72: val_loss improved from 4.79638 to 4.77325, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1939 - mse: 2.5028 - val_loss: 4.7733 - val_mse: 0.1008\n","Epoch 73/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6958 - mse: 3.0277 \n","Epoch 73: val_loss improved from 4.77325 to 4.74504, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.6580 - mse: 2.9918 - val_loss: 4.7450 - val_mse: 0.0977\n","Epoch 74/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6451 - mse: 3.0024 \n","Epoch 74: val_loss improved from 4.74504 to 4.70856, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5729 - mse: 2.9318 - val_loss: 4.7086 - val_mse: 0.0866\n","Epoch 75/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5233 - mse: 2.9058 \n","Epoch 75: val_loss improved from 4.70856 to 4.69286, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5030 - mse: 2.8874 - val_loss: 4.6929 - val_mse: 0.0962\n","Epoch 76/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1248 - mse: 2.5328 \n","Epoch 76: val_loss improved from 4.69286 to 4.65915, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1559 - mse: 2.5656 - val_loss: 4.6591 - val_mse: 0.0881\n","Epoch 77/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2340 - mse: 2.6671 \n","Epoch 77: val_loss improved from 4.65915 to 4.63823, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2664 - mse: 2.7017 - val_loss: 4.6382 - val_mse: 0.0926\n","Epoch 78/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0429 - mse: 2.5018 \n","Epoch 78: val_loss improved from 4.63823 to 4.61531, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.0738 - mse: 2.5346 - val_loss: 4.6153 - val_mse: 0.0955\n","Epoch 79/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7756 - mse: 2.2598 \n","Epoch 79: val_loss improved from 4.61531 to 4.56622, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8094 - mse: 2.2962 - val_loss: 4.5662 - val_mse: 0.0729\n","Epoch 80/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0454 - mse: 2.5567 \n","Epoch 80: val_loss improved from 4.56622 to 4.54795, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.0388 - mse: 2.5521 - val_loss: 4.5479 - val_mse: 0.0811\n","Epoch 81/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8140 - mse: 2.3519 \n","Epoch 81: val_loss improved from 4.54795 to 4.52064, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8322 - mse: 2.3720 - val_loss: 4.5206 - val_mse: 0.0804\n","Epoch 82/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2244 - mse: 2.7891 \n","Epoch 82: val_loss improved from 4.52064 to 4.48695, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1540 - mse: 2.7204 - val_loss: 4.4869 - val_mse: 0.0733\n","Epoch 83/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8011 - mse: 2.3924 \n","Epoch 83: val_loss improved from 4.48695 to 4.45687, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8392 - mse: 2.4323 - val_loss: 4.4569 - val_mse: 0.0700\n","Epoch 84/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7335 - mse: 2.3513 \n","Epoch 84: val_loss improved from 4.45687 to 4.42955, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7862 - mse: 2.4061 - val_loss: 4.4296 - val_mse: 0.0697\n","Epoch 85/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8516 - mse: 2.4964 \n","Epoch 85: val_loss improved from 4.42955 to 4.41487, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7973 - mse: 2.4442 - val_loss: 4.4149 - val_mse: 0.0822\n","Epoch 86/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6971 - mse: 2.3691 \n","Epoch 86: val_loss improved from 4.41487 to 4.39185, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7096 - mse: 2.3837 - val_loss: 4.3918 - val_mse: 0.0865\n","Epoch 87/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7237 - mse: 2.4234 \n","Epoch 87: val_loss improved from 4.39185 to 4.36133, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6738 - mse: 2.3752 - val_loss: 4.3613 - val_mse: 0.0835\n","Epoch 88/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9999 - mse: 2.7269 \n","Epoch 88: val_loss improved from 4.36133 to 4.31894, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.9716 - mse: 2.7008 - val_loss: 4.3189 - val_mse: 0.0692\n","Epoch 89/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0475 - mse: 2.8027 \n","Epoch 89: val_loss improved from 4.31894 to 4.29131, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.0360 - mse: 2.7932 - val_loss: 4.2913 - val_mse: 0.0696\n","Epoch 90/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2003 - mse: 1.9835 \n","Epoch 90: val_loss improved from 4.29131 to 4.25946, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2908 - mse: 2.0761 - val_loss: 4.2595 - val_mse: 0.0661\n","Epoch 91/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7074 - mse: 2.5192 \n","Epoch 91: val_loss improved from 4.25946 to 4.24171, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6550 - mse: 2.4687 - val_loss: 4.2417 - val_mse: 0.0768\n","Epoch 92/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4776 - mse: 2.3179 \n","Epoch 92: val_loss improved from 4.24171 to 4.19684, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4931 - mse: 2.3353 - val_loss: 4.1968 - val_mse: 0.0608\n","Epoch 93/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3856 - mse: 2.2542 \n","Epoch 93: val_loss improved from 4.19684 to 4.16796, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4152 - mse: 2.2862 - val_loss: 4.1680 - val_mse: 0.0605\n","Epoch 94/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2942 - mse: 2.1914 \n","Epoch 94: val_loss improved from 4.16796 to 4.15124, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3616 - mse: 2.2613 - val_loss: 4.1512 - val_mse: 0.0729\n","Epoch 95/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4920 - mse: 2.4185 \n","Epoch 95: val_loss did not improve from 4.15124\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.5026 - mse: 2.4316 - val_loss: 4.1545 - val_mse: 0.1055\n","Epoch 96/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1161 - mse: 2.0725 \n","Epoch 96: val_loss improved from 4.15124 to 4.09692, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1309 - mse: 2.0893 - val_loss: 4.0969 - val_mse: 0.0775\n","Epoch 97/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2332 - mse: 2.2189 \n","Epoch 97: val_loss improved from 4.09692 to 4.05250, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2023 - mse: 2.1903 - val_loss: 4.0525 - val_mse: 0.0628\n","Epoch 98/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4547 - mse: 2.4701 \n","Epoch 98: val_loss improved from 4.05250 to 4.02983, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4710 - mse: 2.4887 - val_loss: 4.0298 - val_mse: 0.0699\n","Epoch 99/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0824 - mse: 2.1271 \n","Epoch 99: val_loss improved from 4.02983 to 4.01560, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.0889 - mse: 2.1365 - val_loss: 4.0156 - val_mse: 0.0858\n","Epoch 100/100\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.1218 - mse: 2.1961 \n","Epoch 100: val_loss improved from 4.01560 to 3.98453, saving model to best_model_Adam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.1549 - mse: 2.2326 - val_loss: 3.9845 - val_mse: 0.0847\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n","Optimizer: Adam, Learning Rate: 0.001, Epochs: 100, Test MSE: 0.10457430003660123\n","Training with optimizer: Adam, learning rate: 0.01, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - loss: 142.8653 - mse: 136.4975\n","Epoch 1: val_loss improved from inf to 81.46194, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 339ms/step - loss: 142.4407 - mse: 136.0451 - val_loss: 81.4619 - val_mse: 72.9032\n","Epoch 2/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.0375 - mse: 74.2068  \n","Epoch 2: val_loss improved from 81.46194 to 43.84258, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 77.0564 - mse: 68.1533 - val_loss: 43.8426 - val_mse: 34.3310\n","Epoch 3/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.1620 - mse: 7.6180 \n","Epoch 3: val_loss improved from 43.84258 to 17.58456, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.1666 - mse: 7.6211 - val_loss: 17.5846 - val_mse: 8.0905\n","Epoch 4/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.1961 - mse: 5.7435 \n","Epoch 4: val_loss improved from 17.58456 to 12.25440, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.9808 - mse: 5.5520 - val_loss: 12.2544 - val_mse: 3.1112\n","Epoch 5/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.3309 - mse: 4.2756 \n","Epoch 5: val_loss improved from 12.25440 to 9.95440, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.1834 - mse: 4.1690 - val_loss: 9.9544 - val_mse: 1.3649\n","Epoch 6/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.9430 - mse: 3.4588 \n","Epoch 6: val_loss improved from 9.95440 to 8.18243, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.9042 - mse: 3.4663 - val_loss: 8.1824 - val_mse: 0.2067\n","Epoch 7/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6428 - mse: 2.7756 \n","Epoch 7: val_loss improved from 8.18243 to 7.45187, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.5939 - mse: 2.7745 - val_loss: 7.4519 - val_mse: 0.1078\n","Epoch 8/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.2720 - mse: 3.0359 \n","Epoch 8: val_loss improved from 7.45187 to 6.83890, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.1529 - mse: 2.9637 - val_loss: 6.8389 - val_mse: 0.1131\n","Epoch 9/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1729 - mse: 2.5525 \n","Epoch 9: val_loss improved from 6.83890 to 6.20839, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.1762 - mse: 2.6012 - val_loss: 6.2084 - val_mse: 0.0804\n","Epoch 10/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2568 - mse: 2.2298 \n","Epoch 10: val_loss improved from 6.20839 to 5.61595, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2425 - mse: 2.2591 - val_loss: 5.6160 - val_mse: 0.0590\n","Epoch 11/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1579 - mse: 2.6932 \n","Epoch 11: val_loss improved from 5.61595 to 5.15990, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0070 - mse: 2.5820 - val_loss: 5.1599 - val_mse: 0.1257\n","Epoch 12/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8791 - mse: 1.9337 \n","Epoch 12: val_loss improved from 5.15990 to 4.64085, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8603 - mse: 1.9529 - val_loss: 4.6409 - val_mse: 0.1036\n","Epoch 13/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2643 - mse: 1.8077 \n","Epoch 13: val_loss improved from 4.64085 to 4.23215, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2669 - mse: 1.8450 - val_loss: 4.2322 - val_mse: 0.1492\n","Epoch 14/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1939 - mse: 2.1851 \n","Epoch 14: val_loss improved from 4.23215 to 3.74521, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.1214 - mse: 2.1442 - val_loss: 3.7452 - val_mse: 0.0782\n","Epoch 15/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5605 - mse: 1.9622 \n","Epoch 15: val_loss improved from 3.74521 to 3.41870, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5246 - mse: 1.9555 - val_loss: 3.4187 - val_mse: 0.1348\n","Epoch 16/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0541 - mse: 1.8337 \n","Epoch 16: val_loss improved from 3.41870 to 2.98926, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.0017 - mse: 1.8086 - val_loss: 2.9893 - val_mse: 0.0624\n","Epoch 17/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8092 - mse: 1.9388 \n","Epoch 17: val_loss improved from 2.98926 to 2.78596, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7563 - mse: 1.9101 - val_loss: 2.7860 - val_mse: 0.1767\n","Epoch 18/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6860 - mse: 2.1253 \n","Epoch 18: val_loss improved from 2.78596 to 2.40456, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6127 - mse: 2.0766 - val_loss: 2.4046 - val_mse: 0.0808\n","Epoch 19/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2033 - mse: 1.9226 \n","Epoch 19: val_loss improved from 2.40456 to 2.15629, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.1756 - mse: 1.9165 - val_loss: 2.1563 - val_mse: 0.0838\n","Epoch 20/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0525 - mse: 2.0210 \n","Epoch 20: val_loss improved from 2.15629 to 1.93164, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0121 - mse: 1.9981 - val_loss: 1.9316 - val_mse: 0.0853\n","Epoch 21/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5958 - mse: 1.7847 \n","Epoch 21: val_loss improved from 1.93164 to 1.73487, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5655 - mse: 1.7695 - val_loss: 1.7349 - val_mse: 0.0867\n","Epoch 22/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3663 - mse: 1.7506 \n","Epoch 22: val_loss improved from 1.73487 to 1.53366, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3530 - mse: 1.7513 - val_loss: 1.5337 - val_mse: 0.0676\n","Epoch 23/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1348 - mse: 1.6964 \n","Epoch 23: val_loss improved from 1.53366 to 1.44936, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1272 - mse: 1.7002 - val_loss: 1.4494 - val_mse: 0.1337\n","Epoch 24/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2228 - mse: 1.9323 \n","Epoch 24: val_loss improved from 1.44936 to 1.26388, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1999 - mse: 1.9200 - val_loss: 1.2639 - val_mse: 0.0868\n","Epoch 25/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9503 - mse: 1.7960 \n","Epoch 25: val_loss improved from 1.26388 to 1.11209, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9636 - mse: 1.8191 - val_loss: 1.1121 - val_mse: 0.0648\n","Epoch 26/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8010 - mse: 1.7744 \n","Epoch 26: val_loss improved from 1.11209 to 1.09343, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8001 - mse: 1.7819 - val_loss: 1.0934 - val_mse: 0.1516\n","Epoch 27/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6745 - mse: 1.7495 \n","Epoch 27: val_loss improved from 1.09343 to 0.93729, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6791 - mse: 1.7613 - val_loss: 0.9373 - val_mse: 0.0842\n","Epoch 28/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8676 - mse: 2.0301 \n","Epoch 28: val_loss improved from 0.93729 to 0.84350, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8177 - mse: 1.9871 - val_loss: 0.8435 - val_mse: 0.0791\n","Epoch 29/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5175 - mse: 1.7647 \n","Epoch 29: val_loss improved from 0.84350 to 0.78414, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5041 - mse: 1.7561 - val_loss: 0.7841 - val_mse: 0.0829\n","Epoch 30/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5702 - mse: 1.8786 \n","Epoch 30: val_loss improved from 0.78414 to 0.76241, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5333 - mse: 1.8461 - val_loss: 0.7624 - val_mse: 0.1217\n","Epoch 31/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2849 - mse: 1.6565 \n","Epoch 31: val_loss improved from 0.76241 to 0.66426, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3085 - mse: 1.6837 - val_loss: 0.6643 - val_mse: 0.0725\n","Epoch 32/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3837 - mse: 1.7983 \n","Epoch 32: val_loss improved from 0.66426 to 0.61883, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3340 - mse: 1.7523 - val_loss: 0.6188 - val_mse: 0.0787\n","Epoch 33/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0255 - mse: 1.4947 \n","Epoch 33: val_loss improved from 0.61883 to 0.56156, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0529 - mse: 1.5260 - val_loss: 0.5616 - val_mse: 0.0662\n","Epoch 34/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2322 - mse: 1.7413 \n","Epoch 34: val_loss improved from 0.56156 to 0.51793, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2517 - mse: 1.7625 - val_loss: 0.5179 - val_mse: 0.0490\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3208 - mse: 1.8573 \n","Epoch 35: val_loss did not improve from 0.51793\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2936 - mse: 1.8310 - val_loss: 0.5880 - val_mse: 0.1326\n","Epoch 36/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4646 - mse: 2.0143 \n","Epoch 36: val_loss improved from 0.51793 to 0.48610, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4343 - mse: 1.9861 - val_loss: 0.4861 - val_mse: 0.0647\n","Epoch 37/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0001 - mse: 1.5871 \n","Epoch 37: val_loss improved from 0.48610 to 0.47287, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0043 - mse: 1.5945 - val_loss: 0.4729 - val_mse: 0.0922\n","Epoch 38/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0036 - mse: 1.6282 \n","Epoch 38: val_loss improved from 0.47287 to 0.45464, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9957 - mse: 1.6216 - val_loss: 0.4546 - val_mse: 0.0869\n","Epoch 39/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1052 - mse: 1.7404 \n","Epoch 39: val_loss improved from 0.45464 to 0.39943, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1253 - mse: 1.7630 - val_loss: 0.3994 - val_mse: 0.0681\n","Epoch 40/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8631 - mse: 1.5345 \n","Epoch 40: val_loss improved from 0.39943 to 0.39548, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8956 - mse: 1.5673 - val_loss: 0.3955 - val_mse: 0.0704\n","Epoch 41/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7631 - mse: 1.4434 \n","Epoch 41: val_loss improved from 0.39548 to 0.36998, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8196 - mse: 1.5022 - val_loss: 0.3700 - val_mse: 0.0718\n","Epoch 42/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9494 - mse: 1.6503 \n","Epoch 42: val_loss improved from 0.36998 to 0.35682, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9270 - mse: 1.6276 - val_loss: 0.3568 - val_mse: 0.0606\n","Epoch 43/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8932 - mse: 1.5938 \n","Epoch 43: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8788 - mse: 1.5771 - val_loss: 0.3944 - val_mse: 0.0799\n","Epoch 44/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7479 - mse: 1.4350 \n","Epoch 44: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8076 - mse: 1.4961 - val_loss: 0.3799 - val_mse: 0.0822\n","Epoch 45/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9132 - mse: 1.6147 \n","Epoch 45: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9385 - mse: 1.6379 - val_loss: 0.3965 - val_mse: 0.0735\n","Epoch 46/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9973 - mse: 1.6783 \n","Epoch 46: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9629 - mse: 1.6458 - val_loss: 0.3841 - val_mse: 0.0832\n","Epoch 47/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1830 - mse: 1.8744 \n","Epoch 47: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2094 - mse: 1.8984 - val_loss: 0.4179 - val_mse: 0.0918\n","Epoch 48/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7381 - mse: 1.4137 \n","Epoch 48: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7438 - mse: 1.4219 - val_loss: 0.3666 - val_mse: 0.0780\n","Epoch 49/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0146 - mse: 1.7246 \n","Epoch 49: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0260 - mse: 1.7343 - val_loss: 0.3770 - val_mse: 0.0746\n","Epoch 50/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7664 - mse: 1.4690 \n","Epoch 50: val_loss improved from 0.35682 to 0.32417, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7512 - mse: 1.4563 - val_loss: 0.3242 - val_mse: 0.0551\n","Epoch 51/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7863 - mse: 1.5183 \n","Epoch 51: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8120 - mse: 1.5432 - val_loss: 0.4276 - val_mse: 0.1451\n","Epoch 52/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7782 - mse: 1.4940 \n","Epoch 52: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7777 - mse: 1.4946 - val_loss: 0.3601 - val_mse: 0.0999\n","Epoch 53/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6677 - mse: 1.4089 \n","Epoch 53: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7043 - mse: 1.4446 - val_loss: 0.4051 - val_mse: 0.1289\n","Epoch 54/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8976 - mse: 1.6218 \n","Epoch 54: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8949 - mse: 1.6191 - val_loss: 0.3636 - val_mse: 0.0708\n","Epoch 55/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8072 - mse: 1.5094 \n","Epoch 55: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8191 - mse: 1.5212 - val_loss: 0.3513 - val_mse: 0.0667\n","Epoch 56/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6938 - mse: 1.4102 \n","Epoch 56: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7473 - mse: 1.4652 - val_loss: 0.3872 - val_mse: 0.1351\n","Epoch 57/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8850 - mse: 1.6436 \n","Epoch 57: val_loss improved from 0.32417 to 0.30226, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8372 - mse: 1.5985 - val_loss: 0.3023 - val_mse: 0.0737\n","Epoch 58/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6683 - mse: 1.4373 \n","Epoch 58: val_loss did not improve from 0.30226\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7056 - mse: 1.4724 - val_loss: 0.3138 - val_mse: 0.0474\n","Epoch 59/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9757 - mse: 1.7095 \n","Epoch 59: val_loss improved from 0.30226 to 0.29829, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9156 - mse: 1.6512 - val_loss: 0.2983 - val_mse: 0.0556\n","Epoch 60/100\n","\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6253 - mse: 1.3818 \n","Epoch 60: val_loss improved from 0.29829 to 0.27227, saving model to best_model_Adam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.6457 - mse: 1.4023 - val_loss: 0.2723 - val_mse: 0.0443\n","Epoch 61/100\n","\u001b[1m13/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6253 - mse: 1.3997 \n","Epoch 61: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6458 - mse: 1.4159 - val_loss: 0.3348 - val_mse: 0.0518\n","Epoch 62/100\n","\u001b[1m13/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8583 - mse: 1.5692 \n","Epoch 62: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8218 - mse: 1.5328 - val_loss: 0.3363 - val_mse: 0.0709\n","Epoch 63/100\n","\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7892 - mse: 1.5248 \n","Epoch 63: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7909 - mse: 1.5269 - val_loss: 0.3175 - val_mse: 0.0603\n","Epoch 64/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8500 - mse: 1.5929 \n","Epoch 64: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8085 - mse: 1.5488 - val_loss: 0.3367 - val_mse: 0.0582\n","Epoch 65/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9717 - mse: 1.6936 \n","Epoch 65: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9037 - mse: 1.6255 - val_loss: 0.3041 - val_mse: 0.0441\n","Epoch 66/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8940 - mse: 1.6438 \n","Epoch 66: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8728 - mse: 1.6222 - val_loss: 0.4771 - val_mse: 0.1414\n","Epoch 67/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0412 - mse: 1.6705 \n","Epoch 67: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0130 - mse: 1.6333 - val_loss: 0.4953 - val_mse: 0.0928\n","Epoch 68/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6270 - mse: 1.2331 \n","Epoch 68: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6189 - mse: 1.2313 - val_loss: 0.4602 - val_mse: 0.1120\n","Epoch 69/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9112 - mse: 1.5705 \n","Epoch 69: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8518 - mse: 1.5174 - val_loss: 0.3361 - val_mse: 0.0684\n","Epoch 70/100\n","\u001b[1m12/26\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6861 - mse: 1.4240 \n","Epoch 70: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7853 - mse: 1.5260 - val_loss: 0.3172 - val_mse: 0.0639\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n","Optimizer: Adam, Learning Rate: 0.01, Epochs: 100, Test MSE: 0.05699531309437809\n","Training with optimizer: Adam, learning rate: 0.1, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - loss: 147.6269 - mse: 66.7081\n","Epoch 1: val_loss improved from inf to 791.91675, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 337ms/step - loss: 148.1136 - mse: 65.4141 - val_loss: 791.9167 - val_mse: 628.9828\n","Epoch 2/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 148.7557 - mse: 4.8119 \n","Epoch 2: val_loss improved from 791.91675 to 64.08260, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 140.2143 - mse: 4.5084 - val_loss: 64.0826 - val_mse: 1.5093\n","Epoch 3/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.4364 - mse: 2.2939 \n","Epoch 3: val_loss improved from 64.08260 to 23.10631, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 51.6129 - mse: 2.2939 - val_loss: 23.1063 - val_mse: 0.1529\n","Epoch 4/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.9595 - mse: 1.8242 \n","Epoch 4: val_loss improved from 23.10631 to 10.72155, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.7879 - mse: 1.8406 - val_loss: 10.7216 - val_mse: 0.4588\n","Epoch 5/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.4141 - mse: 2.0518 \n","Epoch 5: val_loss improved from 10.72155 to 6.33187, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.9666 - mse: 1.9554 - val_loss: 6.3319 - val_mse: 0.1353\n","Epoch 6/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5483 - mse: 1.5988 \n","Epoch 6: val_loss improved from 6.33187 to 6.01744, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.4738 - mse: 1.5965 - val_loss: 6.0174 - val_mse: 0.8146\n","Epoch 7/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7907 - mse: 1.5093 \n","Epoch 7: val_loss improved from 6.01744 to 4.33404, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7223 - mse: 1.5052 - val_loss: 4.3340 - val_mse: 0.1494\n","Epoch 8/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5634 - mse: 1.4971 \n","Epoch 8: val_loss did not improve from 4.33404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7590 - mse: 1.4855 - val_loss: 7.3211 - val_mse: 0.1546\n","Epoch 9/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7245 - mse: 1.7227 \n","Epoch 9: val_loss did not improve from 4.33404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.5139 - mse: 1.7045 - val_loss: 4.4326 - val_mse: 0.2900\n","Epoch 10/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7707 - mse: 1.4386 \n","Epoch 10: val_loss did not improve from 4.33404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.8470 - mse: 1.4402 - val_loss: 5.0489 - val_mse: 0.1510\n","Epoch 11/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8311 - mse: 1.5644 \n","Epoch 11: val_loss did not improve from 4.33404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.7468 - mse: 1.5478 - val_loss: 4.8229 - val_mse: 0.4947\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5495 - mse: 1.2601 \n","Epoch 12: val_loss did not improve from 4.33404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.5584 - mse: 1.2566 - val_loss: 4.7567 - val_mse: 0.3256\n","Epoch 13/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6207 - mse: 1.1302 \n","Epoch 13: val_loss did not improve from 4.33404\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7017 - mse: 1.1822 - val_loss: 5.5313 - val_mse: 0.1717\n","Epoch 14/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6022 - mse: 1.2971 \n","Epoch 14: val_loss improved from 4.33404 to 4.29974, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4268 - mse: 1.2881 - val_loss: 4.2997 - val_mse: 0.2340\n","Epoch 15/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2019 - mse: 1.1726 \n","Epoch 15: val_loss did not improve from 4.29974\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2055 - mse: 1.1544 - val_loss: 5.7249 - val_mse: 0.4788\n","Epoch 16/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3514 - mse: 1.1176 \n","Epoch 16: val_loss did not improve from 4.29974\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.2568 - mse: 1.0955 - val_loss: 10.4634 - val_mse: 0.2641\n","Epoch 17/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.1665 - mse: 1.0119 \n","Epoch 17: val_loss did not improve from 4.29974\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 21.9146 - mse: 0.9969 - val_loss: 13.3597 - val_mse: 0.1289\n","Epoch 18/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.8595 - mse: 1.0459 \n","Epoch 18: val_loss did not improve from 4.29974\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.2963 - mse: 1.0216 - val_loss: 6.7561 - val_mse: 0.3501\n","Epoch 19/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3767 - mse: 0.9204 \n","Epoch 19: val_loss improved from 4.29974 to 4.24026, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1846 - mse: 0.9169 - val_loss: 4.2403 - val_mse: 0.2022\n","Epoch 20/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8389 - mse: 0.9597 \n","Epoch 20: val_loss did not improve from 4.24026\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.0823 - mse: 0.9472 - val_loss: 7.0369 - val_mse: 0.1943\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4615 - mse: 0.7921 \n","Epoch 21: val_loss improved from 4.24026 to 2.89569, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1439 - mse: 0.7952 - val_loss: 2.8957 - val_mse: 0.1207\n","Epoch 22/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2977 - mse: 0.7155 \n","Epoch 22: val_loss did not improve from 2.89569\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2785 - mse: 0.7117 - val_loss: 3.0042 - val_mse: 0.1814\n","Epoch 23/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1079 - mse: 0.6626 \n","Epoch 23: val_loss improved from 2.89569 to 2.11007, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0488 - mse: 0.6858 - val_loss: 2.1101 - val_mse: 0.2414\n","Epoch 24/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6264 - mse: 0.7999 \n","Epoch 24: val_loss did not improve from 2.11007\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7560 - mse: 0.7758 - val_loss: 2.6863 - val_mse: 0.1304\n","Epoch 25/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7081 - mse: 0.5997 \n","Epoch 25: val_loss improved from 2.11007 to 1.78294, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6069 - mse: 0.6151 - val_loss: 1.7829 - val_mse: 0.2882\n","Epoch 26/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9382 - mse: 0.5403 \n","Epoch 26: val_loss improved from 1.78294 to 1.30504, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9010 - mse: 0.5354 - val_loss: 1.3050 - val_mse: 0.0953\n","Epoch 27/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9604 - mse: 0.4927 \n","Epoch 27: val_loss improved from 1.30504 to 1.27707, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9678 - mse: 0.4933 - val_loss: 1.2771 - val_mse: 0.1316\n","Epoch 28/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6047 - mse: 0.5043 \n","Epoch 28: val_loss improved from 1.27707 to 1.08329, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6204 - mse: 0.4954 - val_loss: 1.0833 - val_mse: 0.0535\n","Epoch 29/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5157 - mse: 0.4658 \n","Epoch 29: val_loss did not improve from 1.08329\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5139 - mse: 0.4680 - val_loss: 1.7490 - val_mse: 0.2679\n","Epoch 30/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8106 - mse: 0.4788 \n","Epoch 30: val_loss improved from 1.08329 to 0.99049, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7340 - mse: 0.4649 - val_loss: 0.9905 - val_mse: 0.0602\n","Epoch 31/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4196 - mse: 0.3542 \n","Epoch 31: val_loss did not improve from 0.99049\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4489 - mse: 0.3590 - val_loss: 1.0981 - val_mse: 0.0743\n","Epoch 32/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3311 - mse: 0.3535 \n","Epoch 32: val_loss improved from 0.99049 to 0.84732, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3126 - mse: 0.3468 - val_loss: 0.8473 - val_mse: 0.1514\n","Epoch 33/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3018 - mse: 0.2995 \n","Epoch 33: val_loss did not improve from 0.84732\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4445 - mse: 0.3019 - val_loss: 1.4944 - val_mse: 0.0601\n","Epoch 34/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5115 - mse: 0.2963 \n","Epoch 34: val_loss did not improve from 0.84732\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4604 - mse: 0.2995 - val_loss: 1.1588 - val_mse: 0.0798\n","Epoch 35/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3565 - mse: 0.3040 \n","Epoch 35: val_loss did not improve from 0.84732\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3145 - mse: 0.3002 - val_loss: 1.0417 - val_mse: 0.0713\n","Epoch 36/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2068 - mse: 0.2924 \n","Epoch 36: val_loss improved from 0.84732 to 0.79939, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1626 - mse: 0.2886 - val_loss: 0.7994 - val_mse: 0.0665\n","Epoch 37/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8389 - mse: 0.2225 \n","Epoch 37: val_loss improved from 0.79939 to 0.60973, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8146 - mse: 0.2244 - val_loss: 0.6097 - val_mse: 0.1041\n","Epoch 38/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7494 - mse: 0.2350 \n","Epoch 38: val_loss did not improve from 0.60973\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7909 - mse: 0.2280 - val_loss: 0.7813 - val_mse: 0.0801\n","Epoch 39/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9743 - mse: 0.2153 \n","Epoch 39: val_loss did not improve from 0.60973\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9656 - mse: 0.2172 - val_loss: 0.7346 - val_mse: 0.0847\n","Epoch 40/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7940 - mse: 0.1677 \n","Epoch 40: val_loss improved from 0.60973 to 0.55369, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7875 - mse: 0.1705 - val_loss: 0.5537 - val_mse: 0.0623\n","Epoch 41/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6708 - mse: 0.1432 \n","Epoch 41: val_loss did not improve from 0.55369\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6778 - mse: 0.1514 - val_loss: 0.5931 - val_mse: 0.0685\n","Epoch 42/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7749 - mse: 0.1591 \n","Epoch 42: val_loss did not improve from 0.55369\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7747 - mse: 0.1574 - val_loss: 0.8004 - val_mse: 0.0611\n","Epoch 43/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8219 - mse: 0.1574 \n","Epoch 43: val_loss did not improve from 0.55369\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8088 - mse: 0.1552 - val_loss: 0.8109 - val_mse: 0.0591\n","Epoch 44/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8137 - mse: 0.1331 \n","Epoch 44: val_loss did not improve from 0.55369\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7947 - mse: 0.1365 - val_loss: 0.7118 - val_mse: 0.0842\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6591 - mse: 0.1283 \n","Epoch 45: val_loss did not improve from 0.55369\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6436 - mse: 0.1285 - val_loss: 0.6682 - val_mse: 0.0620\n","Epoch 46/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6433 - mse: 0.1342 \n","Epoch 46: val_loss did not improve from 0.55369\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6170 - mse: 0.1330 - val_loss: 0.6785 - val_mse: 0.1303\n","Epoch 47/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8422 - mse: 0.1472 \n","Epoch 47: val_loss did not improve from 0.55369\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8668 - mse: 0.1461 - val_loss: 0.5587 - val_mse: 0.0611\n","Epoch 48/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5657 - mse: 0.1251 \n","Epoch 48: val_loss did not improve from 0.55369\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5466 - mse: 0.1212 - val_loss: 0.6714 - val_mse: 0.0804\n","Epoch 49/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6869 - mse: 0.1254 \n","Epoch 49: val_loss improved from 0.55369 to 0.53981, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6654 - mse: 0.1215 - val_loss: 0.5398 - val_mse: 0.0372\n","Epoch 50/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5632 - mse: 0.0905 \n","Epoch 50: val_loss improved from 0.53981 to 0.46420, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5446 - mse: 0.0901 - val_loss: 0.4642 - val_mse: 0.0469\n","Epoch 51/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4810 - mse: 0.0781 \n","Epoch 51: val_loss did not improve from 0.46420\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4866 - mse: 0.0805 - val_loss: 0.6626 - val_mse: 0.0640\n","Epoch 52/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6384 - mse: 0.1042 \n","Epoch 52: val_loss did not improve from 0.46420\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6296 - mse: 0.1041 - val_loss: 0.5896 - val_mse: 0.0690\n","Epoch 53/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5842 - mse: 0.0819 \n","Epoch 53: val_loss improved from 0.46420 to 0.38493, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5582 - mse: 0.0825 - val_loss: 0.3849 - val_mse: 0.0511\n","Epoch 54/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4643 - mse: 0.0820 \n","Epoch 54: val_loss did not improve from 0.38493\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4654 - mse: 0.0821 - val_loss: 0.3946 - val_mse: 0.0799\n","Epoch 55/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4595 - mse: 0.0864 \n","Epoch 55: val_loss did not improve from 0.38493\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4728 - mse: 0.0855 - val_loss: 0.4963 - val_mse: 0.0614\n","Epoch 56/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4860 - mse: 0.0719 \n","Epoch 56: val_loss did not improve from 0.38493\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4791 - mse: 0.0740 - val_loss: 0.4489 - val_mse: 0.0441\n","Epoch 57/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5575 - mse: 0.0698 \n","Epoch 57: val_loss improved from 0.38493 to 0.36544, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5480 - mse: 0.0705 - val_loss: 0.3654 - val_mse: 0.0433\n","Epoch 58/100\n","\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4029 - mse: 0.0670 \n","Epoch 58: val_loss did not improve from 0.36544\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4092 - mse: 0.0702 - val_loss: 0.4314 - val_mse: 0.0528\n","Epoch 59/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4371 - mse: 0.0801  \n","Epoch 59: val_loss did not improve from 0.36544\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4338 - mse: 0.0778 - val_loss: 0.3894 - val_mse: 0.0496\n","Epoch 60/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4376 - mse: 0.0675 \n","Epoch 60: val_loss did not improve from 0.36544\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4404 - mse: 0.0689 - val_loss: 0.5076 - val_mse: 0.0529\n","Epoch 61/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5894 - mse: 0.0755 \n","Epoch 61: val_loss did not improve from 0.36544\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5662 - mse: 0.0730 - val_loss: 0.4505 - val_mse: 0.0846\n","Epoch 62/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4381 - mse: 0.0675 \n","Epoch 62: val_loss improved from 0.36544 to 0.34050, saving model to best_model_Adam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4461 - mse: 0.0674 - val_loss: 0.3405 - val_mse: 0.0695\n","Epoch 63/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3358 - mse: 0.0672 \n","Epoch 63: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3432 - mse: 0.0675 - val_loss: 0.5463 - val_mse: 0.1022\n","Epoch 64/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4676 - mse: 0.0677 \n","Epoch 64: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4627 - mse: 0.0672 - val_loss: 0.5744 - val_mse: 0.1215\n","Epoch 65/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5239 - mse: 0.0858 \n","Epoch 65: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5187 - mse: 0.0839 - val_loss: 0.4753 - val_mse: 0.0552\n","Epoch 66/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5025 - mse: 0.0748 \n","Epoch 66: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5124 - mse: 0.0740 - val_loss: 0.5843 - val_mse: 0.0476\n","Epoch 67/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5609 - mse: 0.0747 \n","Epoch 67: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5528 - mse: 0.0737 - val_loss: 0.6480 - val_mse: 0.0632\n","Epoch 68/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5465 - mse: 0.0685 \n","Epoch 68: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5321 - mse: 0.0673 - val_loss: 0.5017 - val_mse: 0.0673\n","Epoch 69/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4262 - mse: 0.0674 \n","Epoch 69: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4249 - mse: 0.0670 - val_loss: 0.3937 - val_mse: 0.0452\n","Epoch 70/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4367 - mse: 0.0668 \n","Epoch 70: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4448 - mse: 0.0662 - val_loss: 0.5117 - val_mse: 0.0501\n","Epoch 71/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4513 - mse: 0.0587 \n","Epoch 71: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4459 - mse: 0.0580 - val_loss: 0.5446 - val_mse: 0.0517\n","Epoch 72/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5016 - mse: 0.0620 \n","Epoch 72: val_loss did not improve from 0.34050\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4890 - mse: 0.0612 - val_loss: 0.4574 - val_mse: 0.0378\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: Adam, Learning Rate: 0.1, Epochs: 100, Test MSE: 0.07827336324104264\n","Training with optimizer: SGD, learning rate: 0.001, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 143.3006 - mse: 137.7065\n","Epoch 1: val_loss improved from inf to 111.16514, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 311ms/step - loss: 142.9938 - mse: 137.3997 - val_loss: 111.1651 - val_mse: 105.5674\n","Epoch 2/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105.6058 - mse: 100.0072 \n","Epoch 2: val_loss improved from 111.16514 to 69.08395, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 104.1524 - mse: 98.5536 - val_loss: 69.0839 - val_mse: 63.4816\n","Epoch 3/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 63.6554 - mse: 58.0522 \n","Epoch 3: val_loss improved from 69.08395 to 31.82775, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 62.5318 - mse: 56.9286 - val_loss: 31.8277 - val_mse: 26.2207\n","Epoch 4/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33.3501 - mse: 27.7426 \n","Epoch 4: val_loss improved from 31.82775 to 12.30051, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32.5684 - mse: 26.9607 - val_loss: 12.3005 - val_mse: 6.6904\n","Epoch 5/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20.3953 - mse: 14.7850 \n","Epoch 5: val_loss improved from 12.30051 to 7.42564, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.1695 - mse: 14.5592 - val_loss: 7.4256 - val_mse: 1.8145\n","Epoch 6/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.9126 - mse: 11.3002 \n","Epoch 6: val_loss improved from 7.42564 to 6.36324, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.6636 - mse: 11.0511 - val_loss: 6.3632 - val_mse: 0.7501\n","Epoch 7/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.8900 - mse: 8.2771 \n","Epoch 7: val_loss improved from 6.36324 to 6.29031, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.9208 - mse: 8.3080 - val_loss: 6.2903 - val_mse: 0.6781\n","Epoch 8/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.1132 - mse: 8.5013  \n","Epoch 8: val_loss improved from 6.29031 to 6.28352, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.9958 - mse: 8.3840 - val_loss: 6.2835 - val_mse: 0.6731\n","Epoch 9/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.9055 - mse: 7.2955 \n","Epoch 9: val_loss improved from 6.28352 to 6.21819, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.8840 - mse: 7.2742 - val_loss: 6.2182 - val_mse: 0.6100\n","Epoch 10/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.6242 - mse: 7.0166 \n","Epoch 10: val_loss improved from 6.21819 to 6.14934, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.6086 - mse: 7.0012 - val_loss: 6.1493 - val_mse: 0.5439\n","Epoch 11/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.2280 - mse: 7.6232  \n","Epoch 11: val_loss improved from 6.14934 to 6.09237, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.1142 - mse: 7.5095 - val_loss: 6.0924 - val_mse: 0.4902\n","Epoch 12/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.7135 - mse: 6.1121 \n","Epoch 12: val_loss did not improve from 6.09237\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.7166 - mse: 6.1153 - val_loss: 6.0974 - val_mse: 0.4989\n","Epoch 13/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.6885 - mse: 6.0908 \n","Epoch 13: val_loss improved from 6.09237 to 6.05151, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.6791 - mse: 6.0816 - val_loss: 6.0515 - val_mse: 0.4570\n","Epoch 14/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.0056 - mse: 5.4119 \n","Epoch 14: val_loss improved from 6.05151 to 5.99642, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.1022 - mse: 5.5087 - val_loss: 5.9964 - val_mse: 0.4058\n","Epoch 15/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6570 - mse: 5.0672 \n","Epoch 15: val_loss improved from 5.99642 to 5.96017, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6371 - mse: 5.0475 - val_loss: 5.9602 - val_mse: 0.3739\n","Epoch 16/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.7597 - mse: 5.1744 \n","Epoch 16: val_loss improved from 5.96017 to 5.94473, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.7274 - mse: 5.1422 - val_loss: 5.9447 - val_mse: 0.3629\n","Epoch 17/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.0124 - mse: 5.4315\n","Epoch 17: val_loss improved from 5.94473 to 5.92179, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.9787 - mse: 5.3980 - val_loss: 5.9218 - val_mse: 0.3445\n","Epoch 18/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.2860 - mse: 5.7096 \n","Epoch 18: val_loss improved from 5.92179 to 5.91649, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.2085 - mse: 5.6322 - val_loss: 5.9165 - val_mse: 0.3437\n","Epoch 19/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.2962 - mse: 4.7243\n","Epoch 19: val_loss improved from 5.91649 to 5.86978, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.3126 - mse: 4.7410 - val_loss: 5.8698 - val_mse: 0.3016\n","Epoch 20/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.3672 - mse: 4.8001 \n","Epoch 20: val_loss improved from 5.86978 to 5.85324, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.3475 - mse: 4.7805 - val_loss: 5.8532 - val_mse: 0.2899\n","Epoch 21/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6977 - mse: 5.1353 \n","Epoch 21: val_loss did not improve from 5.85324\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.6028 - mse: 5.0406 - val_loss: 5.8533 - val_mse: 0.2947\n","Epoch 22/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.0029 - mse: 4.4453\n","Epoch 22: val_loss improved from 5.85324 to 5.83056, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.0245 - mse: 4.4671 - val_loss: 5.8306 - val_mse: 0.2767\n","Epoch 23/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1576 - mse: 4.6049 \n","Epoch 23: val_loss improved from 5.83056 to 5.80456, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.1388 - mse: 4.5861 - val_loss: 5.8046 - val_mse: 0.2556\n","Epoch 24/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5206 - mse: 3.9726 \n","Epoch 24: val_loss did not improve from 5.80456\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.5683 - mse: 4.0206 - val_loss: 5.8342 - val_mse: 0.2902\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8616 - mse: 4.3185 \n","Epoch 25: val_loss did not improve from 5.80456\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.8653 - mse: 4.3225 - val_loss: 5.8113 - val_mse: 0.2722\n","Epoch 26/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4830 - mse: 3.9450  \n","Epoch 26: val_loss did not improve from 5.80456\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.5005 - mse: 3.9627 - val_loss: 5.8390 - val_mse: 0.3049\n","Epoch 27/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9225 - mse: 4.3895 \n","Epoch 27: val_loss did not improve from 5.80456\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.8825 - mse: 4.3497 - val_loss: 5.8226 - val_mse: 0.2936\n","Epoch 28/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8872 - mse: 4.3593  \n","Epoch 28: val_loss improved from 5.80456 to 5.77126, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.8430 - mse: 4.3153 - val_loss: 5.7713 - val_mse: 0.2474\n","Epoch 29/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5982 - mse: 4.0753  \n","Epoch 29: val_loss improved from 5.77126 to 5.77050, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5714 - mse: 4.0488 - val_loss: 5.7705 - val_mse: 0.2517\n","Epoch 30/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5174 - mse: 3.9997  \n","Epoch 30: val_loss improved from 5.77050 to 5.74890, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.4737 - mse: 3.9562 - val_loss: 5.7489 - val_mse: 0.2353\n","Epoch 31/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7074 - mse: 4.1948  \n","Epoch 31: val_loss did not improve from 5.74890\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.6594 - mse: 4.1470 - val_loss: 5.7786 - val_mse: 0.2701\n","Epoch 32/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2939 - mse: 3.7865  \n","Epoch 32: val_loss did not improve from 5.74890\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.2500 - mse: 3.7428 - val_loss: 5.7560 - val_mse: 0.2527\n","Epoch 33/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0920 - mse: 3.5899 \n","Epoch 33: val_loss improved from 5.74890 to 5.71545, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.0672 - mse: 3.5652 - val_loss: 5.7155 - val_mse: 0.2174\n","Epoch 34/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9285 - mse: 3.4317 \n","Epoch 34: val_loss improved from 5.71545 to 5.70832, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9696 - mse: 3.4729 - val_loss: 5.7083 - val_mse: 0.2155\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9835 - mse: 3.4917 \n","Epoch 35: val_loss improved from 5.70832 to 5.68512, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9781 - mse: 3.4867 - val_loss: 5.6851 - val_mse: 0.1976\n","Epoch 36/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9151 - mse: 3.4286 \n","Epoch 36: val_loss did not improve from 5.68512\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.9105 - mse: 3.4243 - val_loss: 5.6873 - val_mse: 0.2051\n","Epoch 37/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1269 - mse: 3.6457 \n","Epoch 37: val_loss did not improve from 5.68512\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.1091 - mse: 3.6282 - val_loss: 5.7015 - val_mse: 0.2245\n","Epoch 38/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5101 - mse: 3.0342 \n","Epoch 38: val_loss improved from 5.68512 to 5.67471, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5432 - mse: 3.0675 - val_loss: 5.6747 - val_mse: 0.2031\n","Epoch 39/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0991 - mse: 3.6285 \n","Epoch 39: val_loss improved from 5.67471 to 5.66564, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.1291 - mse: 3.6588 - val_loss: 5.6656 - val_mse: 0.1993\n","Epoch 40/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7903 - mse: 3.3251 \n","Epoch 40: val_loss improved from 5.66564 to 5.64930, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.7737 - mse: 3.3087 - val_loss: 5.6493 - val_mse: 0.1883\n","Epoch 41/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.4068 - mse: 2.9470 \n","Epoch 41: val_loss improved from 5.64930 to 5.63577, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.4115 - mse: 2.9518 - val_loss: 5.6358 - val_mse: 0.1801\n","Epoch 42/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6237 - mse: 3.1691 \n","Epoch 42: val_loss did not improve from 5.63577\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6090 - mse: 3.1546 - val_loss: 5.6516 - val_mse: 0.2012\n","Epoch 43/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5836 - mse: 3.1344 \n","Epoch 43: val_loss did not improve from 5.63577\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.5853 - mse: 3.1362 - val_loss: 5.6469 - val_mse: 0.2019\n","Epoch 44/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6263 - mse: 3.1825 \n","Epoch 44: val_loss did not improve from 5.63577\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6160 - mse: 3.1723 - val_loss: 5.6544 - val_mse: 0.2148\n","Epoch 45/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.4277 - mse: 2.9891 \n","Epoch 45: val_loss did not improve from 5.63577\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.4319 - mse: 2.9936 - val_loss: 5.6523 - val_mse: 0.2180\n","Epoch 46/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5577 - mse: 3.1245 \n","Epoch 46: val_loss did not improve from 5.63577\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.5308 - mse: 3.0979 - val_loss: 5.6390 - val_mse: 0.2101\n","Epoch 47/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6240 - mse: 3.1962 \n","Epoch 47: val_loss improved from 5.63577 to 5.62921, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6416 - mse: 3.2140 - val_loss: 5.6292 - val_mse: 0.2057\n","Epoch 48/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5114 - mse: 3.0889 \n","Epoch 48: val_loss improved from 5.62921 to 5.60581, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.4853 - mse: 3.0630 - val_loss: 5.6058 - val_mse: 0.1876\n","Epoch 49/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3080 - mse: 2.8910 \n","Epoch 49: val_loss did not improve from 5.60581\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.3582 - mse: 2.9413 - val_loss: 5.6087 - val_mse: 0.1959\n","Epoch 50/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1729 - mse: 2.7612 \n","Epoch 50: val_loss improved from 5.60581 to 5.58342, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1607 - mse: 2.7493 - val_loss: 5.5834 - val_mse: 0.1760\n","Epoch 51/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1395 - mse: 2.7331 \n","Epoch 51: val_loss improved from 5.58342 to 5.55048, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1492 - mse: 2.7431 - val_loss: 5.5505 - val_mse: 0.1485\n","Epoch 52/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9906 - mse: 2.5898 \n","Epoch 52: val_loss did not improve from 5.55048\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.0069 - mse: 2.6063 - val_loss: 5.5570 - val_mse: 0.1604\n","Epoch 53/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0564 - mse: 2.6610 \n","Epoch 53: val_loss did not improve from 5.55048\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.0494 - mse: 2.6541 - val_loss: 5.5707 - val_mse: 0.1794\n","Epoch 54/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3793 - mse: 2.9892 \n","Epoch 54: val_loss improved from 5.55048 to 5.53342, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.3468 - mse: 2.9569 - val_loss: 5.5334 - val_mse: 0.1476\n","Epoch 55/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1531 - mse: 2.7684 \n","Epoch 55: val_loss improved from 5.53342 to 5.53169, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1458 - mse: 2.7613 - val_loss: 5.5317 - val_mse: 0.1513\n","Epoch 56/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9664 - mse: 2.5871 \n","Epoch 56: val_loss improved from 5.53169 to 5.50977, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9826 - mse: 2.6035 - val_loss: 5.5098 - val_mse: 0.1347\n","Epoch 57/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2009 - mse: 2.8269 \n","Epoch 57: val_loss did not improve from 5.50977\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1791 - mse: 2.8054 - val_loss: 5.5268 - val_mse: 0.1571\n","Epoch 58/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9097 - mse: 2.5413 \n","Epoch 58: val_loss improved from 5.50977 to 5.50441, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9079 - mse: 2.5396 - val_loss: 5.5044 - val_mse: 0.1401\n","Epoch 59/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2063 - mse: 2.8432 \n","Epoch 59: val_loss did not improve from 5.50441\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1739 - mse: 2.8110 - val_loss: 5.5087 - val_mse: 0.1498\n","Epoch 60/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7679 - mse: 2.4102 \n","Epoch 60: val_loss did not improve from 5.50441\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.7952 - mse: 2.4377 - val_loss: 5.5052 - val_mse: 0.1517\n","Epoch 61/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8374 - mse: 2.4850 \n","Epoch 61: val_loss improved from 5.50441 to 5.48577, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.8521 - mse: 2.5000 - val_loss: 5.4858 - val_mse: 0.1377\n","Epoch 62/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0758 - mse: 2.7288 \n","Epoch 62: val_loss improved from 5.48577 to 5.48322, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.0610 - mse: 2.7142 - val_loss: 5.4832 - val_mse: 0.1405\n","Epoch 63/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7894 - mse: 2.4478 \n","Epoch 63: val_loss did not improve from 5.48322\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.7890 - mse: 2.4476 - val_loss: 5.4896 - val_mse: 0.1523\n","Epoch 64/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9088 - mse: 2.5726 \n","Epoch 64: val_loss improved from 5.48322 to 5.47236, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8987 - mse: 2.5627 - val_loss: 5.4724 - val_mse: 0.1404\n","Epoch 65/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9749 - mse: 2.6441 \n","Epoch 65: val_loss improved from 5.47236 to 5.46111, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9592 - mse: 2.6286 - val_loss: 5.4611 - val_mse: 0.1345\n","Epoch 66/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9796 - mse: 2.6542 \n","Epoch 66: val_loss improved from 5.46111 to 5.44125, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9589 - mse: 2.6337 - val_loss: 5.4413 - val_mse: 0.1201\n","Epoch 67/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8542 - mse: 2.5341 \n","Epoch 67: val_loss did not improve from 5.44125\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.8726 - mse: 2.5528 - val_loss: 5.4631 - val_mse: 0.1473\n","Epoch 68/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8859 - mse: 2.5712 \n","Epoch 68: val_loss did not improve from 5.44125\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8533 - mse: 2.5388 - val_loss: 5.4459 - val_mse: 0.1354\n","Epoch 69/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0336 - mse: 2.7243 \n","Epoch 69: val_loss did not improve from 5.44125\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.0079 - mse: 2.6988 - val_loss: 5.4490 - val_mse: 0.1439\n","Epoch 70/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6444 - mse: 2.3404 \n","Epoch 70: val_loss improved from 5.44125 to 5.43032, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6619 - mse: 2.3582 - val_loss: 5.4303 - val_mse: 0.1306\n","Epoch 71/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6644 - mse: 2.3658  \n","Epoch 71: val_loss improved from 5.43032 to 5.40722, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6548 - mse: 2.3564 - val_loss: 5.4072 - val_mse: 0.1129\n","Epoch 72/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8101 - mse: 2.5169 \n","Epoch 72: val_loss improved from 5.40722 to 5.40389, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7946 - mse: 2.5016 - val_loss: 5.4039 - val_mse: 0.1149\n","Epoch 73/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9435 - mse: 2.6557 \n","Epoch 73: val_loss did not improve from 5.40389\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.9309 - mse: 2.6433 - val_loss: 5.4215 - val_mse: 0.1379\n","Epoch 74/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1498 - mse: 2.8673 \n","Epoch 74: val_loss did not improve from 5.40389\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1086 - mse: 2.8263 - val_loss: 5.4148 - val_mse: 0.1366\n","Epoch 75/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8782 - mse: 2.6011 \n","Epoch 75: val_loss improved from 5.40389 to 5.38953, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8792 - mse: 2.6023 - val_loss: 5.3895 - val_mse: 0.1167\n","Epoch 76/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3787 - mse: 2.1069 \n","Epoch 76: val_loss did not improve from 5.38953\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.4074 - mse: 2.1359 - val_loss: 5.4153 - val_mse: 0.1478\n","Epoch 77/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5624 - mse: 2.2959 \n","Epoch 77: val_loss improved from 5.38953 to 5.38323, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5636 - mse: 2.2974 - val_loss: 5.3832 - val_mse: 0.1211\n","Epoch 78/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2616 - mse: 2.0006 \n","Epoch 78: val_loss improved from 5.38323 to 5.38246, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2962 - mse: 2.0354 - val_loss: 5.3825 - val_mse: 0.1257\n","Epoch 79/100\n","\u001b[1m14/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.6976 - mse: 2.4415 \n","Epoch 79: val_loss improved from 5.38246 to 5.36882, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.6749 - mse: 2.4194 - val_loss: 5.3688 - val_mse: 0.1174\n","Epoch 80/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2296 - mse: 1.9791 \n","Epoch 80: val_loss improved from 5.36882 to 5.36175, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2731 - mse: 2.0230 - val_loss: 5.3617 - val_mse: 0.1156\n","Epoch 81/100\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.3519 - mse: 2.1065 \n","Epoch 81: val_loss improved from 5.36175 to 5.35137, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3653 - mse: 2.1205 - val_loss: 5.3514 - val_mse: 0.1106\n","Epoch 82/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4756 - mse: 2.2358 \n","Epoch 82: val_loss improved from 5.35137 to 5.33647, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4639 - mse: 2.2245 - val_loss: 5.3365 - val_mse: 0.1011\n","Epoch 83/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5786 - mse: 2.3442 \n","Epoch 83: val_loss did not improve from 5.33647\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.5598 - mse: 2.3257 - val_loss: 5.3407 - val_mse: 0.1106\n","Epoch 84/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8031 - mse: 2.5740 \n","Epoch 84: val_loss improved from 5.33647 to 5.33281, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7758 - mse: 2.5471 - val_loss: 5.3328 - val_mse: 0.1080\n","Epoch 85/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1860 - mse: 1.9621 \n","Epoch 85: val_loss did not improve from 5.33281\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1651 - mse: 1.9417 - val_loss: 5.3431 - val_mse: 0.1237\n","Epoch 86/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2734 - mse: 2.0550 \n","Epoch 86: val_loss improved from 5.33281 to 5.33019, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3481 - mse: 2.1300 - val_loss: 5.3302 - val_mse: 0.1161\n","Epoch 87/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2170 - mse: 2.0039 \n","Epoch 87: val_loss improved from 5.33019 to 5.32233, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2365 - mse: 2.0237 - val_loss: 5.3223 - val_mse: 0.1136\n","Epoch 88/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5480 - mse: 2.3403 \n","Epoch 88: val_loss improved from 5.32233 to 5.30522, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5379 - mse: 2.3305 - val_loss: 5.3052 - val_mse: 0.1017\n","Epoch 89/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6321 - mse: 2.4298 \n","Epoch 89: val_loss improved from 5.30522 to 5.29402, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.6130 - mse: 2.4108 - val_loss: 5.2940 - val_mse: 0.0959\n","Epoch 90/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1074 - mse: 1.9103 \n","Epoch 90: val_loss improved from 5.29402 to 5.28383, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1355 - mse: 1.9387 - val_loss: 5.2838 - val_mse: 0.0910\n","Epoch 91/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5421 - mse: 2.3503 \n","Epoch 91: val_loss did not improve from 5.28383\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.5079 - mse: 2.3164 - val_loss: 5.2885 - val_mse: 0.1010\n","Epoch 92/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3049 - mse: 2.1185 \n","Epoch 92: val_loss improved from 5.28383 to 5.27555, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3143 - mse: 2.1280 - val_loss: 5.2756 - val_mse: 0.0933\n","Epoch 93/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1884 - mse: 2.0072 \n","Epoch 93: val_loss improved from 5.27555 to 5.27264, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2074 - mse: 2.0265 - val_loss: 5.2726 - val_mse: 0.0957\n","Epoch 94/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0253 - mse: 1.8494 \n","Epoch 94: val_loss did not improve from 5.27264\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.0695 - mse: 1.8938 - val_loss: 5.2855 - val_mse: 0.1138\n","Epoch 95/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1996 - mse: 2.0290 \n","Epoch 95: val_loss did not improve from 5.27264\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.2055 - mse: 2.0352 - val_loss: 5.2897 - val_mse: 0.1234\n","Epoch 96/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1016 - mse: 1.9363 \n","Epoch 96: val_loss did not improve from 5.27264\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.0890 - mse: 1.9240 - val_loss: 5.2766 - val_mse: 0.1155\n","Epoch 97/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0640 - mse: 1.9041 \n","Epoch 97: val_loss improved from 5.27264 to 5.25691, saving model to best_model_SGD_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0627 - mse: 1.9030 - val_loss: 5.2569 - val_mse: 0.1011\n","Epoch 98/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2690 - mse: 2.1144 \n","Epoch 98: val_loss did not improve from 5.25691\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.2737 - mse: 2.1193 - val_loss: 5.2591 - val_mse: 0.1086\n","Epoch 99/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9887 - mse: 1.8393 \n","Epoch 99: val_loss did not improve from 5.25691\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.9955 - mse: 1.8463 - val_loss: 5.2676 - val_mse: 0.1224\n","Epoch 100/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1350 - mse: 1.9910 \n","Epoch 100: val_loss did not improve from 5.25691\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1313 - mse: 1.9874 - val_loss: 5.2771 - val_mse: 0.1372\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: SGD, Learning Rate: 0.001, Epochs: 100, Test MSE: 0.1053116068709575\n","Training with optimizer: SGD, learning rate: 0.01, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - loss: 83.6799 - mse: 77.9403\n","Epoch 1: val_loss improved from inf to 6.51946, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 324ms/step - loss: 82.2457 - mse: 76.5026 - val_loss: 6.5195 - val_mse: 0.5442\n","Epoch 2/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2296 - mse: 6.2472 \n","Epoch 2: val_loss did not improve from 6.51946\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.0976 - mse: 6.1149 - val_loss: 6.5987 - val_mse: 0.6186\n","Epoch 3/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.5463 - mse: 4.5714 \n","Epoch 3: val_loss improved from 6.51946 to 6.33304, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.5147 - mse: 4.5409 - val_loss: 6.3330 - val_mse: 0.3808\n","Epoch 4/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3298 - mse: 3.3871 \n","Epoch 4: val_loss improved from 6.33304 to 6.25130, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.3320 - mse: 3.3909 - val_loss: 6.2513 - val_mse: 0.3439\n","Epoch 5/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0426 - mse: 3.1453 \n","Epoch 5: val_loss improved from 6.25130 to 6.05004, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.0154 - mse: 3.1204 - val_loss: 6.0500 - val_mse: 0.1918\n","Epoch 6/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7311 - mse: 2.8823 \n","Epoch 6: val_loss improved from 6.05004 to 5.94264, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6948 - mse: 2.8494 - val_loss: 5.9426 - val_mse: 0.1358\n","Epoch 7/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1729 - mse: 2.3765 \n","Epoch 7: val_loss improved from 5.94264 to 5.92912, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1751 - mse: 2.3817 - val_loss: 5.9291 - val_mse: 0.1766\n","Epoch 8/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1242 - mse: 2.3833 \n","Epoch 8: val_loss improved from 5.92912 to 5.78701, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1103 - mse: 2.3715 - val_loss: 5.7870 - val_mse: 0.0893\n","Epoch 9/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9437 - mse: 2.2577 \n","Epoch 9: val_loss did not improve from 5.78701\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.9360 - mse: 2.2520 - val_loss: 5.9239 - val_mse: 0.2812\n","Epoch 10/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7923 - mse: 2.1613 \n","Epoch 10: val_loss improved from 5.78701 to 5.72042, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7784 - mse: 2.1494 - val_loss: 5.7204 - val_mse: 0.1328\n","Epoch 11/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7491 - mse: 2.1736 \n","Epoch 11: val_loss improved from 5.72042 to 5.69648, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7211 - mse: 2.1472 - val_loss: 5.6965 - val_mse: 0.1642\n","Epoch 12/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2244 - mse: 1.7038 \n","Epoch 12: val_loss improved from 5.69648 to 5.54613, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2391 - mse: 1.7205 - val_loss: 5.5461 - val_mse: 0.0684\n","Epoch 13/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1351 - mse: 1.6684 \n","Epoch 13: val_loss improved from 5.54613 to 5.49570, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1504 - mse: 1.6862 - val_loss: 5.4957 - val_mse: 0.0726\n","Epoch 14/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1763 - mse: 1.7642 \n","Epoch 14: val_loss improved from 5.49570 to 5.44729, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1614 - mse: 1.7519 - val_loss: 5.4473 - val_mse: 0.0782\n","Epoch 15/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9103 - mse: 1.5526 \n","Epoch 15: val_loss improved from 5.44729 to 5.37837, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9004 - mse: 1.5447 - val_loss: 5.3784 - val_mse: 0.0632\n","Epoch 16/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9875 - mse: 1.6832  \n","Epoch 16: val_loss improved from 5.37837 to 5.34839, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.9448 - mse: 1.6430 - val_loss: 5.3484 - val_mse: 0.0869\n","Epoch 17/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6602 - mse: 1.4096 \n","Epoch 17: val_loss did not improve from 5.34839\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.6798 - mse: 1.4316 - val_loss: 5.3533 - val_mse: 0.1449\n","Epoch 18/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9073 - mse: 1.7105 \n","Epoch 18: val_loss did not improve from 5.34839\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.8913 - mse: 1.6959 - val_loss: 5.4924 - val_mse: 0.3365\n","Epoch 19/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6234 - mse: 1.4791 \n","Epoch 19: val_loss improved from 5.34839 to 5.16953, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6220 - mse: 1.4792 - val_loss: 5.1695 - val_mse: 0.0658\n","Epoch 20/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6305 - mse: 1.5378 \n","Epoch 20: val_loss improved from 5.16953 to 5.11978, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6070 - mse: 1.5161 - val_loss: 5.1198 - val_mse: 0.0678\n","Epoch 21/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4727 - mse: 1.4316 \n","Epoch 21: val_loss improved from 5.11978 to 5.05601, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4695 - mse: 1.4303 - val_loss: 5.0560 - val_mse: 0.0553\n","Epoch 22/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2539 - mse: 1.2639 \n","Epoch 22: val_loss improved from 5.05601 to 5.01069, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2527 - mse: 1.2646 - val_loss: 5.0107 - val_mse: 0.0610\n","Epoch 23/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1694 - mse: 1.2309 \n","Epoch 23: val_loss improved from 5.01069 to 4.95388, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.1624 - mse: 1.2252 - val_loss: 4.9539 - val_mse: 0.0547\n","Epoch 24/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0919 - mse: 1.2029 \n","Epoch 24: val_loss improved from 4.95388 to 4.90356, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0910 - mse: 1.2043 - val_loss: 4.9036 - val_mse: 0.0544\n","Epoch 25/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0301 - mse: 1.1910 \n","Epoch 25: val_loss improved from 4.90356 to 4.86404, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0306 - mse: 1.1938 - val_loss: 4.8640 - val_mse: 0.0644\n","Epoch 26/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9065 - mse: 1.1169 \n","Epoch 26: val_loss improved from 4.86404 to 4.81196, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9112 - mse: 1.1239 - val_loss: 4.8120 - val_mse: 0.0615\n","Epoch 27/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9366 - mse: 1.1964 \n","Epoch 27: val_loss improved from 4.81196 to 4.76316, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9287 - mse: 1.1904 - val_loss: 4.7632 - val_mse: 0.0612\n","Epoch 28/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8176 - mse: 1.1264 \n","Epoch 28: val_loss improved from 4.76316 to 4.69793, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8136 - mse: 1.1237 - val_loss: 4.6979 - val_mse: 0.0441\n","Epoch 29/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8274 - mse: 1.1837 \n","Epoch 29: val_loss improved from 4.69793 to 4.65305, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8193 - mse: 1.1773 - val_loss: 4.6531 - val_mse: 0.0469\n","Epoch 30/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7044 - mse: 1.1082 \n","Epoch 30: val_loss improved from 4.65305 to 4.60946, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7016 - mse: 1.1072 - val_loss: 4.6095 - val_mse: 0.0505\n","Epoch 31/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5186 - mse: 0.9701 \n","Epoch 31: val_loss improved from 4.60946 to 4.57921, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5202 - mse: 0.9730 - val_loss: 4.5792 - val_mse: 0.0670\n","Epoch 32/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5206 - mse: 1.0187 \n","Epoch 32: val_loss improved from 4.57921 to 4.51596, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5158 - mse: 1.0152 - val_loss: 4.5160 - val_mse: 0.0500\n","Epoch 33/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4597 - mse: 1.0030 \n","Epoch 33: val_loss improved from 4.51596 to 4.51189, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4418 - mse: 0.9873 - val_loss: 4.5119 - val_mse: 0.0918\n","Epoch 34/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3850 - mse: 0.9741 \n","Epoch 34: val_loss improved from 4.51189 to 4.42063, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3824 - mse: 0.9737 - val_loss: 4.4206 - val_mse: 0.0459\n","Epoch 35/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3860 - mse: 1.0200 \n","Epoch 35: val_loss improved from 4.42063 to 4.39833, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3762 - mse: 1.0126 - val_loss: 4.3983 - val_mse: 0.0685\n","Epoch 36/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2538 - mse: 0.9334 \n","Epoch 36: val_loss improved from 4.39833 to 4.34829, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2531 - mse: 0.9344 - val_loss: 4.3483 - val_mse: 0.0629\n","Epoch 37/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1658 - mse: 0.8893 \n","Epoch 37: val_loss improved from 4.34829 to 4.31824, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1707 - mse: 0.8963 - val_loss: 4.3182 - val_mse: 0.0769\n","Epoch 38/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1040 - mse: 0.8719 \n","Epoch 38: val_loss improved from 4.31824 to 4.25031, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0973 - mse: 0.8668 - val_loss: 4.2503 - val_mse: 0.0525\n","Epoch 39/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0840 - mse: 0.8954 \n","Epoch 39: val_loss improved from 4.25031 to 4.21015, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0874 - mse: 0.9004 - val_loss: 4.2102 - val_mse: 0.0556\n","Epoch 40/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9123 - mse: 0.7667 \n","Epoch 40: val_loss improved from 4.21015 to 4.17041, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9170 - mse: 0.7731 - val_loss: 4.1704 - val_mse: 0.0586\n","Epoch 41/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8153 - mse: 0.7125 \n","Epoch 41: val_loss improved from 4.17041 to 4.11939, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8206 - mse: 0.7194 - val_loss: 4.1194 - val_mse: 0.0499\n","Epoch 42/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8550 - mse: 0.7948 \n","Epoch 42: val_loss improved from 4.11939 to 4.07742, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8521 - mse: 0.7931 - val_loss: 4.0774 - val_mse: 0.0498\n","Epoch 43/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8177 - mse: 0.7993 \n","Epoch 43: val_loss improved from 4.07742 to 4.03384, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8158 - mse: 0.7985 - val_loss: 4.0338 - val_mse: 0.0477\n","Epoch 44/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7053 - mse: 0.7283 \n","Epoch 44: val_loss improved from 4.03384 to 4.01111, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7057 - mse: 0.7298 - val_loss: 4.0111 - val_mse: 0.0660\n","Epoch 45/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6805 - mse: 0.7437 \n","Epoch 45: val_loss improved from 4.01111 to 3.94943, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6810 - mse: 0.7461 - val_loss: 3.9494 - val_mse: 0.0450\n","Epoch 46/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6293 - mse: 0.7334 \n","Epoch 46: val_loss improved from 3.94943 to 3.92836, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6223 - mse: 0.7280 - val_loss: 3.9284 - val_mse: 0.0642\n","Epoch 47/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6635 - mse: 0.8081 \n","Epoch 47: val_loss improved from 3.92836 to 3.92308, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6632 - mse: 0.8090 - val_loss: 3.9231 - val_mse: 0.0987\n","Epoch 48/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4274 - mse: 0.6114 \n","Epoch 48: val_loss improved from 3.92308 to 3.83796, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4284 - mse: 0.6138 - val_loss: 3.8380 - val_mse: 0.0530\n","Epoch 49/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4910 - mse: 0.7147 \n","Epoch 49: val_loss improved from 3.83796 to 3.81669, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4918 - mse: 0.7165 - val_loss: 3.8167 - val_mse: 0.0707\n","Epoch 50/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3956 - mse: 0.6578 \n","Epoch 50: val_loss improved from 3.81669 to 3.75447, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3869 - mse: 0.6505 - val_loss: 3.7545 - val_mse: 0.0471\n","Epoch 51/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3619 - mse: 0.6633 \n","Epoch 51: val_loss improved from 3.75447 to 3.72211, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3616 - mse: 0.6637 - val_loss: 3.7221 - val_mse: 0.0529\n","Epoch 52/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2560 - mse: 0.5949 \n","Epoch 52: val_loss improved from 3.72211 to 3.69056, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2554 - mse: 0.5957 - val_loss: 3.6906 - val_mse: 0.0592\n","Epoch 53/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1959 - mse: 0.5725 \n","Epoch 53: val_loss improved from 3.69056 to 3.63689, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2007 - mse: 0.5787 - val_loss: 3.6369 - val_mse: 0.0430\n","Epoch 54/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2919 - mse: 0.7058 \n","Epoch 54: val_loss improved from 3.63689 to 3.61330, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2841 - mse: 0.6995 - val_loss: 3.6133 - val_mse: 0.0564\n","Epoch 55/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1787 - mse: 0.6296 \n","Epoch 55: val_loss improved from 3.61330 to 3.57768, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1772 - mse: 0.6295 - val_loss: 3.5777 - val_mse: 0.0575\n","Epoch 56/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0579 - mse: 0.5451 \n","Epoch 56: val_loss improved from 3.57768 to 3.54372, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0634 - mse: 0.5523 - val_loss: 3.5437 - val_mse: 0.0599\n","Epoch 57/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1009 - mse: 0.6250 \n","Epoch 57: val_loss improved from 3.54372 to 3.49007, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0974 - mse: 0.6225 - val_loss: 3.4901 - val_mse: 0.0421\n","Epoch 58/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9717 - mse: 0.5317 \n","Epoch 58: val_loss improved from 3.49007 to 3.45952, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9739 - mse: 0.5348 - val_loss: 3.4595 - val_mse: 0.0471\n","Epoch 59/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0495 - mse: 0.6449 \n","Epoch 59: val_loss improved from 3.45952 to 3.42423, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0416 - mse: 0.6380 - val_loss: 3.4242 - val_mse: 0.0470\n","Epoch 60/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8496 - mse: 0.4801 \n","Epoch 60: val_loss improved from 3.42423 to 3.39067, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8540 - mse: 0.4855 - val_loss: 3.3907 - val_mse: 0.0483\n","Epoch 61/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8504 - mse: 0.5154 \n","Epoch 61: val_loss improved from 3.39067 to 3.34723, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8513 - mse: 0.5176 - val_loss: 3.3472 - val_mse: 0.0394\n","Epoch 62/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8710 - mse: 0.5700 \n","Epoch 62: val_loss improved from 3.34723 to 3.31273, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8628 - mse: 0.5634 - val_loss: 3.3127 - val_mse: 0.0390\n","Epoch 63/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7856 - mse: 0.5191 \n","Epoch 63: val_loss improved from 3.31273 to 3.31149, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7804 - mse: 0.5151 - val_loss: 3.3115 - val_mse: 0.0715\n","Epoch 64/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7349 - mse: 0.5017 \n","Epoch 64: val_loss improved from 3.31149 to 3.24364, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7387 - mse: 0.5071 - val_loss: 3.2436 - val_mse: 0.0371\n","Epoch 65/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7222 - mse: 0.5224 \n","Epoch 65: val_loss improved from 3.24364 to 3.22775, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7147 - mse: 0.5164 - val_loss: 3.2278 - val_mse: 0.0543\n","Epoch 66/100\n","\u001b[1m16/26\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7477 - mse: 0.5790  \n","Epoch 66: val_loss did not improve from 3.22775\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7241 - mse: 0.5589 - val_loss: 3.2466 - val_mse: 0.1059\n","Epoch 67/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6641 - mse: 0.5300  \n","Epoch 67: val_loss improved from 3.22775 to 3.15549, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6587 - mse: 0.5261 - val_loss: 3.1555 - val_mse: 0.0472\n","Epoch 68/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5638 - mse: 0.4621 \n","Epoch 68: val_loss improved from 3.15549 to 3.11758, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5603 - mse: 0.4600 - val_loss: 3.1176 - val_mse: 0.0414\n","Epoch 69/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5984 - mse: 0.5286 \n","Epoch 69: val_loss improved from 3.11758 to 3.08881, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5861 - mse: 0.5178 - val_loss: 3.0888 - val_mse: 0.0444\n","Epoch 70/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5226 - mse: 0.4843 \n","Epoch 70: val_loss improved from 3.08881 to 3.05724, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5331 - mse: 0.4965 - val_loss: 3.0572 - val_mse: 0.0442\n","Epoch 71/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4397 - mse: 0.4336 \n","Epoch 71: val_loss improved from 3.05724 to 3.04078, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4386 - mse: 0.4334 - val_loss: 3.0408 - val_mse: 0.0589\n","Epoch 72/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4512 - mse: 0.4752 \n","Epoch 72: val_loss improved from 3.04078 to 3.00093, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4494 - mse: 0.4751 - val_loss: 3.0009 - val_mse: 0.0498\n","Epoch 73/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4111 - mse: 0.4661 \n","Epoch 73: val_loss improved from 3.00093 to 2.95895, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4137 - mse: 0.4701 - val_loss: 2.9590 - val_mse: 0.0383\n","Epoch 74/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3690 - mse: 0.4550 \n","Epoch 74: val_loss improved from 2.95895 to 2.92854, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3666 - mse: 0.4534 - val_loss: 2.9285 - val_mse: 0.0380\n","Epoch 75/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3281 - mse: 0.4442 \n","Epoch 75: val_loss improved from 2.92854 to 2.89641, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3282 - mse: 0.4450 - val_loss: 2.8964 - val_mse: 0.0357\n","Epoch 76/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2570 - mse: 0.4032 \n","Epoch 76: val_loss improved from 2.89641 to 2.86991, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2578 - mse: 0.4044 - val_loss: 2.8699 - val_mse: 0.0387\n","Epoch 77/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2632 - mse: 0.4382 \n","Epoch 77: val_loss improved from 2.86991 to 2.85285, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2589 - mse: 0.4350 - val_loss: 2.8528 - val_mse: 0.0509\n","Epoch 78/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2084 - mse: 0.4123 \n","Epoch 78: val_loss improved from 2.85285 to 2.81179, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2086 - mse: 0.4138 - val_loss: 2.8118 - val_mse: 0.0388\n","Epoch 79/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1750 - mse: 0.4083 \n","Epoch 79: val_loss improved from 2.81179 to 2.77894, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1729 - mse: 0.4070 - val_loss: 2.7789 - val_mse: 0.0345\n","Epoch 80/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1040 - mse: 0.3662 \n","Epoch 80: val_loss improved from 2.77894 to 2.75165, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1042 - mse: 0.3669 - val_loss: 2.7517 - val_mse: 0.0356\n","Epoch 81/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0480 - mse: 0.3381 \n","Epoch 81: val_loss improved from 2.75165 to 2.72486, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0505 - mse: 0.3414 - val_loss: 2.7249 - val_mse: 0.0368\n","Epoch 82/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1076 - mse: 0.4254 \n","Epoch 82: val_loss improved from 2.72486 to 2.70617, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1053 - mse: 0.4242 - val_loss: 2.7062 - val_mse: 0.0459\n","Epoch 83/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0511 - mse: 0.3967 \n","Epoch 83: val_loss improved from 2.70617 to 2.67187, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0478 - mse: 0.3944 - val_loss: 2.6719 - val_mse: 0.0391\n","Epoch 84/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0108 - mse: 0.3835 \n","Epoch 84: val_loss improved from 2.67187 to 2.64178, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0120 - mse: 0.3860 - val_loss: 2.6418 - val_mse: 0.0361\n","Epoch 85/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9528 - mse: 0.3529 \n","Epoch 85: val_loss improved from 2.64178 to 2.61652, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9494 - mse: 0.3505 - val_loss: 2.6165 - val_mse: 0.0378\n","Epoch 86/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9243 - mse: 0.3515 \n","Epoch 86: val_loss improved from 2.61652 to 2.58599, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9271 - mse: 0.3550 - val_loss: 2.5860 - val_mse: 0.0339\n","Epoch 87/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8688 - mse: 0.3226 \n","Epoch 87: val_loss improved from 2.58599 to 2.56317, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8682 - mse: 0.3227 - val_loss: 2.5632 - val_mse: 0.0374\n","Epoch 88/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8439 - mse: 0.3237 \n","Epoch 88: val_loss improved from 2.56317 to 2.53482, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8434 - mse: 0.3242 - val_loss: 2.5348 - val_mse: 0.0352\n","Epoch 89/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8352 - mse: 0.3413 \n","Epoch 89: val_loss improved from 2.53482 to 2.50964, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8371 - mse: 0.3439 - val_loss: 2.5096 - val_mse: 0.0358\n","Epoch 90/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7480 - mse: 0.2796 \n","Epoch 90: val_loss did not improve from 2.50964\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7527 - mse: 0.2853 - val_loss: 2.5262 - val_mse: 0.0779\n","Epoch 91/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8254 - mse: 0.3825 \n","Epoch 91: val_loss improved from 2.50964 to 2.46246, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8172 - mse: 0.3752 - val_loss: 2.4625 - val_mse: 0.0394\n","Epoch 92/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7568 - mse: 0.3391 \n","Epoch 92: val_loss improved from 2.46246 to 2.43432, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7558 - mse: 0.3390 - val_loss: 2.4343 - val_mse: 0.0363\n","Epoch 93/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7207 - mse: 0.3275 \n","Epoch 93: val_loss improved from 2.43432 to 2.40822, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7200 - mse: 0.3282 - val_loss: 2.4082 - val_mse: 0.0350\n","Epoch 94/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6696 - mse: 0.3014 \n","Epoch 94: val_loss improved from 2.40822 to 2.38502, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6743 - mse: 0.3072 - val_loss: 2.3850 - val_mse: 0.0363\n","Epoch 95/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6474 - mse: 0.3039 \n","Epoch 95: val_loss improved from 2.38502 to 2.35681, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6460 - mse: 0.3033 - val_loss: 2.3568 - val_mse: 0.0324\n","Epoch 96/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5861 - mse: 0.2668 \n","Epoch 96: val_loss improved from 2.35681 to 2.33504, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5863 - mse: 0.2678 - val_loss: 2.3350 - val_mse: 0.0346\n","Epoch 97/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5808 - mse: 0.2858 \n","Epoch 97: val_loss improved from 2.33504 to 2.31147, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5803 - mse: 0.2858 - val_loss: 2.3115 - val_mse: 0.0348\n","Epoch 98/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5679 - mse: 0.2967 \n","Epoch 98: val_loss improved from 2.31147 to 2.29057, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5684 - mse: 0.2975 - val_loss: 2.2906 - val_mse: 0.0374\n","Epoch 99/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5215 - mse: 0.2732 \n","Epoch 99: val_loss improved from 2.29057 to 2.26669, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5205 - mse: 0.2731 - val_loss: 2.2667 - val_mse: 0.0368\n","Epoch 100/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5063 - mse: 0.2815 \n","Epoch 100: val_loss improved from 2.26669 to 2.25740, saving model to best_model_SGD_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5057 - mse: 0.2816 - val_loss: 2.2574 - val_mse: 0.0506\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n","Optimizer: SGD, Learning Rate: 0.01, Epochs: 100, Test MSE: 0.06418110654842957\n","Training with optimizer: SGD, learning rate: 0.1, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: nan - mse: nan        \n","Epoch 1: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 295ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 2/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 2: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 3/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 3: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 4/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 4: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 5/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 5: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 6/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 6: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 7/100\n","\u001b[1m25/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 7: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 8/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 8: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 9/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 9: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Epoch 10/100\n","\u001b[1m24/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan \n","Epoch 10: val_loss did not improve from inf\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n","Training with optimizer: SGD, learning rate: 0.1, epochs: 100 resulted in NaN values. Skipping...\n","Training with optimizer: RMSprop, learning rate: 0.001, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 150.7485 - mse: 145.1640\n","Epoch 1: val_loss improved from inf to 142.87920, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 326ms/step - loss: 150.7119 - mse: 145.1277 - val_loss: 142.8792 - val_mse: 137.3174\n","Epoch 2/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 145.4148 - mse: 139.8567 \n","Epoch 2: val_loss improved from 142.87920 to 137.55307, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 145.1432 - mse: 139.5860 - val_loss: 137.5531 - val_mse: 132.0095\n","Epoch 3/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.7395 - mse: 133.1991 \n","Epoch 3: val_loss improved from 137.55307 to 131.90288, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 138.7160 - mse: 133.1763 - val_loss: 131.9029 - val_mse: 126.3718\n","Epoch 4/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 134.4586 - mse: 128.9290 \n","Epoch 4: val_loss improved from 131.90288 to 126.26064, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 134.2324 - mse: 128.7030 - val_loss: 126.2606 - val_mse: 120.7366\n","Epoch 5/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 130.3735 - mse: 124.8504 \n","Epoch 5: val_loss improved from 126.26064 to 120.95538, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 129.8726 - mse: 124.3497 - val_loss: 120.9554 - val_mse: 115.4332\n","Epoch 6/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 122.8023 - mse: 117.2824 \n","Epoch 6: val_loss improved from 120.95538 to 114.99468, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 122.6778 - mse: 117.1585 - val_loss: 114.9947 - val_mse: 109.4807\n","Epoch 7/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112.9103 - mse: 107.3975 \n","Epoch 7: val_loss improved from 114.99468 to 107.25484, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 112.7152 - mse: 107.2027 - val_loss: 107.2548 - val_mse: 101.7483\n","Epoch 8/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.9350 - mse: 101.4297 \n","Epoch 8: val_loss improved from 107.25484 to 98.67563, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 106.6253 - mse: 101.1203 - val_loss: 98.6756 - val_mse: 93.1733\n","Epoch 9/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.6998 - mse: 97.1985  \n","Epoch 9: val_loss improved from 98.67563 to 89.85539, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 101.9776 - mse: 96.4766 - val_loss: 89.8554 - val_mse: 84.3593\n","Epoch 10/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 90.7460 - mse: 85.2499 \n","Epoch 10: val_loss improved from 89.85539 to 80.43270, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 90.2706 - mse: 84.7746 - val_loss: 80.4327 - val_mse: 74.9384\n","Epoch 11/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.3402 - mse: 77.8466 \n","Epoch 11: val_loss improved from 80.43270 to 71.45049, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 82.8148 - mse: 77.3213 - val_loss: 71.4505 - val_mse: 65.9581\n","Epoch 12/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.7647 - mse: 68.2728 \n","Epoch 12: val_loss improved from 71.45049 to 64.34290, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 73.3303 - mse: 67.8385 - val_loss: 64.3429 - val_mse: 58.8536\n","Epoch 13/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.1942 - mse: 61.7049 \n","Epoch 13: val_loss improved from 64.34290 to 55.41885, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 66.5691 - mse: 61.0799 - val_loss: 55.4188 - val_mse: 49.9301\n","Epoch 14/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.8601 - mse: 50.3729 \n","Epoch 14: val_loss improved from 55.41885 to 46.09888, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 55.5787 - mse: 50.0919 - val_loss: 46.0989 - val_mse: 40.6169\n","Epoch 15/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47.7898 - mse: 42.3096 \n","Epoch 15: val_loss improved from 46.09888 to 38.32965, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 47.4998 - mse: 42.0199 - val_loss: 38.3297 - val_mse: 32.8525\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.6395 - mse: 35.1628 \n","Epoch 16: val_loss improved from 38.32965 to 31.33006, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 40.2503 - mse: 34.7739 - val_loss: 31.3301 - val_mse: 25.8567\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34.5954 - mse: 29.1241 \n","Epoch 17: val_loss improved from 31.33006 to 23.98330, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 34.0658 - mse: 28.5952 - val_loss: 23.9833 - val_mse: 18.5203\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27.1060 - mse: 21.6451 \n","Epoch 18: val_loss improved from 23.98330 to 17.20762, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.8888 - mse: 21.4284 - val_loss: 17.2076 - val_mse: 11.7552\n","Epoch 19/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.4382 - mse: 15.9885 \n","Epoch 19: val_loss improved from 17.20762 to 11.80914, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.3119 - mse: 15.8626 - val_loss: 11.8091 - val_mse: 6.3665\n","Epoch 20/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.8403 - mse: 12.4008 \n","Epoch 20: val_loss improved from 11.80914 to 9.01512, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.6767 - mse: 12.2378 - val_loss: 9.0151 - val_mse: 3.5878\n","Epoch 21/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.6606 - mse: 11.2365 \n","Epoch 21: val_loss improved from 9.01512 to 7.33739, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.2658 - mse: 10.8426 - val_loss: 7.3374 - val_mse: 1.9280\n","Epoch 22/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.8777 - mse: 9.4712  \n","Epoch 22: val_loss improved from 7.33739 to 6.45492, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.7023 - mse: 9.2966 - val_loss: 6.4549 - val_mse: 1.0633\n","Epoch 23/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.5947 - mse: 7.2068 \n","Epoch 23: val_loss improved from 6.45492 to 5.81348, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.4729 - mse: 7.0859 - val_loss: 5.8135 - val_mse: 0.4443\n","Epoch 24/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.7934 - mse: 6.4294 \n","Epoch 24: val_loss did not improve from 5.81348\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.7882 - mse: 6.4251 - val_loss: 5.8923 - val_mse: 0.5484\n","Epoch 25/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2023 - mse: 6.8643 \n","Epoch 25: val_loss improved from 5.81348 to 5.80205, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.1466 - mse: 6.8100 - val_loss: 5.8021 - val_mse: 0.4863\n","Epoch 26/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.5592 - mse: 6.2493 \n","Epoch 26: val_loss improved from 5.80205 to 5.63835, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.5465 - mse: 6.2380 - val_loss: 5.6384 - val_mse: 0.3516\n","Epoch 27/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.4620 - mse: 6.1814 \n","Epoch 27: val_loss improved from 5.63835 to 5.61434, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.4692 - mse: 6.1905 - val_loss: 5.6143 - val_mse: 0.3575\n","Epoch 28/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6675 - mse: 5.4177\n","Epoch 28: val_loss improved from 5.61434 to 5.46675, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6675 - mse: 5.4204 - val_loss: 5.4667 - val_mse: 0.2495\n","Epoch 29/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.1002 - mse: 5.8900 \n","Epoch 29: val_loss did not improve from 5.46675\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.9703 - mse: 5.7623 - val_loss: 5.5630 - val_mse: 0.3867\n","Epoch 30/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1535 - mse: 4.9861 \n","Epoch 30: val_loss improved from 5.46675 to 5.41746, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.1758 - mse: 5.0104 - val_loss: 5.4175 - val_mse: 0.2837\n","Epoch 31/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7210 - mse: 4.5967  \n","Epoch 31: val_loss improved from 5.41746 to 5.30310, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.7125 - mse: 4.5905 - val_loss: 5.3031 - val_mse: 0.2162\n","Epoch 32/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6554 - mse: 4.5780 \n","Epoch 32: val_loss did not improve from 5.30310\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.7258 - mse: 4.6507 - val_loss: 5.3727 - val_mse: 0.3340\n","Epoch 33/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2821 - mse: 4.2541 \n","Epoch 33: val_loss improved from 5.30310 to 5.27636, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.3125 - mse: 4.2869 - val_loss: 5.2764 - val_mse: 0.2880\n","Epoch 34/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2477 - mse: 4.2689  \n","Epoch 34: val_loss did not improve from 5.27636\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.2724 - mse: 4.2964 - val_loss: 5.3681 - val_mse: 0.4285\n","Epoch 35/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3384 - mse: 4.4091  \n","Epoch 35: val_loss improved from 5.27636 to 5.08601, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.2958 - mse: 4.3690 - val_loss: 5.0860 - val_mse: 0.1991\n","Epoch 36/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.8751 - mse: 3.9998 \n","Epoch 36: val_loss improved from 5.08601 to 5.06004, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9747 - mse: 4.1026 - val_loss: 5.0600 - val_mse: 0.2314\n","Epoch 37/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1158 - mse: 4.2969  \n","Epoch 37: val_loss did not improve from 5.06004\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.0644 - mse: 4.2493 - val_loss: 5.0602 - val_mse: 0.2894\n","Epoch 38/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1875 - mse: 3.4303 \n","Epoch 38: val_loss improved from 5.06004 to 4.87917, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2343 - mse: 3.4811 - val_loss: 4.8792 - val_mse: 0.1778\n","Epoch 39/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9145 - mse: 4.2259 \n","Epoch 39: val_loss improved from 4.87917 to 4.75568, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9532 - mse: 4.2682 - val_loss: 4.7557 - val_mse: 0.1186\n","Epoch 40/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1712 - mse: 3.5487 \n","Epoch 40: val_loss did not improve from 4.75568\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1503 - mse: 3.5314 - val_loss: 4.7667 - val_mse: 0.2073\n","Epoch 41/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8981 - mse: 3.3556 \n","Epoch 41: val_loss improved from 4.75568 to 4.67162, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9270 - mse: 3.3874 - val_loss: 4.6716 - val_mse: 0.1893\n","Epoch 42/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3896 - mse: 3.9228 \n","Epoch 42: val_loss improved from 4.67162 to 4.53108, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2929 - mse: 3.8299 - val_loss: 4.5311 - val_mse: 0.1303\n","Epoch 43/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7755 - mse: 3.3917 \n","Epoch 43: val_loss improved from 4.53108 to 4.47162, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7659 - mse: 3.3861 - val_loss: 4.4716 - val_mse: 0.1570\n","Epoch 44/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5259 - mse: 3.2307 \n","Epoch 44: val_loss did not improve from 4.47162\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.5453 - mse: 3.2535 - val_loss: 4.5246 - val_mse: 0.2983\n","Epoch 45/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3327 - mse: 3.1256 \n","Epoch 45: val_loss improved from 4.47162 to 4.30505, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3397 - mse: 3.1360 - val_loss: 4.3050 - val_mse: 0.1680\n","Epoch 46/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4740 - mse: 3.3557 \n","Epoch 46: val_loss improved from 4.30505 to 4.20577, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4545 - mse: 3.3405 - val_loss: 4.2058 - val_mse: 0.1605\n","Epoch 47/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1603 - mse: 3.1345 \n","Epoch 47: val_loss improved from 4.20577 to 4.12446, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1902 - mse: 3.1688 - val_loss: 4.1245 - val_mse: 0.1734\n","Epoch 48/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7020 - mse: 2.7704 \n","Epoch 48: val_loss improved from 4.12446 to 3.92867, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6835 - mse: 2.7577 - val_loss: 3.9287 - val_mse: 0.0835\n","Epoch 49/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0174 - mse: 3.1919 \n","Epoch 49: val_loss improved from 3.92867 to 3.85125, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.0071 - mse: 3.1861 - val_loss: 3.8513 - val_mse: 0.1050\n","Epoch 50/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3260 - mse: 2.6013  \n","Epoch 50: val_loss improved from 3.85125 to 3.79212, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3262 - mse: 2.6064 - val_loss: 3.7921 - val_mse: 0.1511\n","Epoch 51/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5920 - mse: 2.9706 \n","Epoch 51: val_loss improved from 3.79212 to 3.62531, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5730 - mse: 2.9564 - val_loss: 3.6253 - val_mse: 0.0846\n","Epoch 52/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3005 - mse: 2.7818 \n","Epoch 52: val_loss improved from 3.62531 to 3.57615, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2774 - mse: 2.7627 - val_loss: 3.5762 - val_mse: 0.1445\n","Epoch 53/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1328 - mse: 2.7242 \n","Epoch 53: val_loss improved from 3.57615 to 3.41183, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1246 - mse: 2.7202 - val_loss: 3.4118 - val_mse: 0.0913\n","Epoch 54/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3886 - mse: 3.0903 \n","Epoch 54: val_loss improved from 3.41183 to 3.31755, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3780 - mse: 3.0836 - val_loss: 3.3175 - val_mse: 0.0988\n","Epoch 55/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9713 - mse: 2.7727 \n","Epoch 55: val_loss improved from 3.31755 to 3.23863, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9565 - mse: 2.7629 - val_loss: 3.2386 - val_mse: 0.1259\n","Epoch 56/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9879 - mse: 2.8933 \n","Epoch 56: val_loss improved from 3.23863 to 3.16756, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9799 - mse: 2.8920 - val_loss: 3.1676 - val_mse: 0.1555\n","Epoch 57/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7899 - mse: 2.7964 \n","Epoch 57: val_loss improved from 3.16756 to 2.98752, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7742 - mse: 2.7861 - val_loss: 2.9875 - val_mse: 0.0679\n","Epoch 58/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3594 - mse: 2.4600 \n","Epoch 58: val_loss improved from 2.98752 to 2.87286, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3779 - mse: 2.4831 - val_loss: 2.8729 - val_mse: 0.0515\n","Epoch 59/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6976 - mse: 2.8954 \n","Epoch 59: val_loss improved from 2.87286 to 2.78871, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6373 - mse: 2.8396 - val_loss: 2.7887 - val_mse: 0.0643\n","Epoch 60/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8659 - mse: 2.1600 \n","Epoch 60: val_loss improved from 2.78871 to 2.68424, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9083 - mse: 2.2079 - val_loss: 2.6842 - val_mse: 0.0556\n","Epoch 61/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9792 - mse: 2.3681 \n","Epoch 61: val_loss improved from 2.68424 to 2.58758, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9747 - mse: 2.3677 - val_loss: 2.5876 - val_mse: 0.0483\n","Epoch 62/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2675 - mse: 2.7452 \n","Epoch 62: val_loss did not improve from 2.58758\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2014 - mse: 2.6831 - val_loss: 2.6042 - val_mse: 0.1501\n","Epoch 63/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1189 - mse: 2.6813 \n","Epoch 63: val_loss improved from 2.58758 to 2.45673, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0852 - mse: 2.6512 - val_loss: 2.4567 - val_mse: 0.0844\n","Epoch 64/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7525 - mse: 2.3958 \n","Epoch 64: val_loss improved from 2.45673 to 2.40559, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7627 - mse: 2.4106 - val_loss: 2.4056 - val_mse: 0.1151\n","Epoch 65/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9469 - mse: 2.6707 \n","Epoch 65: val_loss improved from 2.40559 to 2.26613, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8962 - mse: 2.6243 - val_loss: 2.2661 - val_mse: 0.0562\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9735 - mse: 2.7788 \n","Epoch 66: val_loss improved from 2.26613 to 2.17915, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9183 - mse: 2.7282 - val_loss: 2.1791 - val_mse: 0.0511\n","Epoch 67/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4445 - mse: 2.3332 \n","Epoch 67: val_loss improved from 2.17915 to 2.14709, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4369 - mse: 2.3295 - val_loss: 2.1471 - val_mse: 0.1014\n","Epoch 68/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0379 - mse: 2.0100 \n","Epoch 68: val_loss improved from 2.14709 to 2.06590, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0542 - mse: 2.0302 - val_loss: 2.0659 - val_mse: 0.1025\n","Epoch 69/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1927 - mse: 2.2440 \n","Epoch 69: val_loss improved from 2.06590 to 1.96012, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1777 - mse: 2.2317 - val_loss: 1.9601 - val_mse: 0.0686\n","Epoch 70/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2167 - mse: 2.3401 \n","Epoch 70: val_loss improved from 1.96012 to 1.88208, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2481 - mse: 2.3748 - val_loss: 1.8821 - val_mse: 0.0595\n","Epoch 71/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1749 - mse: 2.3662 \n","Epoch 71: val_loss improved from 1.88208 to 1.80350, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1483 - mse: 2.3421 - val_loss: 1.8035 - val_mse: 0.0476\n","Epoch 72/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8143 - mse: 2.0728 \n","Epoch 72: val_loss improved from 1.80350 to 1.72745, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8149 - mse: 2.0768 - val_loss: 1.7275 - val_mse: 0.0424\n","Epoch 73/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7616 - mse: 2.0900 \n","Epoch 73: val_loss improved from 1.72745 to 1.69304, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7812 - mse: 2.1126 - val_loss: 1.6930 - val_mse: 0.0715\n","Epoch 74/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2663 - mse: 2.6559 \n","Epoch 74: val_loss improved from 1.69304 to 1.64333, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2055 - mse: 2.5978 - val_loss: 1.6433 - val_mse: 0.0801\n","Epoch 75/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7776 - mse: 2.2259 \n","Epoch 75: val_loss improved from 1.64333 to 1.56234, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7615 - mse: 2.2133 - val_loss: 1.5623 - val_mse: 0.0598\n","Epoch 76/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4752 - mse: 1.9834 \n","Epoch 76: val_loss improved from 1.56234 to 1.50892, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5003 - mse: 2.0122 - val_loss: 1.5089 - val_mse: 0.0608\n","Epoch 77/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8413 - mse: 2.4031 \n","Epoch 77: val_loss improved from 1.50892 to 1.45783, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7773 - mse: 2.3423 - val_loss: 1.4578 - val_mse: 0.0649\n","Epoch 78/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5628 - mse: 2.1811 \n","Epoch 78: val_loss improved from 1.45783 to 1.41796, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5811 - mse: 2.2026 - val_loss: 1.4180 - val_mse: 0.0800\n","Epoch 79/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6499 - mse: 2.3220 \n","Epoch 79: val_loss improved from 1.41796 to 1.33243, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6175 - mse: 2.2920 - val_loss: 1.3324 - val_mse: 0.0455\n","Epoch 80/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2551 - mse: 1.9783 \n","Epoch 80: val_loss improved from 1.33243 to 1.30834, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2747 - mse: 2.0006 - val_loss: 1.3083 - val_mse: 0.0700\n","Epoch 81/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0001 - mse: 1.7724 \n","Epoch 81: val_loss improved from 1.30834 to 1.26246, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0287 - mse: 1.8027 - val_loss: 1.2625 - val_mse: 0.0683\n","Epoch 82/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3954 - mse: 2.2090 \n","Epoch 82: val_loss improved from 1.26246 to 1.20215, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3876 - mse: 2.2031 - val_loss: 1.2022 - val_mse: 0.0467\n","Epoch 83/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9985 - mse: 1.8515 \n","Epoch 83: val_loss improved from 1.20215 to 1.17702, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0148 - mse: 1.8691 - val_loss: 1.1770 - val_mse: 0.0590\n","Epoch 84/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0782 - mse: 1.9692 \n","Epoch 84: val_loss improved from 1.17702 to 1.12693, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0885 - mse: 1.9811 - val_loss: 1.1269 - val_mse: 0.0509\n","Epoch 85/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0951 - mse: 2.0271 \n","Epoch 85: val_loss improved from 1.12693 to 1.11967, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0533 - mse: 1.9873 - val_loss: 1.1197 - val_mse: 0.0878\n","Epoch 86/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8713 - mse: 1.8470 \n","Epoch 86: val_loss improved from 1.11967 to 1.06178, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8956 - mse: 1.8742 - val_loss: 1.0618 - val_mse: 0.0696\n","Epoch 87/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7814 - mse: 1.7970 \n","Epoch 87: val_loss improved from 1.06178 to 1.02623, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7811 - mse: 1.7989 - val_loss: 1.0262 - val_mse: 0.0732\n","Epoch 88/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7198 - mse: 1.7748 \n","Epoch 88: val_loss improved from 1.02623 to 0.98107, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7375 - mse: 1.7942 - val_loss: 0.9811 - val_mse: 0.0639\n","Epoch 89/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9209 - mse: 2.0109 \n","Epoch 89: val_loss improved from 0.98107 to 0.94074, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9299 - mse: 2.0213 - val_loss: 0.9407 - val_mse: 0.0545\n","Epoch 90/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5579 - mse: 1.6784 \n","Epoch 90: val_loss improved from 0.94074 to 0.92607, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6011 - mse: 1.7231 - val_loss: 0.9261 - val_mse: 0.0692\n","Epoch 91/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0530 - mse: 2.2012 \n","Epoch 91: val_loss improved from 0.92607 to 0.88053, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9819 - mse: 2.1314 - val_loss: 0.8805 - val_mse: 0.0509\n","Epoch 92/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6257 - mse: 1.8023 \n","Epoch 92: val_loss improved from 0.88053 to 0.85117, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6179 - mse: 1.7959 - val_loss: 0.8512 - val_mse: 0.0496\n","Epoch 93/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7119 - mse: 1.9153 \n","Epoch 93: val_loss did not improve from 0.85117\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7013 - mse: 1.9055 - val_loss: 0.8568 - val_mse: 0.0799\n","Epoch 94/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5513 - mse: 1.7799 \n","Epoch 94: val_loss improved from 0.85117 to 0.81362, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5665 - mse: 1.7960 - val_loss: 0.8136 - val_mse: 0.0640\n","Epoch 95/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4365 - mse: 1.6927 \n","Epoch 95: val_loss improved from 0.81362 to 0.79751, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4482 - mse: 1.7058 - val_loss: 0.7975 - val_mse: 0.0761\n","Epoch 96/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3425 - mse: 1.6272 \n","Epoch 96: val_loss improved from 0.79751 to 0.76112, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3599 - mse: 1.6462 - val_loss: 0.7611 - val_mse: 0.0665\n","Epoch 97/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6011 - mse: 1.9113 \n","Epoch 97: val_loss improved from 0.76112 to 0.72237, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5801 - mse: 1.8915 - val_loss: 0.7224 - val_mse: 0.0503\n","Epoch 98/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5000 - mse: 1.8328 \n","Epoch 98: val_loss improved from 0.72237 to 0.71652, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4989 - mse: 1.8329 - val_loss: 0.7165 - val_mse: 0.0664\n","Epoch 99/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2863 - mse: 1.6416 \n","Epoch 99: val_loss improved from 0.71652 to 0.70477, saving model to best_model_RMSprop_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2967 - mse: 1.6529 - val_loss: 0.7048 - val_mse: 0.0791\n","Epoch 100/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3227 - mse: 1.7027 \n","Epoch 100: val_loss did not improve from 0.70477\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3192 - mse: 1.7001 - val_loss: 0.7221 - val_mse: 0.1200\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: RMSprop, Learning Rate: 0.001, Epochs: 100, Test MSE: 0.09795026393054462\n","Training with optimizer: RMSprop, learning rate: 0.01, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 129.8698 - mse: 122.7247\n","Epoch 1: val_loss improved from inf to 64.01395, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 311ms/step - loss: 129.0517 - mse: 121.8873 - val_loss: 64.0139 - val_mse: 55.5963\n","Epoch 2/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 44.7888 - mse: 36.2619 \n","Epoch 2: val_loss improved from 64.01395 to 16.13426, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 41.5244 - mse: 32.9846 - val_loss: 16.1343 - val_mse: 7.5579\n","Epoch 3/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.5354 - mse: 7.9631 \n","Epoch 3: val_loss improved from 16.13426 to 13.78001, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.3786 - mse: 7.8163 - val_loss: 13.7800 - val_mse: 5.3974\n","Epoch 4/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.5335 - mse: 5.2941 \n","Epoch 4: val_loss improved from 13.78001 to 11.63877, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.4263 - mse: 5.2262 - val_loss: 11.6388 - val_mse: 4.1489\n","Epoch 5/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.3053 - mse: 4.1272 \n","Epoch 5: val_loss improved from 11.63877 to 6.45345, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.1676 - mse: 4.0635 - val_loss: 6.4535 - val_mse: 0.5660\n","Epoch 6/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9240 - mse: 3.3045 \n","Epoch 6: val_loss improved from 6.45345 to 4.80946, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.8239 - mse: 3.2701 - val_loss: 4.8095 - val_mse: 0.2685\n","Epoch 7/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0707 - mse: 2.7538 \n","Epoch 7: val_loss improved from 4.80946 to 3.50121, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.9839 - mse: 2.7404 - val_loss: 3.5012 - val_mse: 0.2848\n","Epoch 8/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5069 - mse: 2.4742 \n","Epoch 8: val_loss improved from 3.50121 to 2.79571, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4414 - mse: 2.4560 - val_loss: 2.7957 - val_mse: 0.3042\n","Epoch 9/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7753 - mse: 2.3754 \n","Epoch 9: val_loss improved from 2.79571 to 2.05177, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7456 - mse: 2.3845 - val_loss: 2.0518 - val_mse: 0.2523\n","Epoch 10/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7099 - mse: 2.0032 \n","Epoch 10: val_loss improved from 2.05177 to 1.74796, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6936 - mse: 1.9959 - val_loss: 1.7480 - val_mse: 0.1523\n","Epoch 11/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6299 - mse: 2.1175 \n","Epoch 11: val_loss improved from 1.74796 to 1.72850, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5850 - mse: 2.0890 - val_loss: 1.7285 - val_mse: 0.4655\n","Epoch 12/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9486 - mse: 1.7043 \n","Epoch 12: val_loss improved from 1.72850 to 1.44352, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9579 - mse: 1.7176 - val_loss: 1.4435 - val_mse: 0.2510\n","Epoch 13/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7177 - mse: 1.5638 \n","Epoch 13: val_loss did not improve from 1.44352\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7471 - mse: 1.5918 - val_loss: 1.4911 - val_mse: 0.1030\n","Epoch 14/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9595 - mse: 1.6859 \n","Epoch 14: val_loss improved from 1.44352 to 1.18000, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9394 - mse: 1.6811 - val_loss: 1.1800 - val_mse: 0.2085\n","Epoch 15/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5493 - mse: 1.6230 \n","Epoch 15: val_loss improved from 1.18000 to 0.95580, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5164 - mse: 1.6000 - val_loss: 0.9558 - val_mse: 0.1464\n","Epoch 16/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2815 - mse: 1.4795 \n","Epoch 16: val_loss improved from 0.95580 to 0.92168, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2621 - mse: 1.4660 - val_loss: 0.9217 - val_mse: 0.1116\n","Epoch 17/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3408 - mse: 1.5326 \n","Epoch 17: val_loss improved from 0.92168 to 0.82299, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3263 - mse: 1.5255 - val_loss: 0.8230 - val_mse: 0.1085\n","Epoch 18/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4168 - mse: 1.7130 \n","Epoch 18: val_loss did not improve from 0.82299\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3788 - mse: 1.6774 - val_loss: 1.1807 - val_mse: 0.5254\n","Epoch 19/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1084 - mse: 1.4778 \n","Epoch 19: val_loss improved from 0.82299 to 0.76997, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1264 - mse: 1.4892 - val_loss: 0.7700 - val_mse: 0.1004\n","Epoch 20/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1360 - mse: 1.5390 \n","Epoch 20: val_loss did not improve from 0.76997\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1278 - mse: 1.5341 - val_loss: 0.7763 - val_mse: 0.1691\n","Epoch 21/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9379 - mse: 1.3451 \n","Epoch 21: val_loss did not improve from 0.76997\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9281 - mse: 1.3437 - val_loss: 0.8324 - val_mse: 0.1088\n","Epoch 22/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9948 - mse: 1.3386 \n","Epoch 22: val_loss did not improve from 0.76997\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9890 - mse: 1.3397 - val_loss: 0.8569 - val_mse: 0.2461\n","Epoch 23/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7317 - mse: 1.1606 \n","Epoch 23: val_loss improved from 0.76997 to 0.74413, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7348 - mse: 1.1680 - val_loss: 0.7441 - val_mse: 0.2458\n","Epoch 24/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9550 - mse: 1.3701 \n","Epoch 24: val_loss improved from 0.74413 to 0.60990, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9522 - mse: 1.3663 - val_loss: 0.6099 - val_mse: 0.1337\n","Epoch 25/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7576 - mse: 1.2632 \n","Epoch 25: val_loss did not improve from 0.60990\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7796 - mse: 1.2842 - val_loss: 0.7010 - val_mse: 0.1495\n","Epoch 26/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7645 - mse: 1.2410 \n","Epoch 26: val_loss did not improve from 0.60990\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7719 - mse: 1.2472 - val_loss: 0.7729 - val_mse: 0.1211\n","Epoch 27/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7669 - mse: 1.2185 \n","Epoch 27: val_loss improved from 0.60990 to 0.49917, saving model to best_model_RMSprop_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7630 - mse: 1.2201 - val_loss: 0.4992 - val_mse: 0.0905\n","Epoch 28/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8633 - mse: 1.3834 \n","Epoch 28: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8478 - mse: 1.3630 - val_loss: 0.6826 - val_mse: 0.1354\n","Epoch 29/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8081 - mse: 1.2565 \n","Epoch 29: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7981 - mse: 1.2471 - val_loss: 0.6743 - val_mse: 0.1256\n","Epoch 30/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7927 - mse: 1.3067 \n","Epoch 30: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7658 - mse: 1.2889 - val_loss: 0.5052 - val_mse: 0.0737\n","Epoch 31/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5342 - mse: 1.1096 \n","Epoch 31: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5392 - mse: 1.1177 - val_loss: 0.5235 - val_mse: 0.0760\n","Epoch 32/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6148 - mse: 1.1606 \n","Epoch 32: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5948 - mse: 1.1446 - val_loss: 0.6298 - val_mse: 0.1056\n","Epoch 33/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4082 - mse: 0.9649 \n","Epoch 33: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4216 - mse: 0.9781 - val_loss: 0.5203 - val_mse: 0.0740\n","Epoch 34/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5605 - mse: 1.0902 \n","Epoch 34: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5672 - mse: 1.0994 - val_loss: 0.6823 - val_mse: 0.1422\n","Epoch 35/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6263 - mse: 1.0750 \n","Epoch 35: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6104 - mse: 1.0652 - val_loss: 0.6070 - val_mse: 0.1628\n","Epoch 36/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6118 - mse: 1.1601 \n","Epoch 36: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5999 - mse: 1.1437 - val_loss: 0.5994 - val_mse: 0.2066\n","Epoch 37/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3577 - mse: 0.9528 \n","Epoch 37: val_loss did not improve from 0.49917\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3707 - mse: 0.9567 - val_loss: 0.7462 - val_mse: 0.1882\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: RMSprop, Learning Rate: 0.01, Epochs: 100, Test MSE: 0.11567626630947042\n","Training with optimizer: RMSprop, learning rate: 0.1, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 131.4871 - mse: 48.0643\n","Epoch 1: val_loss improved from inf to 80.70847, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 315ms/step - loss: 130.0548 - mse: 47.0377 - val_loss: 80.7085 - val_mse: 46.9274\n","Epoch 2/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.9462 - mse: 6.0968 \n","Epoch 2: val_loss improved from 80.70847 to 16.64442, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 29.5387 - mse: 5.9223 - val_loss: 16.6444 - val_mse: 5.0417\n","Epoch 3/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18.1453 - mse: 5.4716 \n","Epoch 3: val_loss did not improve from 16.64442\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17.8180 - mse: 5.1990 - val_loss: 20.5430 - val_mse: 11.5718\n","Epoch 4/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.3667 - mse: 6.3577 \n","Epoch 4: val_loss did not improve from 16.64442\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16.9755 - mse: 5.9928 - val_loss: 23.7345 - val_mse: 8.6447\n","Epoch 5/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.8595 - mse: 3.5843 \n","Epoch 5: val_loss improved from 16.64442 to 10.52498, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.8127 - mse: 3.6577 - val_loss: 10.5250 - val_mse: 0.6549\n","Epoch 6/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19.4067 - mse: 4.2970 \n","Epoch 6: val_loss did not improve from 10.52498\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18.8001 - mse: 4.2696 - val_loss: 11.1075 - val_mse: 2.7478\n","Epoch 7/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.1800 - mse: 3.2480 \n","Epoch 7: val_loss did not improve from 10.52498\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.2180 - mse: 3.2726 - val_loss: 11.1687 - val_mse: 2.3723\n","Epoch 8/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.5604 - mse: 4.7423 \n","Epoch 8: val_loss did not improve from 10.52498\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17.3259 - mse: 4.5557 - val_loss: 13.4644 - val_mse: 6.2268\n","Epoch 9/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2496 - mse: 4.2693 \n","Epoch 9: val_loss improved from 10.52498 to 7.73829, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.0639 - mse: 4.1188 - val_loss: 7.7383 - val_mse: 0.5006\n","Epoch 10/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.1230 - mse: 3.3561\n","Epoch 10: val_loss did not improve from 7.73829\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.1207 - mse: 3.2894 - val_loss: 15.0146 - val_mse: 2.8059\n","Epoch 11/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.1442 - mse: 2.8121 \n","Epoch 11: val_loss did not improve from 7.73829\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.1057 - mse: 2.8780 - val_loss: 11.3756 - val_mse: 1.9112\n","Epoch 12/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.2857 - mse: 2.6496 \n","Epoch 12: val_loss improved from 7.73829 to 7.15867, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.1888 - mse: 2.6669 - val_loss: 7.1587 - val_mse: 1.7901\n","Epoch 13/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.2649 - mse: 2.2615\n","Epoch 13: val_loss did not improve from 7.15867\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.1081 - mse: 2.2511 - val_loss: 9.4299 - val_mse: 2.9932\n","Epoch 14/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9183 - mse: 2.7704  \n","Epoch 14: val_loss improved from 7.15867 to 6.56936, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.8035 - mse: 2.7131 - val_loss: 6.5694 - val_mse: 0.8831\n","Epoch 15/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.7491 - mse: 1.5908 \n","Epoch 15: val_loss did not improve from 6.56936\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.7177 - mse: 1.6342 - val_loss: 6.7285 - val_mse: 0.4437\n","Epoch 16/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8743 - mse: 2.0099 \n","Epoch 16: val_loss did not improve from 6.56936\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.8865 - mse: 1.9656 - val_loss: 10.8169 - val_mse: 3.0407\n","Epoch 17/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7563 - mse: 1.5484  \n","Epoch 17: val_loss did not improve from 6.56936\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.7792 - mse: 1.5436 - val_loss: 18.3381 - val_mse: 13.0482\n","Epoch 18/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3633 - mse: 1.5196 \n","Epoch 18: val_loss did not improve from 6.56936\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.2687 - mse: 1.4883 - val_loss: 7.4032 - val_mse: 3.3359\n","Epoch 19/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6008 - mse: 1.3485 \n","Epoch 19: val_loss improved from 6.56936 to 3.90095, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6322 - mse: 1.3079 - val_loss: 3.9010 - val_mse: 0.0929\n","Epoch 20/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6862 - mse: 1.1564 \n","Epoch 20: val_loss did not improve from 3.90095\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.6044 - mse: 1.1173 - val_loss: 8.7431 - val_mse: 2.6407\n","Epoch 21/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9986 - mse: 1.1459 \n","Epoch 21: val_loss did not improve from 3.90095\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.8247 - mse: 1.0864 - val_loss: 6.5407 - val_mse: 1.0798\n","Epoch 22/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9092 - mse: 0.6819 \n","Epoch 22: val_loss did not improve from 3.90095\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.8638 - mse: 0.6800 - val_loss: 4.5818 - val_mse: 0.3452\n","Epoch 23/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5732 - mse: 0.4255 \n","Epoch 23: val_loss did not improve from 3.90095\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5295 - mse: 0.4226 - val_loss: 4.6411 - val_mse: 0.1504\n","Epoch 24/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3503 - mse: 0.4139 \n","Epoch 24: val_loss did not improve from 3.90095\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2948 - mse: 0.4015 - val_loss: 4.6675 - val_mse: 1.2345\n","Epoch 25/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6790 - mse: 0.3286 \n","Epoch 25: val_loss did not improve from 3.90095\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6543 - mse: 0.3192 - val_loss: 4.9797 - val_mse: 2.8468\n","Epoch 26/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0827 - mse: 0.1899 \n","Epoch 26: val_loss improved from 3.90095 to 2.47907, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2972 - mse: 0.1925 - val_loss: 2.4791 - val_mse: 0.1402\n","Epoch 27/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7830 - mse: 0.2058 \n","Epoch 27: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9040 - mse: 0.2113 - val_loss: 3.5805 - val_mse: 0.1631\n","Epoch 28/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0812 - mse: 0.1617 \n","Epoch 28: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1367 - mse: 0.1619 - val_loss: 4.0057 - val_mse: 0.3252\n","Epoch 29/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2631 - mse: 0.2768 \n","Epoch 29: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2710 - mse: 0.2677 - val_loss: 3.6750 - val_mse: 1.2958\n","Epoch 30/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7481 - mse: 0.1548 \n","Epoch 30: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8916 - mse: 0.1628 - val_loss: 10.2538 - val_mse: 2.7032\n","Epoch 31/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3405 - mse: 0.1786 \n","Epoch 31: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2567 - mse: 0.1761 - val_loss: 4.1095 - val_mse: 0.3395\n","Epoch 32/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9716 - mse: 0.3020 \n","Epoch 32: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.9605 - mse: 0.2886 - val_loss: 4.1074 - val_mse: 2.0131\n","Epoch 33/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2690 - mse: 0.2091 \n","Epoch 33: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.1587 - mse: 0.2073 - val_loss: 2.6970 - val_mse: 0.4032\n","Epoch 34/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6520 - mse: 0.1808 \n","Epoch 34: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7264 - mse: 0.1840 - val_loss: 2.5778 - val_mse: 0.1354\n","Epoch 35/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6027 - mse: 0.1472 \n","Epoch 35: val_loss did not improve from 2.47907\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6865 - mse: 0.1524 - val_loss: 2.9201 - val_mse: 0.2046\n","Epoch 36/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3945 - mse: 0.1864 \n","Epoch 36: val_loss improved from 2.47907 to 1.96378, saving model to best_model_RMSprop_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5028 - mse: 0.1865 - val_loss: 1.9638 - val_mse: 0.0704\n","Epoch 37/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6883 - mse: 0.2907 \n","Epoch 37: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.7311 - mse: 0.2952 - val_loss: 4.1552 - val_mse: 0.1569\n","Epoch 38/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4858 - mse: 0.1526 \n","Epoch 38: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5402 - mse: 0.1526 - val_loss: 3.4106 - val_mse: 0.3209\n","Epoch 39/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1679 - mse: 0.2160 \n","Epoch 39: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2585 - mse: 0.2197 - val_loss: 2.5174 - val_mse: 0.1327\n","Epoch 40/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1556 - mse: 0.1644 \n","Epoch 40: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1476 - mse: 0.1655 - val_loss: 3.7700 - val_mse: 0.1912\n","Epoch 41/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0812 - mse: 0.1841 \n","Epoch 41: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1282 - mse: 0.1826 - val_loss: 2.1123 - val_mse: 0.1212\n","Epoch 42/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8278 - mse: 0.3328 \n","Epoch 42: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8564 - mse: 0.3178 - val_loss: 2.5332 - val_mse: 0.4322\n","Epoch 43/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5596 - mse: 0.1665 \n","Epoch 43: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6092 - mse: 0.1697 - val_loss: 2.7127 - val_mse: 0.1447\n","Epoch 44/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4446 - mse: 0.1830 \n","Epoch 44: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4034 - mse: 0.1846 - val_loss: 5.0807 - val_mse: 0.2875\n","Epoch 45/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6967 - mse: 0.2756 \n","Epoch 45: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5670 - mse: 0.2568 - val_loss: 3.9419 - val_mse: 0.3484\n","Epoch 46/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8280 - mse: 0.2046 \n","Epoch 46: val_loss did not improve from 1.96378\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8517 - mse: 0.2032 - val_loss: 3.0764 - val_mse: 0.2236\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: RMSprop, Learning Rate: 0.1, Epochs: 100, Test MSE: 0.08414712396526099\n","Training with optimizer: Nadam, learning rate: 0.001, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - loss: 151.9328 - mse: 146.3506\n","Epoch 1: val_loss improved from inf to 144.73209, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 322ms/step - loss: 151.9179 - mse: 146.3361 - val_loss: 144.7321 - val_mse: 139.1732\n","Epoch 2/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 147.4115 - mse: 141.8545 \n","Epoch 2: val_loss improved from 144.73209 to 139.77116, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 147.0267 - mse: 141.4704 - val_loss: 139.7712 - val_mse: 134.2240\n","Epoch 3/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 140.5332 - mse: 134.9879  \n","Epoch 3: val_loss improved from 139.77116 to 135.27309, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 140.5960 - mse: 135.0514 - val_loss: 135.2731 - val_mse: 129.7337\n","Epoch 4/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 137.2823 - mse: 131.7431 \n","Epoch 4: val_loss improved from 135.27309 to 128.01175, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 137.0519 - mse: 131.5129 - val_loss: 128.0117 - val_mse: 122.4752\n","Epoch 5/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 134.3633 - mse: 128.8273 \n","Epoch 5: val_loss improved from 128.01175 to 121.07597, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 133.7816 - mse: 128.2456 - val_loss: 121.0760 - val_mse: 115.5393\n","Epoch 6/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.8204 - mse: 119.2832 \n","Epoch 6: val_loss improved from 121.07597 to 114.25851, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 124.6145 - mse: 119.0772 - val_loss: 114.2585 - val_mse: 108.7205\n","Epoch 7/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.3458 - mse: 110.8074 \n","Epoch 7: val_loss improved from 114.25851 to 106.70404, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 116.3340 - mse: 110.7954 - val_loss: 106.7040 - val_mse: 101.1648\n","Epoch 8/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 110.1891 - mse: 104.6493 \n","Epoch 8: val_loss improved from 106.70404 to 96.87434, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 109.3334 - mse: 103.7933 - val_loss: 96.8743 - val_mse: 91.3312\n","Epoch 9/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.7480 - mse: 97.2041  \n","Epoch 9: val_loss improved from 96.87434 to 86.63572, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 101.9048 - mse: 96.3606 - val_loss: 86.6357 - val_mse: 81.0893\n","Epoch 10/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.9612 - mse: 84.4138 \n","Epoch 10: val_loss improved from 86.63572 to 77.38144, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 89.3636 - mse: 83.8159 - val_loss: 77.3814 - val_mse: 71.8308\n","Epoch 11/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.3343 - mse: 74.7823 \n","Epoch 11: val_loss improved from 77.38144 to 66.53938, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 79.6368 - mse: 74.0844 - val_loss: 66.5394 - val_mse: 60.9817\n","Epoch 12/100\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.1998 - mse: 64.6417 \n","Epoch 12: val_loss improved from 66.53938 to 54.66636, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 68.4995 - mse: 62.9411 - val_loss: 54.6664 - val_mse: 49.1055\n","Epoch 13/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.6168 - mse: 57.0554 \n","Epoch 13: val_loss improved from 54.66636 to 45.20464, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 61.2696 - mse: 55.7080 - val_loss: 45.2046 - val_mse: 39.6400\n","Epoch 14/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.0663 - mse: 42.5009 \n","Epoch 14: val_loss improved from 45.20464 to 37.22106, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 47.7416 - mse: 42.1760 - val_loss: 37.2211 - val_mse: 31.6539\n","Epoch 15/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.4429 - mse: 35.8755 \n","Epoch 15: val_loss improved from 37.22106 to 30.05518, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 40.7432 - mse: 35.1757 - val_loss: 30.0552 - val_mse: 24.4869\n","Epoch 16/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33.6865 - mse: 28.1184 \n","Epoch 16: val_loss improved from 30.05518 to 23.42591, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 33.2465 - mse: 27.6783 - val_loss: 23.4259 - val_mse: 17.8580\n","Epoch 17/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.6579 - mse: 24.0903 \n","Epoch 17: val_loss improved from 23.42591 to 17.97111, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 28.6556 - mse: 23.0882 - val_loss: 17.9711 - val_mse: 12.4065\n","Epoch 18/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.9841 - mse: 18.4203 \n","Epoch 18: val_loss improved from 17.97111 to 13.78129, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.6917 - mse: 18.1283 - val_loss: 13.7813 - val_mse: 8.2219\n","Epoch 19/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.4672 - mse: 13.9088 \n","Epoch 19: val_loss improved from 13.78129 to 11.04125, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.3984 - mse: 13.8403 - val_loss: 11.0413 - val_mse: 5.4874\n","Epoch 20/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.1964 - mse: 13.6435 \n","Epoch 20: val_loss improved from 11.04125 to 9.42590, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.7973 - mse: 13.2447 - val_loss: 9.4259 - val_mse: 3.8775\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.0199 - mse: 11.4726 \n","Epoch 21: val_loss improved from 9.42590 to 7.95141, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.5545 - mse: 11.0077 - val_loss: 7.9514 - val_mse: 2.4099\n","Epoch 22/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.0632 - mse: 8.5231 \n","Epoch 22: val_loss improved from 7.95141 to 7.29224, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.9816 - mse: 8.4419 - val_loss: 7.2922 - val_mse: 1.7587\n","Epoch 23/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2478 - mse: 7.7160 \n","Epoch 23: val_loss improved from 7.29224 to 6.80147, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.1854 - mse: 7.6542 - val_loss: 6.8015 - val_mse: 1.2773\n","Epoch 24/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.6445 - mse: 7.1220 \n","Epoch 24: val_loss improved from 6.80147 to 6.36367, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.5819 - mse: 7.0600 - val_loss: 6.3637 - val_mse: 0.8487\n","Epoch 25/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.1339 - mse: 7.6206 \n","Epoch 25: val_loss improved from 6.36367 to 6.28293, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.0914 - mse: 7.5789 - val_loss: 6.2829 - val_mse: 0.7777\n","Epoch 26/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.3133 - mse: 5.8098 \n","Epoch 26: val_loss improved from 6.28293 to 6.27224, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.3952 - mse: 5.8924 - val_loss: 6.2722 - val_mse: 0.7763\n","Epoch 27/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.3223 - mse: 6.8279 \n","Epoch 27: val_loss improved from 6.27224 to 6.09478, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.3050 - mse: 6.8113 - val_loss: 6.0948 - val_mse: 0.6082\n","Epoch 28/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.0047 - mse: 6.5199\n","Epoch 28: val_loss improved from 6.09478 to 5.97888, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.0348 - mse: 6.5505 - val_loss: 5.9789 - val_mse: 0.5018\n","Epoch 29/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.0101 - mse: 6.5347 \n","Epoch 29: val_loss did not improve from 5.97888\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.9579 - mse: 6.4830 - val_loss: 5.9838 - val_mse: 0.5158\n","Epoch 30/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7571 - mse: 5.2911 \n","Epoch 30: val_loss improved from 5.97888 to 5.79692, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.7787 - mse: 5.3133 - val_loss: 5.7969 - val_mse: 0.3399\n","Epoch 31/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.5124 - mse: 6.0575 \n","Epoch 31: val_loss improved from 5.79692 to 5.75873, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.3683 - mse: 5.9143 - val_loss: 5.7587 - val_mse: 0.3138\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.4477 - mse: 6.0049 \n","Epoch 32: val_loss improved from 5.75873 to 5.70470, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.3533 - mse: 5.9113 - val_loss: 5.7047 - val_mse: 0.2713\n","Epoch 33/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0627 - mse: 4.6316\n","Epoch 33: val_loss improved from 5.70470 to 5.68813, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.1314 - mse: 4.7012 - val_loss: 5.6881 - val_mse: 0.2682\n","Epoch 34/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3755 - mse: 4.9578 \n","Epoch 34: val_loss improved from 5.68813 to 5.66300, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.4240 - mse: 5.0073 - val_loss: 5.6630 - val_mse: 0.2561\n","Epoch 35/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6869 - mse: 5.2822 \n","Epoch 35: val_loss did not improve from 5.66300\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.6899 - mse: 5.2862 - val_loss: 5.6657 - val_mse: 0.2717\n","Epoch 36/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7528 - mse: 5.3612\n","Epoch 36: val_loss improved from 5.66300 to 5.63333, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.7014 - mse: 5.3107 - val_loss: 5.6333 - val_mse: 0.2527\n","Epoch 37/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9723 - mse: 4.5943  \n","Epoch 37: val_loss did not improve from 5.63333\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.9586 - mse: 4.5815 - val_loss: 5.6422 - val_mse: 0.2758\n","Epoch 38/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5293 - mse: 4.1656 \n","Epoch 38: val_loss improved from 5.63333 to 5.60023, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.5127 - mse: 4.1499 - val_loss: 5.6002 - val_mse: 0.2487\n","Epoch 39/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0989 - mse: 4.7503\n","Epoch 39: val_loss improved from 5.60023 to 5.51472, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.1917 - mse: 4.8440 - val_loss: 5.5147 - val_mse: 0.1784\n","Epoch 40/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9690 - mse: 3.6354 \n","Epoch 40: val_loss did not improve from 5.51472\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.0555 - mse: 3.7228 - val_loss: 5.5155 - val_mse: 0.1944\n","Epoch 41/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3767 - mse: 4.0588 \n","Epoch 41: val_loss improved from 5.51472 to 5.44432, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.4256 - mse: 4.1085 - val_loss: 5.4443 - val_mse: 0.1394\n","Epoch 42/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.7681 - mse: 4.4663  \n","Epoch 42: val_loss improved from 5.44432 to 5.42671, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6932 - mse: 4.3923 - val_loss: 5.4267 - val_mse: 0.1380\n","Epoch 43/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8273 - mse: 4.5415 \n","Epoch 43: val_loss improved from 5.42671 to 5.41563, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.7119 - mse: 4.4272 - val_loss: 5.4156 - val_mse: 0.1431\n","Epoch 44/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1869 - mse: 3.9174 \n","Epoch 44: val_loss improved from 5.41563 to 5.38453, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.2327 - mse: 3.9644 - val_loss: 5.3845 - val_mse: 0.1293\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0169 - mse: 4.7648\n","Epoch 45: val_loss did not improve from 5.38453\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.9683 - mse: 4.7173 - val_loss: 5.3848 - val_mse: 0.1467\n","Epoch 46/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3003 - mse: 4.0655  \n","Epoch 46: val_loss improved from 5.38453 to 5.35565, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.2358 - mse: 4.0021 - val_loss: 5.3556 - val_mse: 0.1353\n","Epoch 47/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9636 - mse: 4.7465  \n","Epoch 47: val_loss improved from 5.35565 to 5.32723, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.8674 - mse: 4.6515 - val_loss: 5.3272 - val_mse: 0.1250\n","Epoch 48/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7846 - mse: 3.5859 \n","Epoch 48: val_loss improved from 5.32723 to 5.31627, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7803 - mse: 3.5827 - val_loss: 5.3163 - val_mse: 0.1327\n","Epoch 49/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1726 - mse: 3.9925 \n","Epoch 49: val_loss improved from 5.31627 to 5.28505, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.1818 - mse: 4.0027 - val_loss: 5.2851 - val_mse: 0.1196\n","Epoch 50/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7477 - mse: 3.5855 \n","Epoch 50: val_loss improved from 5.28505 to 5.25892, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7652 - mse: 3.6043 - val_loss: 5.2589 - val_mse: 0.1117\n","Epoch 51/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7833 - mse: 3.6394 \n","Epoch 51: val_loss improved from 5.25892 to 5.24122, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.8274 - mse: 3.6848 - val_loss: 5.2412 - val_mse: 0.1130\n","Epoch 52/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3746 - mse: 3.2499 \n","Epoch 52: val_loss did not improve from 5.24122\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.3992 - mse: 3.2758 - val_loss: 5.2538 - val_mse: 0.1451\n","Epoch 53/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2133 - mse: 3.1080 \n","Epoch 53: val_loss improved from 5.24122 to 5.21256, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3115 - mse: 3.2077 - val_loss: 5.2126 - val_mse: 0.1237\n","Epoch 54/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1011 - mse: 4.0156 \n","Epoch 54: val_loss improved from 5.21256 to 5.18182, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.0107 - mse: 3.9267 - val_loss: 5.1818 - val_mse: 0.1129\n","Epoch 55/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5275 - mse: 3.4622 \n","Epoch 55: val_loss improved from 5.18182 to 5.16190, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5151 - mse: 3.4511 - val_loss: 5.1619 - val_mse: 0.1132\n","Epoch 56/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6810 - mse: 3.6360 \n","Epoch 56: val_loss improved from 5.16190 to 5.14868, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.6972 - mse: 3.6535 - val_loss: 5.1487 - val_mse: 0.1203\n","Epoch 57/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1511 - mse: 3.1266 \n","Epoch 57: val_loss improved from 5.14868 to 5.13697, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1606 - mse: 3.1374 - val_loss: 5.1370 - val_mse: 0.1294\n","Epoch 58/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0782 - mse: 3.0747 \n","Epoch 58: val_loss improved from 5.13697 to 5.09011, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0967 - mse: 3.0943 - val_loss: 5.0901 - val_mse: 0.1036\n","Epoch 59/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6779 - mse: 3.6955 \n","Epoch 59: val_loss improved from 5.09011 to 5.06720, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5653 - mse: 3.5841 - val_loss: 5.0672 - val_mse: 0.1020\n","Epoch 60/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0198 - mse: 3.0586 \n","Epoch 60: val_loss improved from 5.06720 to 5.04661, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0411 - mse: 3.0814 - val_loss: 5.0466 - val_mse: 0.1033\n","Epoch 61/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9660 - mse: 3.0269 \n","Epoch 61: val_loss improved from 5.04661 to 5.02793, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.9957 - mse: 3.0579 - val_loss: 5.0279 - val_mse: 0.1069\n","Epoch 62/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6801 - mse: 2.7632 \n","Epoch 62: val_loss improved from 5.02793 to 5.00571, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7403 - mse: 2.8249 - val_loss: 5.0057 - val_mse: 0.1071\n","Epoch 63/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2077 - mse: 3.3130 \n","Epoch 63: val_loss improved from 5.00571 to 4.95860, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2060 - mse: 3.3130 - val_loss: 4.9586 - val_mse: 0.0826\n","Epoch 64/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1480 - mse: 3.2762 \n","Epoch 64: val_loss did not improve from 4.95860\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1249 - mse: 3.2546 - val_loss: 4.9691 - val_mse: 0.1160\n","Epoch 65/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8781 - mse: 3.0291 \n","Epoch 65: val_loss improved from 4.95860 to 4.92443, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.8528 - mse: 3.0053 - val_loss: 4.9244 - val_mse: 0.0943\n","Epoch 66/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0948 - mse: 3.2685 \n","Epoch 66: val_loss improved from 4.92443 to 4.90024, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0390 - mse: 3.2147 - val_loss: 4.9002 - val_mse: 0.0935\n","Epoch 67/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0622 - mse: 3.2594 \n","Epoch 67: val_loss improved from 4.90024 to 4.85993, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0366 - mse: 3.2357 - val_loss: 4.8599 - val_mse: 0.0764\n","Epoch 68/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3880 - mse: 2.6090 \n","Epoch 68: val_loss improved from 4.85993 to 4.83968, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3791 - mse: 2.6014 - val_loss: 4.8397 - val_mse: 0.0799\n","Epoch 69/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6624 - mse: 2.9072 \n","Epoch 69: val_loss improved from 4.83968 to 4.83384, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6568 - mse: 2.9029 - val_loss: 4.8338 - val_mse: 0.0980\n","Epoch 70/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6993 - mse: 2.9677 \n","Epoch 70: val_loss improved from 4.83384 to 4.80700, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7529 - mse: 3.0231 - val_loss: 4.8070 - val_mse: 0.0956\n","Epoch 71/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1354 - mse: 2.4281 \n","Epoch 71: val_loss improved from 4.80700 to 4.77734, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1706 - mse: 2.4653 - val_loss: 4.7773 - val_mse: 0.0908\n","Epoch 72/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1358 - mse: 2.4538 \n","Epoch 72: val_loss improved from 4.77734 to 4.75672, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1822 - mse: 2.5018 - val_loss: 4.7567 - val_mse: 0.0948\n","Epoch 73/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4398 - mse: 2.7824 \n","Epoch 73: val_loss improved from 4.75672 to 4.72514, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.4303 - mse: 2.7745 - val_loss: 4.7251 - val_mse: 0.0880\n","Epoch 74/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5065 - mse: 2.8740 \n","Epoch 74: val_loss improved from 4.72514 to 4.70224, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4858 - mse: 2.8549 - val_loss: 4.7022 - val_mse: 0.0904\n","Epoch 75/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4106 - mse: 2.8034 \n","Epoch 75: val_loss did not improve from 4.70224\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.4009 - mse: 2.7954 - val_loss: 4.7041 - val_mse: 0.1178\n","Epoch 76/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2765 - mse: 2.6953 \n","Epoch 76: val_loss improved from 4.70224 to 4.66060, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2784 - mse: 2.6986 - val_loss: 4.6606 - val_mse: 0.1005\n","Epoch 77/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4048 - mse: 2.8495 \n","Epoch 77: val_loss improved from 4.66060 to 4.62465, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3839 - mse: 2.8303 - val_loss: 4.6247 - val_mse: 0.0908\n","Epoch 78/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1858 - mse: 2.6568 \n","Epoch 78: val_loss improved from 4.62465 to 4.59339, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2026 - mse: 2.6753 - val_loss: 4.5934 - val_mse: 0.0859\n","Epoch 79/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1805 - mse: 2.6779 \n","Epoch 79: val_loss improved from 4.59339 to 4.54801, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1750 - mse: 2.6741 - val_loss: 4.5480 - val_mse: 0.0672\n","Epoch 80/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2309 - mse: 2.7550 \n","Epoch 80: val_loss improved from 4.54801 to 4.51895, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2255 - mse: 2.7513 - val_loss: 4.5189 - val_mse: 0.0647\n","Epoch 81/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6804 - mse: 2.2305 \n","Epoch 81: val_loss improved from 4.51895 to 4.49889, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7266 - mse: 2.2790 - val_loss: 4.4989 - val_mse: 0.0716\n","Epoch 82/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1764 - mse: 2.7540 \n","Epoch 82: val_loss improved from 4.49889 to 4.46215, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1201 - mse: 2.6995 - val_loss: 4.4621 - val_mse: 0.0620\n","Epoch 83/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5791 - mse: 2.1840 \n","Epoch 83: val_loss improved from 4.46215 to 4.43713, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.6453 - mse: 2.2520 - val_loss: 4.4371 - val_mse: 0.0645\n","Epoch 84/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7396 - mse: 2.3720 \n","Epoch 84: val_loss improved from 4.43713 to 4.40253, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7704 - mse: 2.4046 - val_loss: 4.4025 - val_mse: 0.0575\n","Epoch 85/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9327 - mse: 2.5927 \n","Epoch 85: val_loss improved from 4.40253 to 4.38745, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8701 - mse: 2.5319 - val_loss: 4.3874 - val_mse: 0.0698\n","Epoch 86/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6583 - mse: 2.3458 \n","Epoch 86: val_loss improved from 4.38745 to 4.37093, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6802 - mse: 2.3695 - val_loss: 4.3709 - val_mse: 0.0812\n","Epoch 87/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4990 - mse: 2.2144 \n","Epoch 87: val_loss improved from 4.37093 to 4.34864, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4933 - mse: 2.2106 - val_loss: 4.3486 - val_mse: 0.0872\n","Epoch 88/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8115 - mse: 2.5552 \n","Epoch 88: val_loss improved from 4.34864 to 4.30229, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8024 - mse: 2.5480 - val_loss: 4.3023 - val_mse: 0.0691\n","Epoch 89/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7210 - mse: 2.4927 \n","Epoch 89: val_loss improved from 4.30229 to 4.27041, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7243 - mse: 2.4981 - val_loss: 4.2704 - val_mse: 0.0656\n","Epoch 90/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4877 - mse: 2.2879 \n","Epoch 90: val_loss improved from 4.27041 to 4.23439, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5519 - mse: 2.3542 - val_loss: 4.2344 - val_mse: 0.0583\n","Epoch 91/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5994 - mse: 2.4285 \n","Epoch 91: val_loss improved from 4.23439 to 4.21944, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5407 - mse: 2.3716 - val_loss: 4.2194 - val_mse: 0.0718\n","Epoch 92/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4257 - mse: 2.2837 \n","Epoch 92: val_loss improved from 4.21944 to 4.17363, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4385 - mse: 2.2981 - val_loss: 4.1736 - val_mse: 0.0553\n","Epoch 93/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5417 - mse: 2.4291 \n","Epoch 93: val_loss improved from 4.17363 to 4.14623, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5298 - mse: 2.4188 - val_loss: 4.1462 - val_mse: 0.0573\n","Epoch 94/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3104 - mse: 2.2268 \n","Epoch 94: val_loss improved from 4.14623 to 4.12052, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3568 - mse: 2.2751 - val_loss: 4.1205 - val_mse: 0.0609\n","Epoch 95/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2791 - mse: 2.2249 \n","Epoch 95: val_loss improved from 4.12052 to 4.10496, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3082 - mse: 2.2559 - val_loss: 4.1050 - val_mse: 0.0749\n","Epoch 96/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2222 - mse: 2.1976 \n","Epoch 96: val_loss improved from 4.10496 to 4.06184, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2357 - mse: 2.2130 - val_loss: 4.0618 - val_mse: 0.0616\n","Epoch 97/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3022 - mse: 2.3066 \n","Epoch 97: val_loss improved from 4.06184 to 4.02503, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2769 - mse: 2.2842 - val_loss: 4.0250 - val_mse: 0.0550\n","Epoch 98/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4940 - mse: 2.5292 \n","Epoch 98: val_loss improved from 4.02503 to 4.00116, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4712 - mse: 2.5086 - val_loss: 4.0012 - val_mse: 0.0613\n","Epoch 99/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8766 - mse: 1.9422 \n","Epoch 99: val_loss improved from 4.00116 to 3.98344, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9232 - mse: 1.9908 - val_loss: 3.9834 - val_mse: 0.0740\n","Epoch 100/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2597 - mse: 2.3561 \n","Epoch 100: val_loss improved from 3.98344 to 3.95714, saving model to best_model_Nadam_lr_0.001_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2413 - mse: 2.3394 - val_loss: 3.9571 - val_mse: 0.0783\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: Nadam, Learning Rate: 0.001, Epochs: 100, Test MSE: 0.10984575434078131\n","Training with optimizer: Nadam, learning rate: 0.01, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - loss: 144.4173 - mse: 138.5233\n","Epoch 1: val_loss improved from inf to 101.76350, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 331ms/step - loss: 144.0418 - mse: 138.1370 - val_loss: 101.7635 - val_mse: 94.8315\n","Epoch 2/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.3718 - mse: 82.1784   \n","Epoch 2: val_loss improved from 101.76350 to 47.31407, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 83.2062 - mse: 75.9279 - val_loss: 47.3141 - val_mse: 39.1753\n","Epoch 3/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.0810 - mse: 11.8083 \n","Epoch 3: val_loss improved from 47.31407 to 15.04650, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.4686 - mse: 11.1665 - val_loss: 15.0465 - val_mse: 6.4552\n","Epoch 4/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.5134 - mse: 5.9146 \n","Epoch 4: val_loss improved from 15.04650 to 12.71673, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.4532 - mse: 5.8567 - val_loss: 12.7167 - val_mse: 4.1918\n","Epoch 5/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2895 - mse: 4.8062 \n","Epoch 5: val_loss improved from 12.71673 to 8.99031, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.1035 - mse: 4.6420 - val_loss: 8.9903 - val_mse: 0.7760\n","Epoch 6/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.7832 - mse: 3.6352 \n","Epoch 6: val_loss improved from 8.99031 to 8.11399, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.6790 - mse: 3.5673 - val_loss: 8.1140 - val_mse: 0.3345\n","Epoch 7/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3473 - mse: 2.6500 \n","Epoch 7: val_loss improved from 8.11399 to 7.48509, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.3268 - mse: 2.6658 - val_loss: 7.4851 - val_mse: 0.1866\n","Epoch 8/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1036 - mse: 2.8943 \n","Epoch 8: val_loss improved from 7.48509 to 7.00678, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.0027 - mse: 2.8257 - val_loss: 7.0068 - val_mse: 0.2000\n","Epoch 9/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6804 - mse: 2.9634 \n","Epoch 9: val_loss improved from 7.00678 to 6.44712, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.6305 - mse: 2.9456 - val_loss: 6.4471 - val_mse: 0.1270\n","Epoch 10/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5693 - mse: 2.3370 \n","Epoch 10: val_loss improved from 6.44712 to 5.91999, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5526 - mse: 2.3518 - val_loss: 5.9200 - val_mse: 0.0771\n","Epoch 11/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2142 - mse: 2.4602 \n","Epoch 11: val_loss improved from 5.91999 to 5.49072, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1485 - mse: 2.4204 - val_loss: 5.4907 - val_mse: 0.1075\n","Epoch 12/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3038 - mse: 1.9993 \n","Epoch 12: val_loss improved from 5.49072 to 5.07722, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2886 - mse: 2.0180 - val_loss: 5.0772 - val_mse: 0.1390\n","Epoch 13/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7202 - mse: 1.8520 \n","Epoch 13: val_loss improved from 5.07722 to 4.62782, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7128 - mse: 1.8808 - val_loss: 4.6278 - val_mse: 0.1120\n","Epoch 14/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6454 - mse: 2.2004 \n","Epoch 14: val_loss improved from 4.62782 to 4.20206, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5705 - mse: 2.1557 - val_loss: 4.2021 - val_mse: 0.0825\n","Epoch 15/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0048 - mse: 1.9535 \n","Epoch 15: val_loss improved from 4.20206 to 3.84484, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9462 - mse: 1.9193 - val_loss: 3.8448 - val_mse: 0.0933\n","Epoch 16/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4910 - mse: 1.8046 \n","Epoch 16: val_loss improved from 3.84484 to 3.46639, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.4499 - mse: 1.7867 - val_loss: 3.4664 - val_mse: 0.0611\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2399 - mse: 1.8939 \n","Epoch 17: val_loss improved from 3.46639 to 3.21357, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.2067 - mse: 1.8819 - val_loss: 3.2136 - val_mse: 0.1260\n","Epoch 18/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2135 - mse: 2.1812 \n","Epoch 18: val_loss improved from 3.21357 to 2.87124, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1575 - mse: 2.1446 - val_loss: 2.8712 - val_mse: 0.0750\n","Epoch 19/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5998 - mse: 1.8562 \n","Epoch 19: val_loss improved from 2.87124 to 2.60432, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5932 - mse: 1.8649 - val_loss: 2.6043 - val_mse: 0.0771\n","Epoch 20/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5000 - mse: 2.0188 \n","Epoch 20: val_loss improved from 2.60432 to 2.35082, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4642 - mse: 1.9991 - val_loss: 2.3508 - val_mse: 0.0660\n","Epoch 21/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0914 - mse: 1.8442 \n","Epoch 21: val_loss improved from 2.35082 to 2.13045, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0368 - mse: 1.8090 - val_loss: 2.1304 - val_mse: 0.0712\n","Epoch 22/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7652 - mse: 1.7411 \n","Epoch 22: val_loss improved from 2.13045 to 1.90872, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.7573 - mse: 1.7509 - val_loss: 1.9087 - val_mse: 0.0525\n","Epoch 23/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4617 - mse: 1.6371 \n","Epoch 23: val_loss improved from 1.90872 to 1.75168, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4579 - mse: 1.6471 - val_loss: 1.7517 - val_mse: 0.0768\n","Epoch 24/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5235 - mse: 1.8809 \n","Epoch 24: val_loss improved from 1.75168 to 1.57184, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5058 - mse: 1.8743 - val_loss: 1.5718 - val_mse: 0.0614\n","Epoch 25/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2127 - mse: 1.7313 \n","Epoch 25: val_loss improved from 1.57184 to 1.44956, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2334 - mse: 1.7603 - val_loss: 1.4496 - val_mse: 0.0877\n","Epoch 26/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1515 - mse: 1.8145 \n","Epoch 26: val_loss improved from 1.44956 to 1.36997, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1373 - mse: 1.8089 - val_loss: 1.3700 - val_mse: 0.1330\n","Epoch 27/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9909 - mse: 1.7726 \n","Epoch 27: val_loss improved from 1.36997 to 1.22862, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9910 - mse: 1.7776 - val_loss: 1.2286 - val_mse: 0.0836\n","Epoch 28/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1890 - mse: 2.0656 \n","Epoch 28: val_loss improved from 1.22862 to 1.11320, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1358 - mse: 2.0203 - val_loss: 1.1132 - val_mse: 0.0887\n","Epoch 29/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7920 - mse: 1.7857 \n","Epoch 29: val_loss improved from 1.11320 to 1.00676, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7756 - mse: 1.7755 - val_loss: 1.0068 - val_mse: 0.0765\n","Epoch 30/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8102 - mse: 1.8967 \n","Epoch 30: val_loss improved from 1.00676 to 0.92349, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7673 - mse: 1.8611 - val_loss: 0.9235 - val_mse: 0.0898\n","Epoch 31/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4834 - mse: 1.6627 \n","Epoch 31: val_loss improved from 0.92349 to 0.84119, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4989 - mse: 1.6831 - val_loss: 0.8412 - val_mse: 0.0684\n","Epoch 32/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5789 - mse: 1.8164 \n","Epoch 32: val_loss improved from 0.84119 to 0.79304, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5227 - mse: 1.7650 - val_loss: 0.7930 - val_mse: 0.0843\n","Epoch 33/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1940 - mse: 1.4955 \n","Epoch 33: val_loss improved from 0.79304 to 0.71592, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2280 - mse: 1.5347 - val_loss: 0.7159 - val_mse: 0.0672\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3955 - mse: 1.7557 \n","Epoch 34: val_loss improved from 0.71592 to 0.66317, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4084 - mse: 1.7714 - val_loss: 0.6632 - val_mse: 0.0547\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4247 - mse: 1.8218 \n","Epoch 35: val_loss did not improve from 0.66317\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4053 - mse: 1.8042 - val_loss: 0.7176 - val_mse: 0.1353\n","Epoch 36/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5572 - mse: 1.9817 \n","Epoch 36: val_loss improved from 0.66317 to 0.59403, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5179 - mse: 1.9460 - val_loss: 0.5940 - val_mse: 0.0588\n","Epoch 37/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1201 - mse: 1.5927 \n","Epoch 37: val_loss did not improve from 0.59403\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1250 - mse: 1.6010 - val_loss: 0.5964 - val_mse: 0.1055\n","Epoch 38/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1219 - mse: 1.6367 \n","Epoch 38: val_loss improved from 0.59403 to 0.53873, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1150 - mse: 1.6320 - val_loss: 0.5387 - val_mse: 0.0665\n","Epoch 39/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2078 - mse: 1.7393 \n","Epoch 39: val_loss improved from 0.53873 to 0.49640, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2390 - mse: 1.7735 - val_loss: 0.4964 - val_mse: 0.0631\n","Epoch 40/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9472 - mse: 1.5199 \n","Epoch 40: val_loss improved from 0.49640 to 0.45598, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9834 - mse: 1.5583 - val_loss: 0.4560 - val_mse: 0.0474\n","Epoch 41/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8501 - mse: 1.4461 \n","Epoch 41: val_loss did not improve from 0.45598\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9128 - mse: 1.5105 - val_loss: 0.4703 - val_mse: 0.0820\n","Epoch 42/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0282 - mse: 1.6390 \n","Epoch 42: val_loss improved from 0.45598 to 0.45315, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0133 - mse: 1.6236 - val_loss: 0.4532 - val_mse: 0.0636\n","Epoch 43/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0173 - mse: 1.6317 \n","Epoch 43: val_loss improved from 0.45315 to 0.43772, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9972 - mse: 1.6137 - val_loss: 0.4377 - val_mse: 0.0762\n","Epoch 44/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8534 - mse: 1.4949 \n","Epoch 44: val_loss improved from 0.43772 to 0.39111, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8939 - mse: 1.5371 - val_loss: 0.3911 - val_mse: 0.0604\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9290 - mse: 1.6019 \n","Epoch 45: val_loss improved from 0.39111 to 0.36400, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9511 - mse: 1.6246 - val_loss: 0.3640 - val_mse: 0.0443\n","Epoch 46/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9580 - mse: 1.6422 \n","Epoch 46: val_loss did not improve from 0.36400\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9303 - mse: 1.6156 - val_loss: 0.3954 - val_mse: 0.0834\n","Epoch 47/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1605 - mse: 1.8482 \n","Epoch 47: val_loss did not improve from 0.36400\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2062 - mse: 1.8934 - val_loss: 0.4119 - val_mse: 0.0811\n","Epoch 48/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7642 - mse: 1.4275 \n","Epoch 48: val_loss did not improve from 0.36400\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7731 - mse: 1.4359 - val_loss: 0.4092 - val_mse: 0.0807\n","Epoch 49/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0589 - mse: 1.7326 \n","Epoch 49: val_loss did not improve from 0.36400\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0638 - mse: 1.7380 - val_loss: 0.3800 - val_mse: 0.0577\n","Epoch 50/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8185 - mse: 1.5007 \n","Epoch 50: val_loss improved from 0.36400 to 0.33024, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8091 - mse: 1.4932 - val_loss: 0.3302 - val_mse: 0.0450\n","Epoch 51/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8118 - mse: 1.5303 \n","Epoch 51: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8292 - mse: 1.5485 - val_loss: 0.3986 - val_mse: 0.1182\n","Epoch 52/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8048 - mse: 1.5235 \n","Epoch 52: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8033 - mse: 1.5226 - val_loss: 0.3526 - val_mse: 0.0872\n","Epoch 53/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6892 - mse: 1.4250 \n","Epoch 53: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7117 - mse: 1.4470 - val_loss: 0.3811 - val_mse: 0.1097\n","Epoch 54/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8985 - mse: 1.6247 \n","Epoch 54: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8978 - mse: 1.6231 - val_loss: 0.3597 - val_mse: 0.0717\n","Epoch 55/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8102 - mse: 1.5171 \n","Epoch 55: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8186 - mse: 1.5247 - val_loss: 0.3626 - val_mse: 0.0629\n","Epoch 56/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7309 - mse: 1.4292 \n","Epoch 56: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7757 - mse: 1.4750 - val_loss: 0.3745 - val_mse: 0.1010\n","Epoch 57/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8950 - mse: 1.6310 \n","Epoch 57: val_loss improved from 0.33024 to 0.28853, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8659 - mse: 1.6040 - val_loss: 0.2885 - val_mse: 0.0505\n","Epoch 58/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6676 - mse: 1.4324 \n","Epoch 58: val_loss did not improve from 0.28853\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7104 - mse: 1.4748 - val_loss: 0.2980 - val_mse: 0.0505\n","Epoch 59/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9624 - mse: 1.7151 \n","Epoch 59: val_loss did not improve from 0.28853\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9081 - mse: 1.6609 - val_loss: 0.3085 - val_mse: 0.0584\n","Epoch 60/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6312 - mse: 1.3799 \n","Epoch 60: val_loss improved from 0.28853 to 0.27520, saving model to best_model_Nadam_lr_0.01_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6532 - mse: 1.4031 - val_loss: 0.2752 - val_mse: 0.0466\n","Epoch 61/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6637 - mse: 1.4328 \n","Epoch 61: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6810 - mse: 1.4459 - val_loss: 0.3356 - val_mse: 0.0462\n","Epoch 62/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8717 - mse: 1.5816 \n","Epoch 62: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8490 - mse: 1.5595 - val_loss: 0.3558 - val_mse: 0.0767\n","Epoch 63/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8220 - mse: 1.5472 \n","Epoch 63: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8140 - mse: 1.5405 - val_loss: 0.3275 - val_mse: 0.0665\n","Epoch 64/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8712 - mse: 1.6076 \n","Epoch 64: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8437 - mse: 1.5784 - val_loss: 0.3394 - val_mse: 0.0566\n","Epoch 65/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9920 - mse: 1.7024 \n","Epoch 65: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9478 - mse: 1.6570 - val_loss: 0.3321 - val_mse: 0.0483\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9499 - mse: 1.6734 \n","Epoch 66: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9399 - mse: 1.6646 - val_loss: 0.3313 - val_mse: 0.0657\n","Epoch 67/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9512 - mse: 1.6905 \n","Epoch 67: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9226 - mse: 1.6632 - val_loss: 0.3171 - val_mse: 0.0655\n","Epoch 68/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5020 - mse: 1.2459 \n","Epoch 68: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5055 - mse: 1.2492 - val_loss: 0.3323 - val_mse: 0.0701\n","Epoch 69/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8259 - mse: 1.5644 \n","Epoch 69: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7793 - mse: 1.5197 - val_loss: 0.2984 - val_mse: 0.0631\n","Epoch 70/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7135 - mse: 1.4812 \n","Epoch 70: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7544 - mse: 1.5228 - val_loss: 0.3052 - val_mse: 0.0789\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: Nadam, Learning Rate: 0.01, Epochs: 100, Test MSE: 0.06353791964770494\n","Training with optimizer: Nadam, learning rate: 0.1, epochs: 100\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 95.0734 - mse: 66.5764\n","Epoch 1: val_loss improved from inf to 481.95038, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 329ms/step - loss: 94.1131 - mse: 65.1963 - val_loss: 481.9504 - val_mse: 432.2494\n","Epoch 2/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 46.9349 - mse: 2.5694 \n","Epoch 2: val_loss improved from 481.95038 to 21.77613, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 44.7942 - mse: 2.5094 - val_loss: 21.7761 - val_mse: 1.6410\n","Epoch 3/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18.9329 - mse: 1.9874 \n","Epoch 3: val_loss improved from 21.77613 to 7.72559, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.8525 - mse: 1.9973 - val_loss: 7.7256 - val_mse: 0.2297\n","Epoch 4/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.4350 - mse: 1.6863 \n","Epoch 4: val_loss improved from 7.72559 to 4.78343, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2655 - mse: 1.7078 - val_loss: 4.7834 - val_mse: 0.3667\n","Epoch 5/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0182 - mse: 1.9454 \n","Epoch 5: val_loss improved from 4.78343 to 3.90366, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9325 - mse: 1.9199 - val_loss: 3.9037 - val_mse: 0.2706\n","Epoch 6/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6529 - mse: 1.8840 \n","Epoch 6: val_loss improved from 3.90366 to 3.40294, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6515 - mse: 1.8567 - val_loss: 3.4029 - val_mse: 0.1511\n","Epoch 7/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5299 - mse: 1.5369 \n","Epoch 7: val_loss did not improve from 3.40294\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5892 - mse: 1.5437 - val_loss: 4.4986 - val_mse: 0.1991\n","Epoch 8/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0270 - mse: 1.5776 \n","Epoch 8: val_loss did not improve from 3.40294\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.9628 - mse: 1.5535 - val_loss: 4.5173 - val_mse: 0.6776\n","Epoch 9/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4700 - mse: 1.8144 \n","Epoch 9: val_loss improved from 3.40294 to 3.16974, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.3873 - mse: 1.7915 - val_loss: 3.1697 - val_mse: 0.2885\n","Epoch 10/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4848 - mse: 1.4701 \n","Epoch 10: val_loss did not improve from 3.16974\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6653 - mse: 1.4798 - val_loss: 4.0313 - val_mse: 0.1209\n","Epoch 11/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0512 - mse: 1.5242 \n","Epoch 11: val_loss did not improve from 3.16974\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.0445 - mse: 1.5033 - val_loss: 3.8268 - val_mse: 0.2713\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9818 - mse: 1.3178 \n","Epoch 12: val_loss did not improve from 3.16974\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.0100 - mse: 1.3235 - val_loss: 3.9629 - val_mse: 0.2252\n","Epoch 13/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8375 - mse: 1.0983 \n","Epoch 13: val_loss did not improve from 3.16974\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.8169 - mse: 1.1374 - val_loss: 3.4478 - val_mse: 0.1490\n","Epoch 14/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4450 - mse: 1.2755 \n","Epoch 14: val_loss improved from 3.16974 to 2.95837, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4393 - mse: 1.2635 - val_loss: 2.9584 - val_mse: 0.3168\n","Epoch 15/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7184 - mse: 1.2128 \n","Epoch 15: val_loss improved from 2.95837 to 2.50973, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6595 - mse: 1.1839 - val_loss: 2.5097 - val_mse: 0.1382\n","Epoch 16/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1821 - mse: 1.0529 \n","Epoch 16: val_loss improved from 2.50973 to 2.37773, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1111 - mse: 1.0465 - val_loss: 2.3777 - val_mse: 0.1613\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0598 - mse: 1.0146 \n","Epoch 17: val_loss did not improve from 2.37773\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1045 - mse: 1.0064 - val_loss: 2.9445 - val_mse: 0.1820\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9029 - mse: 1.0787 \n","Epoch 18: val_loss did not improve from 2.37773\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8657 - mse: 1.0654 - val_loss: 4.5760 - val_mse: 0.7453\n","Epoch 19/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4202 - mse: 1.0505 \n","Epoch 19: val_loss improved from 2.37773 to 2.11551, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2513 - mse: 1.0316 - val_loss: 2.1155 - val_mse: 0.1756\n","Epoch 20/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6994 - mse: 0.9555 \n","Epoch 20: val_loss improved from 2.11551 to 1.85204, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6727 - mse: 0.9401 - val_loss: 1.8520 - val_mse: 0.0786\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4127 - mse: 0.7697 \n","Epoch 21: val_loss did not improve from 1.85204\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3785 - mse: 0.7657 - val_loss: 1.9253 - val_mse: 0.0755\n","Epoch 22/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7168 - mse: 0.6909 \n","Epoch 22: val_loss improved from 1.85204 to 1.77729, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7590 - mse: 0.6962 - val_loss: 1.7773 - val_mse: 0.1013\n","Epoch 23/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0720 - mse: 0.6112 \n","Epoch 23: val_loss improved from 1.77729 to 1.59213, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0523 - mse: 0.6134 - val_loss: 1.5921 - val_mse: 0.1895\n","Epoch 24/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3736 - mse: 0.6942 \n","Epoch 24: val_loss did not improve from 1.59213\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3836 - mse: 0.6910 - val_loss: 1.8684 - val_mse: 0.2692\n","Epoch 25/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1002 - mse: 0.6489 \n","Epoch 25: val_loss improved from 1.59213 to 1.52480, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1005 - mse: 0.6539 - val_loss: 1.5248 - val_mse: 0.0781\n","Epoch 26/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8795 - mse: 0.5558 \n","Epoch 26: val_loss improved from 1.52480 to 1.47401, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8697 - mse: 0.5559 - val_loss: 1.4740 - val_mse: 0.0825\n","Epoch 27/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0275 - mse: 0.5410 \n","Epoch 27: val_loss improved from 1.47401 to 1.16301, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0310 - mse: 0.5415 - val_loss: 1.1630 - val_mse: 0.0740\n","Epoch 28/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6143 - mse: 0.5512 \n","Epoch 28: val_loss did not improve from 1.16301\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5985 - mse: 0.5374 - val_loss: 1.2020 - val_mse: 0.0713\n","Epoch 29/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6207 - mse: 0.4597 \n","Epoch 29: val_loss did not improve from 1.16301\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6115 - mse: 0.4533 - val_loss: 1.6462 - val_mse: 0.1221\n","Epoch 30/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7958 - mse: 0.4607 \n","Epoch 30: val_loss improved from 1.16301 to 1.06061, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7203 - mse: 0.4444 - val_loss: 1.0606 - val_mse: 0.0597\n","Epoch 31/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1936 - mse: 0.3349 \n","Epoch 31: val_loss improved from 1.06061 to 0.95400, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2043 - mse: 0.3411 - val_loss: 0.9540 - val_mse: 0.1249\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1110 - mse: 0.3774 \n","Epoch 32: val_loss improved from 0.95400 to 0.77914, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0868 - mse: 0.3649 - val_loss: 0.7791 - val_mse: 0.0831\n","Epoch 33/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9439 - mse: 0.2800 \n","Epoch 33: val_loss improved from 0.77914 to 0.60994, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9501 - mse: 0.2853 - val_loss: 0.6099 - val_mse: 0.0576\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8044 - mse: 0.2886 \n","Epoch 34: val_loss did not improve from 0.60994\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8324 - mse: 0.2928 - val_loss: 1.9059 - val_mse: 0.1461\n","Epoch 35/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8302 - mse: 0.3000 \n","Epoch 35: val_loss did not improve from 0.60994\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7373 - mse: 0.2952 - val_loss: 0.6257 - val_mse: 0.0652\n","Epoch 36/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8132 - mse: 0.2697 \n","Epoch 36: val_loss improved from 0.60994 to 0.53688, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8121 - mse: 0.2663 - val_loss: 0.5369 - val_mse: 0.0684\n","Epoch 37/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6624 - mse: 0.2074 \n","Epoch 37: val_loss did not improve from 0.53688\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6836 - mse: 0.2096 - val_loss: 0.6224 - val_mse: 0.0708\n","Epoch 38/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7904 - mse: 0.2278 \n","Epoch 38: val_loss did not improve from 0.53688\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7980 - mse: 0.2238 - val_loss: 0.7121 - val_mse: 0.0809\n","Epoch 39/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7496 - mse: 0.1935 \n","Epoch 39: val_loss did not improve from 0.53688\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7434 - mse: 0.1959 - val_loss: 0.6082 - val_mse: 0.0790\n","Epoch 40/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6372 - mse: 0.1675 \n","Epoch 40: val_loss did not improve from 0.53688\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6361 - mse: 0.1692 - val_loss: 0.5432 - val_mse: 0.0740\n","Epoch 41/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5647 - mse: 0.1474 \n","Epoch 41: val_loss improved from 0.53688 to 0.43713, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5721 - mse: 0.1527 - val_loss: 0.4371 - val_mse: 0.0486\n","Epoch 42/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4874 - mse: 0.1468 \n","Epoch 42: val_loss improved from 0.43713 to 0.38521, saving model to best_model_Nadam_lr_0.1_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4803 - mse: 0.1443 - val_loss: 0.3852 - val_mse: 0.1119\n","Epoch 43/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4344 - mse: 0.1392 \n","Epoch 43: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4430 - mse: 0.1372 - val_loss: 0.4424 - val_mse: 0.0753\n","Epoch 44/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5069 - mse: 0.1282 \n","Epoch 44: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5077 - mse: 0.1304 - val_loss: 0.4609 - val_mse: 0.0553\n","Epoch 45/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5340 - mse: 0.1275 \n","Epoch 45: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5224 - mse: 0.1284 - val_loss: 0.3952 - val_mse: 0.0566\n","Epoch 46/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4369 - mse: 0.1233 \n","Epoch 46: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4314 - mse: 0.1198 - val_loss: 0.4526 - val_mse: 0.0502\n","Epoch 47/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5220 - mse: 0.1268 \n","Epoch 47: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5346 - mse: 0.1263 - val_loss: 0.4112 - val_mse: 0.0422\n","Epoch 48/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4428 - mse: 0.1115 \n","Epoch 48: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4429 - mse: 0.1097 - val_loss: 0.4128 - val_mse: 0.0521\n","Epoch 49/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4934 - mse: 0.1147 \n","Epoch 49: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4937 - mse: 0.1124 - val_loss: 0.3930 - val_mse: 0.0376\n","Epoch 50/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3982 - mse: 0.0861 \n","Epoch 50: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3955 - mse: 0.0863 - val_loss: 0.4293 - val_mse: 0.0606\n","Epoch 51/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4298 - mse: 0.0740 \n","Epoch 51: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4315 - mse: 0.0761 - val_loss: 0.5041 - val_mse: 0.0914\n","Epoch 52/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4107 - mse: 0.0847 \n","Epoch 52: val_loss did not improve from 0.38521\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4014 - mse: 0.0841 - val_loss: 0.4965 - val_mse: 0.0685\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: Nadam, Learning Rate: 0.1, Epochs: 100, Test MSE: 0.1158791359705946\n"]}],"source":["# Additional training with different learning rates for 100 epochs\n","for opt in optimizers:\n","    opt_name = opt.__name__  # Access the class name directly\n","    for lr in learning_rates:\n","        print(f\"Training with optimizer: {opt_name}, learning rate: {lr}, epochs: 100\")\n","        \n","        set_seed(seed_value)  # Reinitialize the seed before creating the model\n","        optimizer_instance = opt(learning_rate=lr)  # Set learning rate for the optimizer\n","        model = create_model(optimizer=optimizer_instance, loss='mse')\n","        \n","        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","        model_checkpoint = ModelCheckpoint(f'best_model_{opt_name}_lr_{lr}_epochs_100.keras', monitor='val_loss', save_best_only=True, verbose=1)\n","        \n","        history = model.fit(X_train_scaled, y_train_log, validation_split=0.2, epochs=100, batch_size=32, \n","                            callbacks=[early_stopping, model_checkpoint], verbose=1)\n","        \n","        # Check if the training produced NaN values\n","        if np.isnan(history.history['loss']).any() or np.isnan(history.history['val_loss']).any():\n","            print(f\"Training with optimizer: {opt_name}, learning rate: {lr}, epochs: 100 resulted in NaN values. Skipping...\")\n","            continue\n","        \n","        # Load the best model saved by ModelCheckpoint\n","        model.load_weights(f'best_model_{opt_name}_lr_{lr}_epochs_100.keras')\n","        \n","        y_pred_log = model.predict(X_val_scaled)\n","        \n","        mse = mean_squared_error(y_val_log, y_pred_log.flatten())\n","        \n","        results[opt_name][f'lr_{lr}_epochs_100'] = {'MSE': mse, 'history': history.history}\n","        \n","        print(f\"Optimizer: {opt_name}, Learning Rate: {lr}, Epochs: 100, Test MSE: {mse}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Test Optimizers Parameters"]},{"cell_type":"markdown","metadata":{},"source":["We train models with different optimizers using unique parameters for each optimizer, for 100 epochs. For each optimizer in the optimizers list, we obtain its name (opt_name) and the associated unique parameters (params) from the unique_params dictionary. We display a message showing the unique optimizer and settings for 100 epochs.\n","\n","We reset the random seed to ensure reproducibility (set_seed(seed_value)). Next, we create an instance of the optimizer using the unique settings and display the optimizer settings.\n","\n","We create the model using the create_model function with the default optimizer and loss function 'mse'. We define callbacks for early stopping (EarlyStopping) and saving the best model (ModelCheckpoint). We train the model on normalized and log-transformed training data (X_train_scaled and y_train_log), using 20% cross-validation of the data, for 100 epochs.\n","\n","Next, we load the weights of the best model saved by ModelCheckpoint. We make predictions on the normalized validation data (X_val_scaled), calculate the mean square error (MSE) between the predicted and actual log-transformed values, and store the results (MSE and training history) in the results dictionary.\n","\n","Finally, we display the MSE for the optimizer with the single parameters and the 100 training epochs."]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T13:15:05.873546Z","iopub.status.busy":"2024-06-02T13:15:05.872721Z","iopub.status.idle":"2024-06-02T13:17:06.876075Z","shell.execute_reply":"2024-06-02T13:17:06.874908Z","shell.execute_reply.started":"2024-06-02T13:15:05.873511Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training with optimizer: Adam with unique parameters for 100 epochs\n","Optimizer parameters for Adam with unique parameters: {'name': 'adam', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 142.8653 - mse: 136.4975\n","Epoch 1: val_loss improved from inf to 81.46194, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 315ms/step - loss: 142.4407 - mse: 136.0451 - val_loss: 81.4619 - val_mse: 72.9032\n","Epoch 2/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 83.9420 - mse: 75.1227  \n","Epoch 2: val_loss improved from 81.46194 to 43.84258, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 77.0564 - mse: 68.1533 - val_loss: 43.8426 - val_mse: 34.3310\n","Epoch 3/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.1610 - mse: 7.6182 \n","Epoch 3: val_loss improved from 43.84258 to 17.58456, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.1666 - mse: 7.6211 - val_loss: 17.5846 - val_mse: 8.0905\n","Epoch 4/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.1215 - mse: 5.6747 \n","Epoch 4: val_loss improved from 17.58456 to 12.25440, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.9808 - mse: 5.5520 - val_loss: 12.2544 - val_mse: 3.1112\n","Epoch 5/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2925 - mse: 4.2475 \n","Epoch 5: val_loss improved from 12.25440 to 9.95440, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.1834 - mse: 4.1690 - val_loss: 9.9544 - val_mse: 1.3649\n","Epoch 6/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.9410 - mse: 3.4627 \n","Epoch 6: val_loss improved from 9.95440 to 8.18243, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.9042 - mse: 3.4663 - val_loss: 8.1824 - val_mse: 0.2067\n","Epoch 7/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6273 - mse: 2.7722 \n","Epoch 7: val_loss improved from 8.18243 to 7.45187, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.5939 - mse: 2.7745 - val_loss: 7.4519 - val_mse: 0.1078\n","Epoch 8/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.2407 - mse: 3.0165 \n","Epoch 8: val_loss improved from 7.45187 to 6.83890, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.1529 - mse: 2.9637 - val_loss: 6.8389 - val_mse: 0.1131\n","Epoch 9/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1807 - mse: 2.5719 \n","Epoch 9: val_loss improved from 6.83890 to 6.20839, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.1762 - mse: 2.6012 - val_loss: 6.2084 - val_mse: 0.0804\n","Epoch 10/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2549 - mse: 2.2390 \n","Epoch 10: val_loss improved from 6.20839 to 5.61595, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2425 - mse: 2.2591 - val_loss: 5.6160 - val_mse: 0.0590\n","Epoch 11/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1579 - mse: 2.6932 \n","Epoch 11: val_loss improved from 5.61595 to 5.15990, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0070 - mse: 2.5820 - val_loss: 5.1599 - val_mse: 0.1257\n","Epoch 12/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8791 - mse: 1.9337 \n","Epoch 12: val_loss improved from 5.15990 to 4.64085, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8603 - mse: 1.9529 - val_loss: 4.6409 - val_mse: 0.1036\n","Epoch 13/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2657 - mse: 1.8136 \n","Epoch 13: val_loss improved from 4.64085 to 4.23215, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2669 - mse: 1.8450 - val_loss: 4.2322 - val_mse: 0.1492\n","Epoch 14/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1854 - mse: 2.1807 \n","Epoch 14: val_loss improved from 4.23215 to 3.74521, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1214 - mse: 2.1442 - val_loss: 3.7452 - val_mse: 0.0782\n","Epoch 15/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5535 - mse: 1.9627 \n","Epoch 15: val_loss improved from 3.74521 to 3.41870, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5246 - mse: 1.9555 - val_loss: 3.4187 - val_mse: 0.1348\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0374 - mse: 1.8240 \n","Epoch 16: val_loss improved from 3.41870 to 2.98926, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0017 - mse: 1.8086 - val_loss: 2.9893 - val_mse: 0.0624\n","Epoch 17/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7912 - mse: 1.9271 \n","Epoch 17: val_loss improved from 2.98926 to 2.78596, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7563 - mse: 1.9101 - val_loss: 2.7860 - val_mse: 0.1767\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6594 - mse: 2.1071 \n","Epoch 18: val_loss improved from 2.78596 to 2.40456, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6127 - mse: 2.0766 - val_loss: 2.4046 - val_mse: 0.0808\n","Epoch 19/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1945 - mse: 1.9187 \n","Epoch 19: val_loss improved from 2.40456 to 2.15629, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1756 - mse: 1.9165 - val_loss: 2.1563 - val_mse: 0.0838\n","Epoch 20/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0525 - mse: 2.0210 \n","Epoch 20: val_loss improved from 2.15629 to 1.93164, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0121 - mse: 1.9981 - val_loss: 1.9316 - val_mse: 0.0853\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5897 - mse: 1.7805 \n","Epoch 21: val_loss improved from 1.93164 to 1.73487, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5655 - mse: 1.7695 - val_loss: 1.7349 - val_mse: 0.0867\n","Epoch 22/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3652 - mse: 1.7513 \n","Epoch 22: val_loss improved from 1.73487 to 1.53366, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3530 - mse: 1.7513 - val_loss: 1.5337 - val_mse: 0.0676\n","Epoch 23/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1333 - mse: 1.6979 \n","Epoch 23: val_loss improved from 1.53366 to 1.44936, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1272 - mse: 1.7002 - val_loss: 1.4494 - val_mse: 0.1337\n","Epoch 24/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2177 - mse: 1.9299 \n","Epoch 24: val_loss improved from 1.44936 to 1.26388, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1999 - mse: 1.9200 - val_loss: 1.2639 - val_mse: 0.0868\n","Epoch 25/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9542 - mse: 1.8024 \n","Epoch 25: val_loss improved from 1.26388 to 1.11209, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9636 - mse: 1.8191 - val_loss: 1.1121 - val_mse: 0.0648\n","Epoch 26/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7969 - mse: 1.7726 \n","Epoch 26: val_loss improved from 1.11209 to 1.09343, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8001 - mse: 1.7819 - val_loss: 1.0934 - val_mse: 0.1516\n","Epoch 27/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6799 - mse: 1.7568 \n","Epoch 27: val_loss improved from 1.09343 to 0.93729, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6791 - mse: 1.7613 - val_loss: 0.9373 - val_mse: 0.0842\n","Epoch 28/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8619 - mse: 2.0252 \n","Epoch 28: val_loss improved from 0.93729 to 0.84350, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8177 - mse: 1.9871 - val_loss: 0.8435 - val_mse: 0.0791\n","Epoch 29/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5175 - mse: 1.7647 \n","Epoch 29: val_loss improved from 0.84350 to 0.78414, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5041 - mse: 1.7561 - val_loss: 0.7841 - val_mse: 0.0829\n","Epoch 30/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5702 - mse: 1.8786 \n","Epoch 30: val_loss improved from 0.78414 to 0.76241, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5333 - mse: 1.8461 - val_loss: 0.7624 - val_mse: 0.1217\n","Epoch 31/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2887 - mse: 1.6608 \n","Epoch 31: val_loss improved from 0.76241 to 0.66426, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3085 - mse: 1.6837 - val_loss: 0.6643 - val_mse: 0.0725\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3736 - mse: 1.7887 \n","Epoch 32: val_loss improved from 0.66426 to 0.61883, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3340 - mse: 1.7523 - val_loss: 0.6188 - val_mse: 0.0787\n","Epoch 33/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0284 - mse: 1.4981 \n","Epoch 33: val_loss improved from 0.61883 to 0.56156, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0529 - mse: 1.5260 - val_loss: 0.5616 - val_mse: 0.0662\n","Epoch 34/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2372 - mse: 1.7467 \n","Epoch 34: val_loss improved from 0.56156 to 0.51793, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2517 - mse: 1.7625 - val_loss: 0.5179 - val_mse: 0.0490\n","Epoch 35/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3159 - mse: 1.8526 \n","Epoch 35: val_loss did not improve from 0.51793\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2936 - mse: 1.8310 - val_loss: 0.5880 - val_mse: 0.1326\n","Epoch 36/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4573 - mse: 2.0076 \n","Epoch 36: val_loss improved from 0.51793 to 0.48610, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4343 - mse: 1.9861 - val_loss: 0.4861 - val_mse: 0.0647\n","Epoch 37/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0007 - mse: 1.5885 \n","Epoch 37: val_loss improved from 0.48610 to 0.47287, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0043 - mse: 1.5945 - val_loss: 0.4729 - val_mse: 0.0922\n","Epoch 38/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0091 - mse: 1.6332 \n","Epoch 38: val_loss improved from 0.47287 to 0.45464, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9957 - mse: 1.6216 - val_loss: 0.4546 - val_mse: 0.0869\n","Epoch 39/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1089 - mse: 1.7443 \n","Epoch 39: val_loss improved from 0.45464 to 0.39943, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1253 - mse: 1.7630 - val_loss: 0.3994 - val_mse: 0.0681\n","Epoch 40/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8631 - mse: 1.5345 \n","Epoch 40: val_loss improved from 0.39943 to 0.39548, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8956 - mse: 1.5673 - val_loss: 0.3955 - val_mse: 0.0704\n","Epoch 41/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7825 - mse: 1.4633 \n","Epoch 41: val_loss improved from 0.39548 to 0.36998, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8196 - mse: 1.5022 - val_loss: 0.3700 - val_mse: 0.0718\n","Epoch 42/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9398 - mse: 1.6406 \n","Epoch 42: val_loss improved from 0.36998 to 0.35682, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9270 - mse: 1.6276 - val_loss: 0.3568 - val_mse: 0.0606\n","Epoch 43/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8812 - mse: 1.5810 \n","Epoch 43: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8788 - mse: 1.5771 - val_loss: 0.3944 - val_mse: 0.0799\n","Epoch 44/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7767 - mse: 1.4643 \n","Epoch 44: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8076 - mse: 1.4961 - val_loss: 0.3799 - val_mse: 0.0822\n","Epoch 45/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9199 - mse: 1.6209 \n","Epoch 45: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9385 - mse: 1.6379 - val_loss: 0.3965 - val_mse: 0.0735\n","Epoch 46/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9973 - mse: 1.6783 \n","Epoch 46: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9629 - mse: 1.6458 - val_loss: 0.3841 - val_mse: 0.0832\n","Epoch 47/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1830 - mse: 1.8744 \n","Epoch 47: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2094 - mse: 1.8984 - val_loss: 0.4179 - val_mse: 0.0918\n","Epoch 48/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7390 - mse: 1.4149 \n","Epoch 48: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7438 - mse: 1.4219 - val_loss: 0.3666 - val_mse: 0.0780\n","Epoch 49/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0146 - mse: 1.7246 \n","Epoch 49: val_loss did not improve from 0.35682\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0260 - mse: 1.7343 - val_loss: 0.3770 - val_mse: 0.0746\n","Epoch 50/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7615 - mse: 1.4647 \n","Epoch 50: val_loss improved from 0.35682 to 0.32417, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7512 - mse: 1.4563 - val_loss: 0.3242 - val_mse: 0.0551\n","Epoch 51/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7890 - mse: 1.5209 \n","Epoch 51: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8120 - mse: 1.5432 - val_loss: 0.4276 - val_mse: 0.1451\n","Epoch 52/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7771 - mse: 1.4930 \n","Epoch 52: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7777 - mse: 1.4946 - val_loss: 0.3601 - val_mse: 0.0999\n","Epoch 53/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6718 - mse: 1.4129 \n","Epoch 53: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7043 - mse: 1.4446 - val_loss: 0.4051 - val_mse: 0.1289\n","Epoch 54/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8946 - mse: 1.6189 \n","Epoch 54: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8949 - mse: 1.6191 - val_loss: 0.3636 - val_mse: 0.0708\n","Epoch 55/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8134 - mse: 1.5153 \n","Epoch 55: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8191 - mse: 1.5212 - val_loss: 0.3513 - val_mse: 0.0667\n","Epoch 56/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7148 - mse: 1.4317 \n","Epoch 56: val_loss did not improve from 0.32417\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7473 - mse: 1.4652 - val_loss: 0.3872 - val_mse: 0.1351\n","Epoch 57/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8692 - mse: 1.6285 \n","Epoch 57: val_loss improved from 0.32417 to 0.30226, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8372 - mse: 1.5985 - val_loss: 0.3023 - val_mse: 0.0737\n","Epoch 58/100\n","\u001b[1m15/26\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6246 - mse: 1.3947 \n","Epoch 58: val_loss did not improve from 0.30226\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7056 - mse: 1.4724 - val_loss: 0.3138 - val_mse: 0.0474\n","Epoch 59/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9566 - mse: 1.6908 \n","Epoch 59: val_loss improved from 0.30226 to 0.29829, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9156 - mse: 1.6512 - val_loss: 0.2983 - val_mse: 0.0556\n","Epoch 60/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6233 - mse: 1.3797 \n","Epoch 60: val_loss improved from 0.29829 to 0.27227, saving model to best_model_Adam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6457 - mse: 1.4023 - val_loss: 0.2723 - val_mse: 0.0443\n","Epoch 61/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6317 - mse: 1.4048 \n","Epoch 61: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6458 - mse: 1.4159 - val_loss: 0.3348 - val_mse: 0.0518\n","Epoch 62/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8378 - mse: 1.5480 \n","Epoch 62: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8218 - mse: 1.5328 - val_loss: 0.3363 - val_mse: 0.0709\n","Epoch 63/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7962 - mse: 1.5320 \n","Epoch 63: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7909 - mse: 1.5269 - val_loss: 0.3175 - val_mse: 0.0603\n","Epoch 64/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8260 - mse: 1.5679 \n","Epoch 64: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8085 - mse: 1.5488 - val_loss: 0.3367 - val_mse: 0.0582\n","Epoch 65/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9503 - mse: 1.6721 \n","Epoch 65: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9037 - mse: 1.6255 - val_loss: 0.3041 - val_mse: 0.0441\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8833 - mse: 1.6336 \n","Epoch 66: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8728 - mse: 1.6222 - val_loss: 0.4771 - val_mse: 0.1414\n","Epoch 67/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0285 - mse: 1.6532 \n","Epoch 67: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0130 - mse: 1.6333 - val_loss: 0.4953 - val_mse: 0.0928\n","Epoch 68/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6163 - mse: 1.2256 \n","Epoch 68: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6189 - mse: 1.2313 - val_loss: 0.4602 - val_mse: 0.1120\n","Epoch 69/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8791 - mse: 1.5411 \n","Epoch 69: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8518 - mse: 1.5174 - val_loss: 0.3361 - val_mse: 0.0684\n","Epoch 70/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7522 - mse: 1.4920 \n","Epoch 70: val_loss did not improve from 0.27227\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7853 - mse: 1.5260 - val_loss: 0.3172 - val_mse: 0.0639\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: Adam, Unique Parameters, Epochs: 100, Test MSE: 0.05699531309437809\n","Training with optimizer: SGD with unique parameters for 100 epochs\n","Optimizer parameters for SGD with unique parameters: {'name': 'SGD', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'momentum': 0.9, 'nesterov': True}\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - loss: 58.2808 - mse: 50.1441\n","Epoch 1: val_loss improved from inf to 17.98398, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 326ms/step - loss: 57.2313 - mse: 48.9957 - val_loss: 17.9840 - val_mse: 2.6592\n","Epoch 2/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.6554 - mse: 1.0508 \n","Epoch 2: val_loss improved from 17.98398 to 17.68482, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.6711 - mse: 1.0401 - val_loss: 17.6848 - val_mse: 2.3319\n","Epoch 3/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.7107 - mse: 0.6362 \n","Epoch 3: val_loss improved from 17.68482 to 14.96054, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.6488 - mse: 0.6411 - val_loss: 14.9605 - val_mse: 1.0403\n","Epoch 4/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.1509 - mse: 0.5054 \n","Epoch 4: val_loss improved from 14.96054 to 12.68857, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.0615 - mse: 0.4951 - val_loss: 12.6886 - val_mse: 0.1574\n","Epoch 5/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.7317 - mse: 0.4626 \n","Epoch 5: val_loss improved from 12.68857 to 11.87815, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.6587 - mse: 0.4489 - val_loss: 11.8781 - val_mse: 0.6039\n","Epoch 6/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.4723 - mse: 0.4341 \n","Epoch 6: val_loss improved from 11.87815 to 10.21100, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.3976 - mse: 0.4128 - val_loss: 10.2110 - val_mse: 0.0684\n","Epoch 7/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.2061 - mse: 0.2759 \n","Epoch 7: val_loss improved from 10.21100 to 9.17365, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.1581 - mse: 0.2759 - val_loss: 9.1737 - val_mse: 0.0492\n","Epoch 8/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1669 - mse: 0.2245 \n","Epoch 8: val_loss improved from 9.17365 to 8.53355, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.1128 - mse: 0.2226 - val_loss: 8.5335 - val_mse: 0.3251\n","Epoch 9/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2492 - mse: 0.2046 \n","Epoch 9: val_loss improved from 8.53355 to 7.45663, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2038 - mse: 0.2060 - val_loss: 7.4566 - val_mse: 0.0722\n","Epoch 10/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4312 - mse: 0.1797 \n","Epoch 10: val_loss improved from 7.45663 to 6.71632, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3752 - mse: 0.1804 - val_loss: 6.7163 - val_mse: 0.0732\n","Epoch 11/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6559 - mse: 0.1519 \n","Epoch 11: val_loss improved from 6.71632 to 6.04886, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6254 - mse: 0.1528 - val_loss: 6.0489 - val_mse: 0.0726\n","Epoch 12/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9962 - mse: 0.1451 \n","Epoch 12: val_loss improved from 6.04886 to 5.49668, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9694 - mse: 0.1466 - val_loss: 5.4967 - val_mse: 0.1205\n","Epoch 13/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3944 - mse: 0.1307 \n","Epoch 13: val_loss improved from 5.49668 to 4.95063, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3712 - mse: 0.1331 - val_loss: 4.9506 - val_mse: 0.1141\n","Epoch 14/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8387 - mse: 0.1035 \n","Epoch 14: val_loss improved from 4.95063 to 4.47233, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8141 - mse: 0.1018 - val_loss: 4.4723 - val_mse: 0.1214\n","Epoch 15/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3802 - mse: 0.1204 \n","Epoch 15: val_loss improved from 4.47233 to 4.00487, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3578 - mse: 0.1186 - val_loss: 4.0049 - val_mse: 0.0907\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9406 - mse: 0.1046 \n","Epoch 16: val_loss improved from 4.00487 to 3.60662, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9190 - mse: 0.1054 - val_loss: 3.6066 - val_mse: 0.0854\n","Epoch 17/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5259 - mse: 0.0785 \n","Epoch 17: val_loss improved from 3.60662 to 3.26301, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5098 - mse: 0.0790 - val_loss: 3.2630 - val_mse: 0.0953\n","Epoch 18/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2163 - mse: 0.1149 \n","Epoch 18: val_loss improved from 3.26301 to 2.93106, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1993 - mse: 0.1129 - val_loss: 2.9311 - val_mse: 0.0813\n","Epoch 19/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8956 - mse: 0.0999 \n","Epoch 19: val_loss improved from 2.93106 to 2.64835, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8745 - mse: 0.0979 - val_loss: 2.6484 - val_mse: 0.0846\n","Epoch 20/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6052 - mse: 0.0901 \n","Epoch 20: val_loss improved from 2.64835 to 2.38743, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5871 - mse: 0.0892 - val_loss: 2.3874 - val_mse: 0.0810\n","Epoch 21/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3414 - mse: 0.0810 \n","Epoch 21: val_loss improved from 2.38743 to 2.16188, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3290 - mse: 0.0817 - val_loss: 2.1619 - val_mse: 0.0869\n","Epoch 22/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1116 - mse: 0.0800 \n","Epoch 22: val_loss improved from 2.16188 to 1.95938, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1015 - mse: 0.0797 - val_loss: 1.9594 - val_mse: 0.0926\n","Epoch 23/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9115 - mse: 0.0838 \n","Epoch 23: val_loss improved from 1.95938 to 1.81625, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9024 - mse: 0.0835 - val_loss: 1.8163 - val_mse: 0.1368\n","Epoch 24/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7147 - mse: 0.0704 \n","Epoch 24: val_loss improved from 1.81625 to 1.64875, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7071 - mse: 0.0707 - val_loss: 1.6488 - val_mse: 0.1378\n","Epoch 25/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5444 - mse: 0.0665 \n","Epoch 25: val_loss improved from 1.64875 to 1.44610, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5390 - mse: 0.0668 - val_loss: 1.4461 - val_mse: 0.0867\n","Epoch 26/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3958 - mse: 0.0648 \n","Epoch 26: val_loss improved from 1.44610 to 1.29994, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.3902 - mse: 0.0656 - val_loss: 1.2999 - val_mse: 0.0769\n","Epoch 27/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2673 - mse: 0.0698 \n","Epoch 27: val_loss improved from 1.29994 to 1.19255, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2604 - mse: 0.0687 - val_loss: 1.1926 - val_mse: 0.0921\n","Epoch 28/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1307 - mse: 0.0512 \n","Epoch 28: val_loss improved from 1.19255 to 1.10076, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.1242 - mse: 0.0520 - val_loss: 1.1008 - val_mse: 0.1107\n","Epoch 29/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0327 - mse: 0.0623 \n","Epoch 29: val_loss improved from 1.10076 to 0.97179, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0265 - mse: 0.0617 - val_loss: 0.9718 - val_mse: 0.0809\n","Epoch 30/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9332 - mse: 0.0600 \n","Epoch 30: val_loss improved from 0.97179 to 0.87199, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9285 - mse: 0.0605 - val_loss: 0.8720 - val_mse: 0.0703\n","Epoch 31/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8515 - mse: 0.0665 \n","Epoch 31: val_loss improved from 0.87199 to 0.82790, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8470 - mse: 0.0659 - val_loss: 0.8279 - val_mse: 0.1065\n","Epoch 32/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7735 - mse: 0.0671 \n","Epoch 32: val_loss improved from 0.82790 to 0.75095, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7698 - mse: 0.0668 - val_loss: 0.7509 - val_mse: 0.1017\n","Epoch 33/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6920 - mse: 0.0563 \n","Epoch 33: val_loss improved from 0.75095 to 0.69810, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6899 - mse: 0.0573 - val_loss: 0.6981 - val_mse: 0.1137\n","Epoch 34/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6237 - mse: 0.0520 \n","Epoch 34: val_loss improved from 0.69810 to 0.66005, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6220 - mse: 0.0526 - val_loss: 0.6600 - val_mse: 0.1340\n","Epoch 35/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5776 - mse: 0.0625 \n","Epoch 35: val_loss improved from 0.66005 to 0.58545, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5741 - mse: 0.0616 - val_loss: 0.5854 - val_mse: 0.1120\n","Epoch 36/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5145 - mse: 0.0509 \n","Epoch 36: val_loss improved from 0.58545 to 0.51102, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5123 - mse: 0.0509 - val_loss: 0.5110 - val_mse: 0.0848\n","Epoch 37/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4719 - mse: 0.0537 \n","Epoch 37: val_loss improved from 0.51102 to 0.47154, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4692 - mse: 0.0538 - val_loss: 0.4715 - val_mse: 0.0878\n","Epoch 38/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4373 - mse: 0.0608 \n","Epoch 38: val_loss improved from 0.47154 to 0.46076, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4348 - mse: 0.0608 - val_loss: 0.4608 - val_mse: 0.1152\n","Epoch 39/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3968 - mse: 0.0584 \n","Epoch 39: val_loss improved from 0.46076 to 0.43742, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3945 - mse: 0.0577 - val_loss: 0.4374 - val_mse: 0.1262\n","Epoch 40/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3626 - mse: 0.0578 \n","Epoch 40: val_loss improved from 0.43742 to 0.38539, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3599 - mse: 0.0566 - val_loss: 0.3854 - val_mse: 0.1051\n","Epoch 41/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3257 - mse: 0.0512 \n","Epoch 41: val_loss improved from 0.38539 to 0.33737, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3247 - mse: 0.0515 - val_loss: 0.3374 - val_mse: 0.0849\n","Epoch 42/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2994 - mse: 0.0521 \n","Epoch 42: val_loss improved from 0.33737 to 0.31321, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2976 - mse: 0.0516 - val_loss: 0.3132 - val_mse: 0.0858\n","Epoch 43/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2725 - mse: 0.0500 \n","Epoch 43: val_loss improved from 0.31321 to 0.30271, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2715 - mse: 0.0498 - val_loss: 0.3027 - val_mse: 0.0978\n","Epoch 44/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2540 - mse: 0.0529 \n","Epoch 44: val_loss improved from 0.30271 to 0.28149, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2529 - mse: 0.0532 - val_loss: 0.2815 - val_mse: 0.0967\n","Epoch 45/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2435 - mse: 0.0620 \n","Epoch 45: val_loss improved from 0.28149 to 0.25209, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2401 - mse: 0.0600 - val_loss: 0.2521 - val_mse: 0.0854\n","Epoch 46/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2143 - mse: 0.0507 \n","Epoch 46: val_loss did not improve from 0.25209\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2137 - mse: 0.0512 - val_loss: 0.2641 - val_mse: 0.1137\n","Epoch 47/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2036 - mse: 0.0557 \n","Epoch 47: val_loss improved from 0.25209 to 0.21890, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2022 - mse: 0.0554 - val_loss: 0.2189 - val_mse: 0.0830\n","Epoch 48/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1897 - mse: 0.0565 \n","Epoch 48: val_loss did not improve from 0.21890\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1881 - mse: 0.0556 - val_loss: 0.2195 - val_mse: 0.0968\n","Epoch 49/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1652 - mse: 0.0451 \n","Epoch 49: val_loss improved from 0.21890 to 0.21037, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1642 - mse: 0.0445 - val_loss: 0.2104 - val_mse: 0.0994\n","Epoch 50/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1604 - mse: 0.0516 \n","Epoch 50: val_loss improved from 0.21037 to 0.17995, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1595 - mse: 0.0513 - val_loss: 0.1800 - val_mse: 0.0797\n","Epoch 51/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1354 - mse: 0.0371 \n","Epoch 51: val_loss did not improve from 0.17995\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1358 - mse: 0.0379 - val_loss: 0.2012 - val_mse: 0.1105\n","Epoch 52/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1383 - mse: 0.0494 \n","Epoch 52: val_loss improved from 0.17995 to 0.15524, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1375 - mse: 0.0489 - val_loss: 0.1552 - val_mse: 0.0730\n","Epoch 53/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1198 - mse: 0.0391 \n","Epoch 53: val_loss did not improve from 0.15524\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1202 - mse: 0.0398 - val_loss: 0.1674 - val_mse: 0.0927\n","Epoch 54/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1138 - mse: 0.0406 \n","Epoch 54: val_loss improved from 0.15524 to 0.13532, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1137 - mse: 0.0408 - val_loss: 0.1353 - val_mse: 0.0676\n","Epoch 55/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1021 - mse: 0.0357 \n","Epoch 55: val_loss did not improve from 0.13532\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1025 - mse: 0.0363 - val_loss: 0.1540 - val_mse: 0.0925\n","Epoch 56/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1010 - mse: 0.0407 \n","Epoch 56: val_loss did not improve from 0.13532\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1009 - mse: 0.0408 - val_loss: 0.1391 - val_mse: 0.0831\n","Epoch 57/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0998 - mse: 0.0447 \n","Epoch 57: val_loss did not improve from 0.13532\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0992 - mse: 0.0444 - val_loss: 0.1416 - val_mse: 0.0906\n","Epoch 58/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0874 - mse: 0.0372 \n","Epoch 58: val_loss did not improve from 0.13532\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0880 - mse: 0.0381 - val_loss: 0.1695 - val_mse: 0.1228\n","Epoch 59/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1049 - mse: 0.0590 \n","Epoch 59: val_loss improved from 0.13532 to 0.12065, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1024 - mse: 0.0567 - val_loss: 0.1206 - val_mse: 0.0779\n","Epoch 60/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0824 - mse: 0.0404 \n","Epoch 60: val_loss improved from 0.12065 to 0.10728, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0824 - mse: 0.0406 - val_loss: 0.1073 - val_mse: 0.0680\n","Epoch 61/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0869 - mse: 0.0482 \n","Epoch 61: val_loss did not improve from 0.10728\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0858 - mse: 0.0473 - val_loss: 0.1110 - val_mse: 0.0747\n","Epoch 62/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0709 - mse: 0.0352 \n","Epoch 62: val_loss did not improve from 0.10728\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0705 - mse: 0.0350 - val_loss: 0.1200 - val_mse: 0.0866\n","Epoch 63/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0741 - mse: 0.0412 \n","Epoch 63: val_loss did not improve from 0.10728\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0736 - mse: 0.0408 - val_loss: 0.1153 - val_mse: 0.0845\n","Epoch 64/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0681 - mse: 0.0378 \n","Epoch 64: val_loss improved from 0.10728 to 0.10045, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0681 - mse: 0.0378 - val_loss: 0.1004 - val_mse: 0.0719\n","Epoch 65/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0686 - mse: 0.0405 \n","Epoch 65: val_loss did not improve from 0.10045\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0681 - mse: 0.0401 - val_loss: 0.1480 - val_mse: 0.1214\n","Epoch 66/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0692 - mse: 0.0430 \n","Epoch 66: val_loss did not improve from 0.10045\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0691 - mse: 0.0429 - val_loss: 0.1111 - val_mse: 0.0862\n","Epoch 67/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0690 - mse: 0.0444 \n","Epoch 67: val_loss did not improve from 0.10045\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0683 - mse: 0.0438 - val_loss: 0.1014 - val_mse: 0.0781\n","Epoch 68/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0622 - mse: 0.0392 \n","Epoch 68: val_loss improved from 0.10045 to 0.09216, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0621 - mse: 0.0391 - val_loss: 0.0922 - val_mse: 0.0703\n","Epoch 69/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0565 - mse: 0.0349 \n","Epoch 69: val_loss did not improve from 0.09216\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0565 - mse: 0.0349 - val_loss: 0.0967 - val_mse: 0.0763\n","Epoch 70/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0615 - mse: 0.0413 \n","Epoch 70: val_loss improved from 0.09216 to 0.06774, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0612 - mse: 0.0412 - val_loss: 0.0677 - val_mse: 0.0487\n","Epoch 71/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0541 - mse: 0.0353 \n","Epoch 71: val_loss did not improve from 0.06774\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0544 - mse: 0.0357 - val_loss: 0.0879 - val_mse: 0.0701\n","Epoch 72/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0574 - mse: 0.0397 \n","Epoch 72: val_loss did not improve from 0.06774\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0575 - mse: 0.0398 - val_loss: 0.0851 - val_mse: 0.0681\n","Epoch 73/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0597 - mse: 0.0428 \n","Epoch 73: val_loss did not improve from 0.06774\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0597 - mse: 0.0428 - val_loss: 0.0700 - val_mse: 0.0538\n","Epoch 74/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0589 - mse: 0.0429 \n","Epoch 74: val_loss did not improve from 0.06774\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0584 - mse: 0.0424 - val_loss: 0.0885 - val_mse: 0.0731\n","Epoch 75/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0570 - mse: 0.0417 \n","Epoch 75: val_loss did not improve from 0.06774\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0563 - mse: 0.0410 - val_loss: 0.0901 - val_mse: 0.0753\n","Epoch 76/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501 - mse: 0.0354 \n","Epoch 76: val_loss did not improve from 0.06774\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0503 - mse: 0.0357 - val_loss: 0.0903 - val_mse: 0.0762\n","Epoch 77/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0573 - mse: 0.0434 \n","Epoch 77: val_loss did not improve from 0.06774\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0568 - mse: 0.0429 - val_loss: 0.0733 - val_mse: 0.0598\n","Epoch 78/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0561 - mse: 0.0427 \n","Epoch 78: val_loss improved from 0.06774 to 0.06681, saving model to best_model_SGD_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0557 - mse: 0.0424 - val_loss: 0.0668 - val_mse: 0.0539\n","Epoch 79/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0483 - mse: 0.0354 \n","Epoch 79: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0485 - mse: 0.0357 - val_loss: 0.0839 - val_mse: 0.0714\n","Epoch 80/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0526 - mse: 0.0401 \n","Epoch 80: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0526 - mse: 0.0402 - val_loss: 0.0786 - val_mse: 0.0665\n","Epoch 81/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0513 - mse: 0.0393 \n","Epoch 81: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0511 - mse: 0.0390 - val_loss: 0.0678 - val_mse: 0.0560\n","Epoch 82/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0460 - mse: 0.0343 \n","Epoch 82: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0462 - mse: 0.0345 - val_loss: 0.0852 - val_mse: 0.0738\n","Epoch 83/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501 - mse: 0.0387 \n","Epoch 83: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0495 - mse: 0.0381 - val_loss: 0.0849 - val_mse: 0.0737\n","Epoch 84/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0461 - mse: 0.0349 \n","Epoch 84: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0459 - mse: 0.0347 - val_loss: 0.0776 - val_mse: 0.0666\n","Epoch 85/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0477 - mse: 0.0368 \n","Epoch 85: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0477 - mse: 0.0367 - val_loss: 0.0769 - val_mse: 0.0662\n","Epoch 86/100\n","\u001b[1m22/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506 - mse: 0.0399 \n","Epoch 86: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0499 - mse: 0.0392 - val_loss: 0.0859 - val_mse: 0.0754\n","Epoch 87/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0473 - mse: 0.0368 \n","Epoch 87: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0470 - mse: 0.0365 - val_loss: 0.0701 - val_mse: 0.0597\n","Epoch 88/100\n","\u001b[1m23/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0464 - mse: 0.0360 \n","Epoch 88: val_loss did not improve from 0.06681\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0462 - mse: 0.0359 - val_loss: 0.0760 - val_mse: 0.0658\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: SGD, Unique Parameters, Epochs: 100, Test MSE: 0.06804067128133794\n","Training with optimizer: RMSprop with unique parameters for 100 epochs\n","Optimizer parameters for RMSprop with unique parameters: {'name': 'rmsprop', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': 100, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'rho': 0.9, 'momentum': 0.9, 'epsilon': 1e-07, 'centered': True}\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - loss: 134.0215 - mse: 86.2555\n","Epoch 1: val_loss improved from inf to 14789.42676, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 351ms/step - loss: 134.2720 - mse: 84.9925 - val_loss: 14789.4268 - val_mse: 14655.6670\n","Epoch 2/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 121.7054 - mse: 8.5497  \n","Epoch 2: val_loss improved from 14789.42676 to 21.76500, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 112.7707 - mse: 7.8348 - val_loss: 21.7650 - val_mse: 0.3599\n","Epoch 3/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.7498 - mse: 2.9564 \n","Epoch 3: val_loss improved from 21.76500 to 13.50484, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16.7118 - mse: 2.8942 - val_loss: 13.5048 - val_mse: 0.1417\n","Epoch 4/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.4372 - mse: 1.7912 \n","Epoch 4: val_loss improved from 13.50484 to 7.86748, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.9604 - mse: 1.7821 - val_loss: 7.8675 - val_mse: 0.2613\n","Epoch 5/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9126 - mse: 1.6304 \n","Epoch 5: val_loss improved from 7.86748 to 5.57209, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.8854 - mse: 1.5869 - val_loss: 5.5721 - val_mse: 0.1467\n","Epoch 6/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9411 - mse: 1.4057 \n","Epoch 6: val_loss did not improve from 5.57209\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1227 - mse: 1.3765 - val_loss: 6.4698 - val_mse: 0.1607\n","Epoch 7/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2663 - mse: 1.2223 \n","Epoch 7: val_loss improved from 5.57209 to 5.55129, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.1613 - mse: 1.1923 - val_loss: 5.5513 - val_mse: 0.1546\n","Epoch 8/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9970 - mse: 0.8355 \n","Epoch 8: val_loss improved from 5.55129 to 4.78186, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0090 - mse: 0.8331 - val_loss: 4.7819 - val_mse: 0.1079\n","Epoch 9/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5821 - mse: 0.9231 \n","Epoch 9: val_loss did not improve from 4.78186\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7283 - mse: 0.9219 - val_loss: 6.5617 - val_mse: 0.2733\n","Epoch 10/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9091 - mse: 0.7504 \n","Epoch 10: val_loss did not improve from 4.78186\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.7676 - mse: 0.7566 - val_loss: 8.5332 - val_mse: 0.9020\n","Epoch 11/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2261 - mse: 0.5929 \n","Epoch 11: val_loss improved from 4.78186 to 3.45357, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7048 - mse: 0.5941 - val_loss: 3.4536 - val_mse: 0.0999\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6413 - mse: 0.3851 \n","Epoch 12: val_loss did not improve from 3.45357\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7138 - mse: 0.3857 - val_loss: 4.2691 - val_mse: 0.1007\n","Epoch 13/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4951 - mse: 0.3527 \n","Epoch 13: val_loss improved from 3.45357 to 3.09797, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2901 - mse: 0.3469 - val_loss: 3.0980 - val_mse: 0.1292\n","Epoch 14/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7621 - mse: 0.3356 \n","Epoch 14: val_loss improved from 3.09797 to 2.14877, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6728 - mse: 0.3187 - val_loss: 2.1488 - val_mse: 0.2092\n","Epoch 15/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3135 - mse: 0.2782 \n","Epoch 15: val_loss improved from 2.14877 to 2.04330, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3186 - mse: 0.2639 - val_loss: 2.0433 - val_mse: 0.0559\n","Epoch 16/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0743 - mse: 0.2084 \n","Epoch 16: val_loss did not improve from 2.04330\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1088 - mse: 0.2104 - val_loss: 2.3264 - val_mse: 0.2947\n","Epoch 17/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1326 - mse: 0.1691 \n","Epoch 17: val_loss did not improve from 2.04330\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1018 - mse: 0.1604 - val_loss: 2.1798 - val_mse: 0.0856\n","Epoch 18/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9461 - mse: 0.1315 \n","Epoch 18: val_loss improved from 2.04330 to 1.69686, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9688 - mse: 0.1330 - val_loss: 1.6969 - val_mse: 0.0615\n","Epoch 19/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6979 - mse: 0.1387 \n","Epoch 19: val_loss improved from 1.69686 to 1.20137, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6499 - mse: 0.1329 - val_loss: 1.2014 - val_mse: 0.0464\n","Epoch 20/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4710 - mse: 0.0970 \n","Epoch 20: val_loss did not improve from 1.20137\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4755 - mse: 0.0947 - val_loss: 1.5500 - val_mse: 0.1183\n","Epoch 21/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5579 - mse: 0.1004 \n","Epoch 21: val_loss did not improve from 1.20137\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5678 - mse: 0.0972 - val_loss: 1.2915 - val_mse: 0.0482\n","Epoch 22/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2002 - mse: 0.0764 \n","Epoch 22: val_loss improved from 1.20137 to 1.12509, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1922 - mse: 0.0743 - val_loss: 1.1251 - val_mse: 0.0529\n","Epoch 23/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2316 - mse: 0.0846 \n","Epoch 23: val_loss did not improve from 1.12509\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2797 - mse: 0.0858 - val_loss: 1.5385 - val_mse: 0.1648\n","Epoch 24/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4539 - mse: 0.0959 \n","Epoch 24: val_loss did not improve from 1.12509\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4677 - mse: 0.0940 - val_loss: 1.1840 - val_mse: 0.0694\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2167 - mse: 0.0812 \n","Epoch 25: val_loss did not improve from 1.12509\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2202 - mse: 0.0807 - val_loss: 1.5135 - val_mse: 0.1820\n","Epoch 26/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4398 - mse: 0.0827 \n","Epoch 26: val_loss did not improve from 1.12509\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4650 - mse: 0.0834 - val_loss: 1.6073 - val_mse: 0.0589\n","Epoch 27/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4799 - mse: 0.0819 \n","Epoch 27: val_loss did not improve from 1.12509\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4455 - mse: 0.0806 - val_loss: 1.1460 - val_mse: 0.0572\n","Epoch 28/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1163 - mse: 0.0812 \n","Epoch 28: val_loss did not improve from 1.12509\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1516 - mse: 0.0816 - val_loss: 1.2642 - val_mse: 0.0744\n","Epoch 29/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2527 - mse: 0.0803 \n","Epoch 29: val_loss did not improve from 1.12509\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2380 - mse: 0.0791 - val_loss: 1.1489 - val_mse: 0.0669\n","Epoch 30/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1191 - mse: 0.0957 \n","Epoch 30: val_loss did not improve from 1.12509\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1390 - mse: 0.0945 - val_loss: 1.4280 - val_mse: 0.0745\n","Epoch 31/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2597 - mse: 0.0863 \n","Epoch 31: val_loss improved from 1.12509 to 1.10858, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2349 - mse: 0.0855 - val_loss: 1.1086 - val_mse: 0.0426\n","Epoch 32/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1314 - mse: 0.0844 \n","Epoch 32: val_loss improved from 1.10858 to 0.92152, saving model to best_model_RMSprop_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1394 - mse: 0.0850 - val_loss: 0.9215 - val_mse: 0.0475\n","Epoch 33/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0932 - mse: 0.0841 \n","Epoch 33: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1260 - mse: 0.0827 - val_loss: 1.5615 - val_mse: 0.1598\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3013 - mse: 0.0861 \n","Epoch 34: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2883 - mse: 0.0857 - val_loss: 1.3132 - val_mse: 0.0620\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2986 - mse: 0.0873 \n","Epoch 35: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2986 - mse: 0.0872 - val_loss: 1.0832 - val_mse: 0.0513\n","Epoch 36/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1603 - mse: 0.0849 \n","Epoch 36: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1974 - mse: 0.0835 - val_loss: 1.3086 - val_mse: 0.0537\n","Epoch 37/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1768 - mse: 0.0869 \n","Epoch 37: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1529 - mse: 0.0854 - val_loss: 1.0650 - val_mse: 0.1076\n","Epoch 38/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0265 - mse: 0.0839 \n","Epoch 38: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0312 - mse: 0.0830 - val_loss: 1.1740 - val_mse: 0.0595\n","Epoch 39/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2223 - mse: 0.0838 \n","Epoch 39: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1922 - mse: 0.0806 - val_loss: 1.2100 - val_mse: 0.0415\n","Epoch 40/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1876 - mse: 0.0751 \n","Epoch 40: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1916 - mse: 0.0754 - val_loss: 1.2795 - val_mse: 0.0487\n","Epoch 41/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1825 - mse: 0.0857 \n","Epoch 41: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1863 - mse: 0.0861 - val_loss: 1.0652 - val_mse: 0.0897\n","Epoch 42/100\n","\u001b[1m13/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0761 - mse: 0.0907 \n","Epoch 42: val_loss did not improve from 0.92152\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1042 - mse: 0.0859 - val_loss: 1.0900 - val_mse: 0.0694\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","Optimizer: RMSprop, Unique Parameters, Epochs: 100, Test MSE: 0.06474062877909176\n","Training with optimizer: Nadam with unique parameters for 100 epochs\n","Optimizer parameters for Nadam with unique parameters: {'name': 'nadam', 'learning_rate': 0.009999999776482582, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07}\n","Epoch 1/100\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 144.4173 - mse: 138.5233\n","Epoch 1: val_loss improved from inf to 101.76350, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 326ms/step - loss: 144.0418 - mse: 138.1370 - val_loss: 101.7635 - val_mse: 94.8315\n","Epoch 2/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.3718 - mse: 82.1784   \n","Epoch 2: val_loss improved from 101.76350 to 47.31407, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 83.2062 - mse: 75.9279 - val_loss: 47.3141 - val_mse: 39.1753\n","Epoch 3/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.0810 - mse: 11.8083 \n","Epoch 3: val_loss improved from 47.31407 to 15.04650, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.4686 - mse: 11.1665 - val_loss: 15.0465 - val_mse: 6.4552\n","Epoch 4/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.5134 - mse: 5.9146 \n","Epoch 4: val_loss improved from 15.04650 to 12.71673, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.4532 - mse: 5.8567 - val_loss: 12.7167 - val_mse: 4.1918\n","Epoch 5/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.3219 - mse: 4.8360 \n","Epoch 5: val_loss improved from 12.71673 to 8.99031, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.1035 - mse: 4.6420 - val_loss: 8.9903 - val_mse: 0.7760\n","Epoch 6/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.7697 - mse: 3.6257 \n","Epoch 6: val_loss improved from 8.99031 to 8.11399, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11.6790 - mse: 3.5673 - val_loss: 8.1140 - val_mse: 0.3345\n","Epoch 7/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3437 - mse: 2.6510 \n","Epoch 7: val_loss improved from 8.11399 to 7.48509, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.3268 - mse: 2.6658 - val_loss: 7.4851 - val_mse: 0.1866\n","Epoch 8/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1036 - mse: 2.8943 \n","Epoch 8: val_loss improved from 7.48509 to 7.00678, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.0027 - mse: 2.8257 - val_loss: 7.0068 - val_mse: 0.2000\n","Epoch 9/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6804 - mse: 2.9634 \n","Epoch 9: val_loss improved from 7.00678 to 6.44712, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.6305 - mse: 2.9456 - val_loss: 6.4471 - val_mse: 0.1270\n","Epoch 10/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5693 - mse: 2.3370 \n","Epoch 10: val_loss improved from 6.44712 to 5.91999, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5526 - mse: 2.3518 - val_loss: 5.9200 - val_mse: 0.0771\n","Epoch 11/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2267 - mse: 2.4683 \n","Epoch 11: val_loss improved from 5.91999 to 5.49072, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.1485 - mse: 2.4204 - val_loss: 5.4907 - val_mse: 0.1075\n","Epoch 12/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3013 - mse: 2.0011 \n","Epoch 12: val_loss improved from 5.49072 to 5.07722, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2886 - mse: 2.0180 - val_loss: 5.0772 - val_mse: 0.1390\n","Epoch 13/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7186 - mse: 1.8545 \n","Epoch 13: val_loss improved from 5.07722 to 4.62782, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7128 - mse: 1.8808 - val_loss: 4.6278 - val_mse: 0.1120\n","Epoch 14/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6563 - mse: 2.2074 \n","Epoch 14: val_loss improved from 4.62782 to 4.20206, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5705 - mse: 2.1557 - val_loss: 4.2021 - val_mse: 0.0825\n","Epoch 15/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0048 - mse: 1.9535 \n","Epoch 15: val_loss improved from 4.20206 to 3.84484, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9462 - mse: 1.9193 - val_loss: 3.8448 - val_mse: 0.0933\n","Epoch 16/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4842 - mse: 1.8013 \n","Epoch 16: val_loss improved from 3.84484 to 3.46639, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4499 - mse: 1.7867 - val_loss: 3.4664 - val_mse: 0.0611\n","Epoch 17/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2336 - mse: 1.8907 \n","Epoch 17: val_loss improved from 3.46639 to 3.21357, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2067 - mse: 1.8819 - val_loss: 3.2136 - val_mse: 0.1260\n","Epoch 18/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2042 - mse: 2.1747 \n","Epoch 18: val_loss improved from 3.21357 to 2.87124, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.1575 - mse: 2.1446 - val_loss: 2.8712 - val_mse: 0.0750\n","Epoch 19/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6006 - mse: 1.8544 \n","Epoch 19: val_loss improved from 2.87124 to 2.60432, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5932 - mse: 1.8649 - val_loss: 2.6043 - val_mse: 0.0771\n","Epoch 20/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5062 - mse: 2.0226 \n","Epoch 20: val_loss improved from 2.60432 to 2.35082, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4642 - mse: 1.9991 - val_loss: 2.3508 - val_mse: 0.0660\n","Epoch 21/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0994 - mse: 1.8500 \n","Epoch 21: val_loss improved from 2.35082 to 2.13045, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0368 - mse: 1.8090 - val_loss: 2.1304 - val_mse: 0.0712\n","Epoch 22/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7654 - mse: 1.7432 \n","Epoch 22: val_loss improved from 2.13045 to 1.90872, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.7573 - mse: 1.7509 - val_loss: 1.9087 - val_mse: 0.0525\n","Epoch 23/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4618 - mse: 1.6390 \n","Epoch 23: val_loss improved from 1.90872 to 1.75168, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4579 - mse: 1.6471 - val_loss: 1.7517 - val_mse: 0.0768\n","Epoch 24/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5235 - mse: 1.8809 \n","Epoch 24: val_loss improved from 1.75168 to 1.57184, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5058 - mse: 1.8743 - val_loss: 1.5718 - val_mse: 0.0614\n","Epoch 25/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2096 - mse: 1.7267 \n","Epoch 25: val_loss improved from 1.57184 to 1.44956, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2334 - mse: 1.7603 - val_loss: 1.4496 - val_mse: 0.0877\n","Epoch 26/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1515 - mse: 1.8145 \n","Epoch 26: val_loss improved from 1.44956 to 1.36997, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1373 - mse: 1.8089 - val_loss: 1.3700 - val_mse: 0.1330\n","Epoch 27/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9850 - mse: 1.7648 \n","Epoch 27: val_loss improved from 1.36997 to 1.22862, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9910 - mse: 1.7776 - val_loss: 1.2286 - val_mse: 0.0836\n","Epoch 28/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1969 - mse: 2.0723 \n","Epoch 28: val_loss improved from 1.22862 to 1.11320, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1358 - mse: 2.0203 - val_loss: 1.1132 - val_mse: 0.0887\n","Epoch 29/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7942 - mse: 1.7870 \n","Epoch 29: val_loss improved from 1.11320 to 1.00676, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7756 - mse: 1.7755 - val_loss: 1.0068 - val_mse: 0.0765\n","Epoch 30/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8163 - mse: 1.9020 \n","Epoch 30: val_loss improved from 1.00676 to 0.92349, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7673 - mse: 1.8611 - val_loss: 0.9235 - val_mse: 0.0898\n","Epoch 31/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4848 - mse: 1.6648 \n","Epoch 31: val_loss improved from 0.92349 to 0.84119, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4989 - mse: 1.6831 - val_loss: 0.8412 - val_mse: 0.0684\n","Epoch 32/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5682 - mse: 1.8063 \n","Epoch 32: val_loss improved from 0.84119 to 0.79304, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5227 - mse: 1.7650 - val_loss: 0.7930 - val_mse: 0.0843\n","Epoch 33/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2025 - mse: 1.5052 \n","Epoch 33: val_loss improved from 0.79304 to 0.71592, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2280 - mse: 1.5347 - val_loss: 0.7159 - val_mse: 0.0672\n","Epoch 34/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3955 - mse: 1.7557 \n","Epoch 34: val_loss improved from 0.71592 to 0.66317, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4084 - mse: 1.7714 - val_loss: 0.6632 - val_mse: 0.0547\n","Epoch 35/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4247 - mse: 1.8218 \n","Epoch 35: val_loss did not improve from 0.66317\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4053 - mse: 1.8042 - val_loss: 0.7176 - val_mse: 0.1353\n","Epoch 36/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5537 - mse: 1.9787 \n","Epoch 36: val_loss improved from 0.66317 to 0.59403, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5179 - mse: 1.9460 - val_loss: 0.5940 - val_mse: 0.0588\n","Epoch 37/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1201 - mse: 1.5927 \n","Epoch 37: val_loss did not improve from 0.59403\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1250 - mse: 1.6010 - val_loss: 0.5964 - val_mse: 0.1055\n","Epoch 38/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1192 - mse: 1.6343 \n","Epoch 38: val_loss improved from 0.59403 to 0.53873, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1150 - mse: 1.6320 - val_loss: 0.5387 - val_mse: 0.0665\n","Epoch 39/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2193 - mse: 1.7515 \n","Epoch 39: val_loss improved from 0.53873 to 0.49640, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2390 - mse: 1.7735 - val_loss: 0.4964 - val_mse: 0.0631\n","Epoch 40/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9472 - mse: 1.5199 \n","Epoch 40: val_loss improved from 0.49640 to 0.45598, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9834 - mse: 1.5583 - val_loss: 0.4560 - val_mse: 0.0474\n","Epoch 41/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8615 - mse: 1.4577 \n","Epoch 41: val_loss did not improve from 0.45598\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9128 - mse: 1.5105 - val_loss: 0.4703 - val_mse: 0.0820\n","Epoch 42/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0282 - mse: 1.6390 \n","Epoch 42: val_loss improved from 0.45598 to 0.45315, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0133 - mse: 1.6236 - val_loss: 0.4532 - val_mse: 0.0636\n","Epoch 43/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0130 - mse: 1.6276 \n","Epoch 43: val_loss improved from 0.45315 to 0.43772, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9972 - mse: 1.6137 - val_loss: 0.4377 - val_mse: 0.0762\n","Epoch 44/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8534 - mse: 1.4949 \n","Epoch 44: val_loss improved from 0.43772 to 0.39111, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8939 - mse: 1.5371 - val_loss: 0.3911 - val_mse: 0.0604\n","Epoch 45/100\n","\u001b[1m17/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9135 - mse: 1.5862 \n","Epoch 45: val_loss improved from 0.39111 to 0.36400, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9511 - mse: 1.6246 - val_loss: 0.3640 - val_mse: 0.0443\n","Epoch 46/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9620 - mse: 1.6460 \n","Epoch 46: val_loss did not improve from 0.36400\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9303 - mse: 1.6156 - val_loss: 0.3954 - val_mse: 0.0834\n","Epoch 47/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1869 - mse: 1.8746 \n","Epoch 47: val_loss did not improve from 0.36400\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2062 - mse: 1.8934 - val_loss: 0.4119 - val_mse: 0.0811\n","Epoch 48/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7669 - mse: 1.4299 \n","Epoch 48: val_loss did not improve from 0.36400\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7731 - mse: 1.4359 - val_loss: 0.4092 - val_mse: 0.0807\n","Epoch 49/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0614 - mse: 1.7353 \n","Epoch 49: val_loss did not improve from 0.36400\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0638 - mse: 1.7380 - val_loss: 0.3800 - val_mse: 0.0577\n","Epoch 50/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8216 - mse: 1.5035 \n","Epoch 50: val_loss improved from 0.36400 to 0.33024, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8091 - mse: 1.4932 - val_loss: 0.3302 - val_mse: 0.0450\n","Epoch 51/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8118 - mse: 1.5303 \n","Epoch 51: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8292 - mse: 1.5485 - val_loss: 0.3986 - val_mse: 0.1182\n","Epoch 52/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8040 - mse: 1.5227 \n","Epoch 52: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8033 - mse: 1.5226 - val_loss: 0.3526 - val_mse: 0.0872\n","Epoch 53/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6834 - mse: 1.4193 \n","Epoch 53: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7117 - mse: 1.4470 - val_loss: 0.3811 - val_mse: 0.1097\n","Epoch 54/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8985 - mse: 1.6247 \n","Epoch 54: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8978 - mse: 1.6231 - val_loss: 0.3597 - val_mse: 0.0717\n","Epoch 55/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8070 - mse: 1.5142 \n","Epoch 55: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8186 - mse: 1.5247 - val_loss: 0.3626 - val_mse: 0.0629\n","Epoch 56/100\n","\u001b[1m18/26\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7165 - mse: 1.4147 \n","Epoch 56: val_loss did not improve from 0.33024\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7757 - mse: 1.4750 - val_loss: 0.3745 - val_mse: 0.1010\n","Epoch 57/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9095 - mse: 1.6447 \n","Epoch 57: val_loss improved from 0.33024 to 0.28853, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8659 - mse: 1.6040 - val_loss: 0.2885 - val_mse: 0.0505\n","Epoch 58/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6747 - mse: 1.4395 \n","Epoch 58: val_loss did not improve from 0.28853\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7104 - mse: 1.4748 - val_loss: 0.2980 - val_mse: 0.0505\n","Epoch 59/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9535 - mse: 1.7062 \n","Epoch 59: val_loss did not improve from 0.28853\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9081 - mse: 1.6609 - val_loss: 0.3085 - val_mse: 0.0584\n","Epoch 60/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6316 - mse: 1.3805 \n","Epoch 60: val_loss improved from 0.28853 to 0.27520, saving model to best_model_Nadam_unique_params_epochs_100.keras\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6532 - mse: 1.4031 - val_loss: 0.2752 - val_mse: 0.0466\n","Epoch 61/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6637 - mse: 1.4328 \n","Epoch 61: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6810 - mse: 1.4459 - val_loss: 0.3356 - val_mse: 0.0462\n","Epoch 62/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8686 - mse: 1.5785 \n","Epoch 62: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8490 - mse: 1.5595 - val_loss: 0.3558 - val_mse: 0.0767\n","Epoch 63/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8220 - mse: 1.5472 \n","Epoch 63: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8140 - mse: 1.5405 - val_loss: 0.3275 - val_mse: 0.0665\n","Epoch 64/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8645 - mse: 1.6007 \n","Epoch 64: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8437 - mse: 1.5784 - val_loss: 0.3394 - val_mse: 0.0566\n","Epoch 65/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9920 - mse: 1.7024 \n","Epoch 65: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9478 - mse: 1.6570 - val_loss: 0.3321 - val_mse: 0.0483\n","Epoch 66/100\n","\u001b[1m21/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9499 - mse: 1.6734 \n","Epoch 66: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9399 - mse: 1.6646 - val_loss: 0.3313 - val_mse: 0.0657\n","Epoch 67/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9564 - mse: 1.6954 \n","Epoch 67: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9226 - mse: 1.6632 - val_loss: 0.3171 - val_mse: 0.0655\n","Epoch 68/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5026 - mse: 1.2465 \n","Epoch 68: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5055 - mse: 1.2492 - val_loss: 0.3323 - val_mse: 0.0701\n","Epoch 69/100\n","\u001b[1m19/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8259 - mse: 1.5644 \n","Epoch 69: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7793 - mse: 1.5197 - val_loss: 0.2984 - val_mse: 0.0631\n","Epoch 70/100\n","\u001b[1m20/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7204 - mse: 1.4881 \n","Epoch 70: val_loss did not improve from 0.27520\n","\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7544 - mse: 1.5228 - val_loss: 0.3052 - val_mse: 0.0789\n","\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n","Optimizer: Nadam, Unique Parameters, Epochs: 100, Test MSE: 0.06353791964770494\n"]}],"source":["for opt in optimizers:\n","    opt_name = opt.__name__\n","    params = unique_params[opt_name]\n","    print(f\"Training with optimizer: {opt_name} with unique parameters for 100 epochs\")\n","\n","    set_seed(seed_value)\n","    optimizer_instance = opt(**params)\n","\n","    # Print optimizer parameters\n","    print(f\"Optimizer parameters for {opt_name} with unique parameters: {optimizer_instance.get_config()}\")\n","\n","    model = create_model(optimizer=optimizer_instance, loss='mse')\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","    model_checkpoint = ModelCheckpoint(f'best_model_{opt_name}_unique_params_epochs_100.keras', monitor='val_loss', save_best_only=True, verbose=1)\n","\n","    history = model.fit(X_train_scaled, y_train_log, validation_split=0.2, epochs=100, batch_size=32,\n","                        callbacks=[early_stopping, model_checkpoint], verbose=1)\n","\n","    # Load the best model saved by ModelCheckpoint\n","    model.load_weights(f'best_model_{opt_name}_unique_params_epochs_100.keras')\n","\n","    y_pred_log = model.predict(X_val_scaled)\n","\n","    mse = mean_squared_error(y_val_log, y_pred_log.flatten())\n","\n","    if opt_name not in results:\n","        results[opt_name] = {}\n","\n","    results[opt_name][f'unique_params_epochs_100'] = {'MSE': mse, 'history': history.history}\n","\n","    print(f\"Optimizer: {opt_name}, Unique Parameters, Epochs: 100, Test MSE: {mse}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Find best parameters for Optimizers:"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T13:17:34.556470Z","iopub.status.busy":"2024-06-02T13:17:34.555396Z","iopub.status.idle":"2024-06-02T13:17:35.701597Z","shell.execute_reply":"2024-06-02T13:17:35.700340Z","shell.execute_reply.started":"2024-06-02T13:17:34.556411Z"},"trusted":true},"outputs":[],"source":["rm -rf hyperparam_tuning\n"]},{"cell_type":"markdown","metadata":{},"source":["We define a class MyHyperModel which inherits from HyperModel. The build method constructs a sequential neural network model with 4 fixed dense layers (256, 128, 64, 32 units) followed by batch normalization and dropout. The output layer is dense with linear activation. The choice of the optimizer (adam, sgd, rmsprop, nadam) and its hyperparameters are determined dynamically via hp.Choice and hp.Float objects. The model is compiled with an mse loss function and an mse metric.\n","\n","Generally speaking, this model is the same as the previous one. It is simply adapted to be used with keras Tuner for hyperparameter search."]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T13:17:38.163060Z","iopub.status.busy":"2024-06-02T13:17:38.162653Z","iopub.status.idle":"2024-06-02T13:17:38.181494Z","shell.execute_reply":"2024-06-02T13:17:38.180552Z","shell.execute_reply.started":"2024-06-02T13:17:38.163028Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class MyHyperModel(HyperModel):\n","    def build(self, hp):\n","        model = Sequential()\n","        model.add(Input(shape=(X_train_scaled.shape[1],)))\n","        \n","        # Fixed number of dense layers with specific units\n","        units = [256, 128, 64, 32]\n","        dropout_rate = 0.3  # Fixed dropout rate\n","        \n","        for i in range(4):  # Exactly 4 layers\n","            model.add(Dense(\n","                units=units[i],\n","                activation='relu',\n","                kernel_regularizer=l2(),\n","                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value)\n","            ))\n","            model.add(BatchNormalization())\n","            model.add(Dropout(rate=dropout_rate))\n","        \n","        model.add(Dense(1, activation='linear', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value)))\n","        \n","        # Choosing the optimizer and its parameters\n","        optimizer_choice = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop', 'nadam'])\n","        if optimizer_choice == 'adam':\n","            opt = Adam(\n","                learning_rate=hp.Float('lr_adam', min_value=1e-4, max_value=1e-2, sampling='LOG'),\n","                beta_1=hp.Float('beta_1_adam', min_value=0.8, max_value=0.99, step=0.01),\n","                beta_2=hp.Float('beta_2_adam', min_value=0.9, max_value=0.999, step=0.001),\n","                epsilon=hp.Float('epsilon_adam', min_value=1e-8, max_value=1e-6, sampling='LOG'),\n","                amsgrad=hp.Boolean('amsgrad_adam')\n","            )\n","        elif optimizer_choice == 'sgd':\n","            opt = SGD(\n","                learning_rate=hp.Float('lr_sgd', min_value=1e-4, max_value=1e-2, sampling='LOG'),\n","                momentum=hp.Float('momentum_sgd', min_value=0.0, max_value=0.99, step=0.01),\n","                nesterov=hp.Boolean('nesterov_sgd')\n","            )\n","        elif optimizer_choice == 'rmsprop':\n","            opt = RMSprop(\n","                learning_rate=hp.Float('lr_rmsprop', min_value=1e-4, max_value=1e-2, sampling='LOG'),\n","                rho=hp.Float('rho_rmsprop', min_value=0.8, max_value=0.99, step=0.01),\n","                momentum=hp.Float('momentum_rmsprop', min_value=0.0, max_value=0.99, step=0.01),\n","                epsilon=hp.Float('epsilon_rmsprop', min_value=1e-8, max_value=1e-6, sampling='LOG'),\n","                centered=hp.Boolean('centered_rmsprop')\n","            )\n","        elif optimizer_choice == 'nadam':\n","            opt = Nadam(\n","                learning_rate=hp.Float('lr_nadam', min_value=1e-4, max_value=1e-2, sampling='LOG'),\n","                beta_1=hp.Float('beta_1_nadam', min_value=0.8, max_value=0.99, step=0.01),\n","                beta_2=hp.Float('beta_2_nadam', min_value=0.9, max_value=0.999, step=0.001),\n","                epsilon=hp.Float('epsilon_nadam', min_value=1e-8, max_value=1e-6, sampling='LOG')\n","            )\n","\n","        model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n","        return model\n"]},{"cell_type":"markdown","metadata":{},"source":["We define a tuner using the RandomSearch class to perform a random hyperparameter search on our model. The tuner uses the hyperparametric model defined in `MyHyperModel()` and optimizes the val_mse objective (mean squared error on validation data). It will run up to 100 trials (`max_trials`) and each trial will run once (`executions_per_trial`). The search results will be saved in the hyperparam_tuning directory under the project named house_prices."]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T13:17:42.511888Z","iopub.status.busy":"2024-06-02T13:17:42.510994Z","iopub.status.idle":"2024-06-02T13:17:42.681470Z","shell.execute_reply":"2024-06-02T13:17:42.680449Z","shell.execute_reply.started":"2024-06-02T13:17:42.511851Z"},"trusted":true},"outputs":[],"source":["# Define a tuner\n","tuner = RandomSearch(\n","    MyHyperModel(),\n","    objective='val_mse',\n","    max_trials=100,\n","    executions_per_trial=1,\n","    directory='hyperparam_tuning',\n","    project_name='house_prices'\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Here, we set a seed (`set_seed(seed_value)`) to ensure the reproducibility of the results. Then, we launch the hyperparameter search with the tuner defined previously using the training data `X_train_scaled` and `y_train_log`, and the validation data `X_val_scaled` and `y_val_log`. The search is performed over 150 epochs (`epochs`) with a batch size of 32 (`batch_size`). An `EarlyStopping` callback is used to stop training if the validation loss (`val_loss`) does not improve for 10 consecutive epochs."]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T13:17:45.845926Z","iopub.status.busy":"2024-06-02T13:17:45.845300Z","iopub.status.idle":"2024-06-02T14:34:06.744327Z","shell.execute_reply":"2024-06-02T14:34:06.743353Z","shell.execute_reply.started":"2024-06-02T13:17:45.845895Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 100 Complete [00h 00m 44s]\n","val_mse: 0.0448736809194088\n","\n","Best val_mse So Far: 0.029791157692670822\n","Total elapsed time: 01h 16m 21s\n"]}],"source":["# Set seed for reproducibility\n","set_seed(seed_value)\n","\n","# Perform hyperparameter search\n","tuner.search(\n","    X_train_scaled, y_train_log,\n","    validation_data=(X_val_scaled, y_val_log),\n","    epochs=150,\n","    batch_size=32,\n","    callbacks=[EarlyStopping(monitor='val_loss', patience=10)]\n",")"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T14:42:24.437159Z","iopub.status.busy":"2024-06-02T14:42:24.436726Z","iopub.status.idle":"2024-06-02T14:42:24.443346Z","shell.execute_reply":"2024-06-02T14:42:24.442475Z","shell.execute_reply.started":"2024-06-02T14:42:24.437129Z"},"trusted":true},"outputs":[],"source":["# Get the optimal hyperparameters\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n"]},{"cell_type":"markdown","metadata":{},"source":["We extract the best hyperparameters after the search and identify the optimal optimizer among several options (`adam`, `sgd`, `rmsprop`, `nadam`). We retrieve the corresponding hyperparameters and print them. If the optimizer is `adam`, it configures the Adam optimizer with specific values ​​of `learning_rate`, `beta_1`, `beta_2`, `epsilon`, and `amsgrad`. Likewise, it configures `sgd`, `rmsprop`, or `nadam` with their specific hyperparameters if the selected optimizer is one of those. The code also prints the corresponding hyperparameter values ​​for the selected optimizer."]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T14:42:27.743886Z","iopub.status.busy":"2024-06-02T14:42:27.743509Z","iopub.status.idle":"2024-06-02T14:42:27.761431Z","shell.execute_reply":"2024-06-02T14:42:27.760514Z","shell.execute_reply.started":"2024-06-02T14:42:27.743857Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The hyperparameter search is complete. The optimal hyperparameters are:\n","Optimizer: nadam\n","Learning Rate: 0.008850221174397929\n","Beta 1: 0.8300000000000001\n","Beta 2: 0.9390000000000001\n","Epsilon: 1.1037353035334539e-07\n"]}],"source":["# Get the optimal hyperparameters\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","# Retrieve the best optimizer\n","best_optimizer = best_hps.get('optimizer')\n","\n","print(\"The hyperparameter search is complete. The optimal hyperparameters are:\")\n","print(f\"Optimizer: {best_optimizer}\")\n","\n","if best_optimizer == 'adam':\n","    opt = Adam(\n","        learning_rate=best_hps.get('lr_adam'),\n","        beta_1=best_hps.get('beta_1_adam'),\n","        beta_2=best_hps.get('beta_2_adam'),\n","        epsilon=best_hps.get('epsilon_adam'),\n","        amsgrad=best_hps.get('amsgrad_adam')\n","    )\n","    print(f\"Learning Rate: {best_hps.get('lr_adam')}\")\n","    print(f\"Beta 1: {best_hps.get('beta_1_adam')}\")\n","    print(f\"Beta 2: {best_hps.get('beta_2_adam')}\")\n","    print(f\"Epsilon: {best_hps.get('epsilon_adam')}\")\n","    print(f\"Amsgrad: {best_hps.get('amsgrad_adam')}\")\n","elif best_optimizer == 'sgd':\n","    opt = SGD(\n","        learning_rate=best_hps.get('lr_sgd'),\n","        momentum=best_hps.get('momentum_sgd'),\n","        nesterov=best_hps.get('nesterov_sgd')\n","    )\n","    print(f\"Learning Rate: {best_hps.get('lr_sgd')}\")\n","    print(f\"Momentum: {best_hps.get('momentum_sgd')}\")\n","    print(f\"Nesterov: {best_hps.get('nesterov_sgd')}\")\n","elif best_optimizer == 'rmsprop':\n","    opt = RMSprop(\n","        learning_rate=best_hps.get('lr_rmsprop'),\n","        rho=best_hps.get('rho_rmsprop'),\n","        momentum=best_hps.get('momentum_rmsprop'),\n","        epsilon=best_hps.get('epsilon_rmsprop'),\n","        centered=best_hps.get('centered_rmsprop')\n","    )\n","    print(f\"Learning Rate: {best_hps.get('lr_rmsprop')}\")\n","    print(f\"Rho: {best_hps.get('rho_rmsprop')}\")\n","    print(f\"Momentum: {best_hps.get('momentum_rmsprop')}\")\n","    print(f\"Epsilon: {best_hps.get('epsilon_rmsprop')}\")\n","    print(f\"Centered: {best_hps.get('centered_rmsprop')}\")\n","elif best_optimizer == 'nadam':\n","    opt = Nadam(\n","        learning_rate=best_hps.get('lr_nadam'),\n","        beta_1=best_hps.get('beta_1_nadam'),\n","        beta_2=best_hps.get('beta_2_nadam'),\n","        epsilon=best_hps.get('epsilon_nadam')\n","    )\n","    print(f\"Learning Rate: {best_hps.get('lr_nadam')}\")\n","    print(f\"Beta 1: {best_hps.get('beta_1_nadam')}\")\n","    print(f\"Beta 2: {best_hps.get('beta_2_nadam')}\")\n","    print(f\"Epsilon: {best_hps.get('epsilon_nadam')}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have found our best optimizer with these best hyperparameters, we reconstruct the model with the best hyperparameters found and compile it with the optimal optimizer (`opt`), the loss `mse` (mean squared error), and the metric `mse`. We define two callbacks: `EarlyStopping` to stop training if the validation loss does not improve for 10 epochs, and `ModelCheckpoint` to save the best model based on the validation loss. Then, we train the model using the training (`X_train_scaled`, `y_train_log`) and validation (`X_val_scaled`, `y_val_log`) data, over 150 epochs with a batch size of 32, while using the defined callbacks."]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T14:42:34.621538Z","iopub.status.busy":"2024-06-02T14:42:34.621140Z","iopub.status.idle":"2024-06-02T14:43:19.705073Z","shell.execute_reply":"2024-06-02T14:43:19.704260Z","shell.execute_reply.started":"2024-06-02T14:42:34.621506Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/150\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 140.9977 - mse: 135.0581\n","Epoch 1: val_loss improved from inf to 100.77865, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 259ms/step - loss: 140.5313 - mse: 134.5816 - val_loss: 100.7786 - val_mse: 93.8129\n","Epoch 2/150\n","\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63.1293 - mse: 55.9305\n","Epoch 2: val_loss improved from 100.77865 to 19.86769, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 60.7767 - mse: 53.5517 - val_loss: 19.8677 - val_mse: 12.1028\n","Epoch 3/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.6122 - mse: 6.8127 \n","Epoch 3: val_loss improved from 19.86769 to 16.64290, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.4758 - mse: 6.6676 - val_loss: 16.6429 - val_mse: 8.8934\n","Epoch 4/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.7839 - mse: 5.0947 \n","Epoch 4: val_loss improved from 16.64290 to 9.67585, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12.4878 - mse: 4.8597 - val_loss: 9.6758 - val_mse: 2.5410\n","Epoch 5/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.9353 - mse: 3.9074 \n","Epoch 5: val_loss improved from 9.67585 to 6.44791, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.8263 - mse: 3.9044 - val_loss: 6.4479 - val_mse: 0.4183\n","Epoch 6/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2333 - mse: 3.4085  \n","Epoch 6: val_loss improved from 6.44791 to 4.97277, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.9306 - mse: 3.2641 - val_loss: 4.9728 - val_mse: 0.4293\n","Epoch 7/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8736 - mse: 2.5577 \n","Epoch 7: val_loss improved from 4.97277 to 3.61924, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7828 - mse: 2.6009 - val_loss: 3.6192 - val_mse: 0.2755\n","Epoch 8/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6623 - mse: 2.4394 \n","Epoch 8: val_loss improved from 3.61924 to 3.02922, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5230 - mse: 2.3803 - val_loss: 3.0292 - val_mse: 0.3180\n","Epoch 9/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4507 - mse: 1.8165 \n","Epoch 9: val_loss improved from 3.02922 to 2.48201, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5250 - mse: 1.9337 - val_loss: 2.4820 - val_mse: 0.1382\n","Epoch 10/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2226 - mse: 1.9331 \n","Epoch 10: val_loss improved from 2.48201 to 2.03727, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1589 - mse: 1.9097 - val_loss: 2.0373 - val_mse: 0.1719\n","Epoch 11/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6329 - mse: 1.8480 \n","Epoch 11: val_loss improved from 2.03727 to 1.63421, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5985 - mse: 1.8546 - val_loss: 1.6342 - val_mse: 0.1459\n","Epoch 12/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2174 - mse: 1.7372 \n","Epoch 12: val_loss improved from 1.63421 to 1.18954, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1368 - mse: 1.6934 - val_loss: 1.1895 - val_mse: 0.1105\n","Epoch 13/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6525 - mse: 1.6235 \n","Epoch 13: val_loss improved from 1.18954 to 1.16464, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6041 - mse: 1.5964 - val_loss: 1.1646 - val_mse: 0.1172\n","Epoch 14/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6943 - mse: 1.6082 \n","Epoch 14: val_loss improved from 1.16464 to 0.98441, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6833 - mse: 1.5937 - val_loss: 0.9844 - val_mse: 0.0964\n","Epoch 15/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2410 - mse: 1.3729 \n","Epoch 15: val_loss improved from 0.98441 to 0.97471, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2254 - mse: 1.3733 - val_loss: 0.9747 - val_mse: 0.1466\n","Epoch 16/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4194 - mse: 1.6023 \n","Epoch 16: val_loss improved from 0.97471 to 0.74019, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.3670 - mse: 1.5660 - val_loss: 0.7402 - val_mse: 0.0846\n","Epoch 17/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8418 - mse: 1.2041 \n","Epoch 17: val_loss improved from 0.74019 to 0.68360, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8747 - mse: 1.2511 - val_loss: 0.6836 - val_mse: 0.1290\n","Epoch 18/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8862 - mse: 1.3481 \n","Epoch 18: val_loss did not improve from 0.68360\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8960 - mse: 1.3718 - val_loss: 0.8020 - val_mse: 0.1502\n","Epoch 19/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0516 - mse: 1.3585 \n","Epoch 19: val_loss improved from 0.68360 to 0.65400, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0258 - mse: 1.3461 - val_loss: 0.6540 - val_mse: 0.1382\n","Epoch 20/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7785 - mse: 1.2969 \n","Epoch 20: val_loss did not improve from 0.65400\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7638 - mse: 1.2853 - val_loss: 0.6765 - val_mse: 0.0901\n","Epoch 21/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8495 - mse: 1.2813 \n","Epoch 21: val_loss improved from 0.65400 to 0.61510, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8105 - mse: 1.2575 - val_loss: 0.6151 - val_mse: 0.0699\n","Epoch 22/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7503 - mse: 1.2096 \n","Epoch 22: val_loss did not improve from 0.61510\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7273 - mse: 1.2185 - val_loss: 0.7644 - val_mse: 0.2371\n","Epoch 23/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6786 - mse: 1.1298 \n","Epoch 23: val_loss improved from 0.61510 to 0.55633, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7114 - mse: 1.1713 - val_loss: 0.5563 - val_mse: 0.0908\n","Epoch 24/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6272 - mse: 1.1187 \n","Epoch 24: val_loss improved from 0.55633 to 0.46620, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6326 - mse: 1.1140 - val_loss: 0.4662 - val_mse: 0.0638\n","Epoch 25/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4704 - mse: 1.0765 \n","Epoch 25: val_loss did not improve from 0.46620\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4834 - mse: 1.0841 - val_loss: 0.5981 - val_mse: 0.1112\n","Epoch 26/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5193 - mse: 1.0405 \n","Epoch 26: val_loss did not improve from 0.46620\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5090 - mse: 1.0387 - val_loss: 0.4818 - val_mse: 0.1074\n","Epoch 27/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4097 - mse: 0.9832 \n","Epoch 27: val_loss did not improve from 0.46620\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4321 - mse: 0.9963 - val_loss: 0.7337 - val_mse: 0.2840\n","Epoch 28/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6316 - mse: 1.1709 \n","Epoch 28: val_loss did not improve from 0.46620\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6125 - mse: 1.1494 - val_loss: 0.6594 - val_mse: 0.1109\n","Epoch 29/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4016 - mse: 0.9124 \n","Epoch 29: val_loss did not improve from 0.46620\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4126 - mse: 0.9446 - val_loss: 0.6013 - val_mse: 0.0766\n","Epoch 30/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3261 - mse: 0.8376 \n","Epoch 30: val_loss improved from 0.46620 to 0.43938, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3372 - mse: 0.8616 - val_loss: 0.4394 - val_mse: 0.0667\n","Epoch 31/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1957 - mse: 0.8428 \n","Epoch 31: val_loss did not improve from 0.43938\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2625 - mse: 0.8964 - val_loss: 0.6075 - val_mse: 0.0938\n","Epoch 32/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3940 - mse: 0.8850 \n","Epoch 32: val_loss did not improve from 0.43938\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4105 - mse: 0.8947 - val_loss: 0.4809 - val_mse: 0.0852\n","Epoch 33/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3051 - mse: 0.9124 \n","Epoch 33: val_loss did not improve from 0.43938\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3094 - mse: 0.9178 - val_loss: 0.5815 - val_mse: 0.1298\n","Epoch 34/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3125 - mse: 0.8542 \n","Epoch 34: val_loss did not improve from 0.43938\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3161 - mse: 0.8460 - val_loss: 0.6422 - val_mse: 0.1677\n","Epoch 35/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2676 - mse: 0.8602 \n","Epoch 35: val_loss improved from 0.43938 to 0.42078, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2043 - mse: 0.8187 - val_loss: 0.4208 - val_mse: 0.0699\n","Epoch 36/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0991 - mse: 0.7310 \n","Epoch 36: val_loss did not improve from 0.42078\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1281 - mse: 0.7431 - val_loss: 0.8041 - val_mse: 0.3322\n","Epoch 37/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2295 - mse: 0.8054 \n","Epoch 37: val_loss did not improve from 0.42078\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1866 - mse: 0.7912 - val_loss: 0.4693 - val_mse: 0.0946\n","Epoch 38/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0729 - mse: 0.6985 \n","Epoch 38: val_loss did not improve from 0.42078\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0915 - mse: 0.7251 - val_loss: 0.6026 - val_mse: 0.2189\n","Epoch 39/150\n","\u001b[1m31/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0118 - mse: 0.6454\n","Epoch 39: val_loss improved from 0.42078 to 0.39998, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0128 - mse: 0.6474 - val_loss: 0.4000 - val_mse: 0.0499\n","Epoch 40/150\n","\u001b[1m28/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0496 - mse: 0.7282\n","Epoch 40: val_loss did not improve from 0.39998\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0454 - mse: 0.7250 - val_loss: 0.4704 - val_mse: 0.1184\n","Epoch 41/150\n","\u001b[1m30/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9430 - mse: 0.6206\n","Epoch 41: val_loss did not improve from 0.39998\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9448 - mse: 0.6224 - val_loss: 0.5767 - val_mse: 0.1598\n","Epoch 42/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9500 - mse: 0.5733 \n","Epoch 42: val_loss did not improve from 0.39998\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9980 - mse: 0.6349 - val_loss: 0.4717 - val_mse: 0.0988\n","Epoch 43/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9713 - mse: 0.6061 \n","Epoch 43: val_loss did not improve from 0.39998\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9681 - mse: 0.6075 - val_loss: 0.4003 - val_mse: 0.0714\n","Epoch 44/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9424 - mse: 0.6072 \n","Epoch 44: val_loss did not improve from 0.39998\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9680 - mse: 0.6218 - val_loss: 0.4264 - val_mse: 0.0763\n","Epoch 45/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9946 - mse: 0.6508  \n","Epoch 45: val_loss did not improve from 0.39998\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9635 - mse: 0.6358 - val_loss: 0.4021 - val_mse: 0.0708\n","Epoch 46/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8782 - mse: 0.5614 \n","Epoch 46: val_loss improved from 0.39998 to 0.34528, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8667 - mse: 0.5600 - val_loss: 0.3453 - val_mse: 0.0701\n","Epoch 47/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9045 - mse: 0.6127 \n","Epoch 47: val_loss did not improve from 0.34528\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8861 - mse: 0.5945 - val_loss: 0.4353 - val_mse: 0.0989\n","Epoch 48/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8743 - mse: 0.5527 \n","Epoch 48: val_loss did not improve from 0.34528\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8801 - mse: 0.5629 - val_loss: 0.3596 - val_mse: 0.0701\n","Epoch 49/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7907 - mse: 0.5117 \n","Epoch 49: val_loss did not improve from 0.34528\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7852 - mse: 0.5075 - val_loss: 0.3889 - val_mse: 0.1130\n","Epoch 50/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8503 - mse: 0.5521 \n","Epoch 50: val_loss improved from 0.34528 to 0.32517, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8366 - mse: 0.5407 - val_loss: 0.3252 - val_mse: 0.0637\n","Epoch 51/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8255 - mse: 0.5362 \n","Epoch 51: val_loss improved from 0.32517 to 0.30283, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8003 - mse: 0.5104 - val_loss: 0.3028 - val_mse: 0.0513\n","Epoch 52/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7309 - mse: 0.4725 \n","Epoch 52: val_loss did not improve from 0.30283\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7280 - mse: 0.4726 - val_loss: 0.3351 - val_mse: 0.0612\n","Epoch 53/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7241 - mse: 0.4505 \n","Epoch 53: val_loss did not improve from 0.30283\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7221 - mse: 0.4560 - val_loss: 0.3274 - val_mse: 0.1176\n","Epoch 54/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7102 - mse: 0.4866 \n","Epoch 54: val_loss did not improve from 0.30283\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7082 - mse: 0.4822 - val_loss: 0.3661 - val_mse: 0.1169\n","Epoch 55/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6304 - mse: 0.4017 \n","Epoch 55: val_loss improved from 0.30283 to 0.27463, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6384 - mse: 0.4102 - val_loss: 0.2746 - val_mse: 0.0588\n","Epoch 56/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5755 - mse: 0.3602 \n","Epoch 56: val_loss did not improve from 0.27463\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5821 - mse: 0.3664 - val_loss: 0.2965 - val_mse: 0.0748\n","Epoch 57/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6374 - mse: 0.4086 \n","Epoch 57: val_loss did not improve from 0.27463\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6345 - mse: 0.4042 - val_loss: 0.3226 - val_mse: 0.0689\n","Epoch 58/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5861 - mse: 0.3501 \n","Epoch 58: val_loss did not improve from 0.27463\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5747 - mse: 0.3486 - val_loss: 0.3078 - val_mse: 0.0740\n","Epoch 59/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5514 - mse: 0.3202 \n","Epoch 59: val_loss did not improve from 0.27463\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5411 - mse: 0.3179 - val_loss: 0.2979 - val_mse: 0.0736\n","Epoch 60/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5765 - mse: 0.3405 \n","Epoch 60: val_loss improved from 0.27463 to 0.24550, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5830 - mse: 0.3490 - val_loss: 0.2455 - val_mse: 0.0596\n","Epoch 61/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4767 - mse: 0.2849 \n","Epoch 61: val_loss did not improve from 0.24550\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4861 - mse: 0.2926 - val_loss: 0.2651 - val_mse: 0.0615\n","Epoch 62/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4464 - mse: 0.2677 \n","Epoch 62: val_loss did not improve from 0.24550\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4560 - mse: 0.2775 - val_loss: 0.2734 - val_mse: 0.0772\n","Epoch 63/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4763 - mse: 0.2763 \n","Epoch 63: val_loss did not improve from 0.24550\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4740 - mse: 0.2779 - val_loss: 0.2562 - val_mse: 0.0612\n","Epoch 64/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4353 - mse: 0.2408 \n","Epoch 64: val_loss did not improve from 0.24550\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4394 - mse: 0.2471 - val_loss: 0.2668 - val_mse: 0.0844\n","Epoch 65/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5027 - mse: 0.3151 \n","Epoch 65: val_loss improved from 0.24550 to 0.22795, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4927 - mse: 0.3080 - val_loss: 0.2279 - val_mse: 0.0680\n","Epoch 66/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3755 - mse: 0.2130 \n","Epoch 66: val_loss did not improve from 0.22795\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3847 - mse: 0.2201 - val_loss: 0.2335 - val_mse: 0.0533\n","Epoch 67/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3677 - mse: 0.1899 \n","Epoch 67: val_loss improved from 0.22795 to 0.21572, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3740 - mse: 0.1997 - val_loss: 0.2157 - val_mse: 0.0574\n","Epoch 68/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4004 - mse: 0.2323 \n","Epoch 68: val_loss improved from 0.21572 to 0.21394, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4057 - mse: 0.2359 - val_loss: 0.2139 - val_mse: 0.0649\n","Epoch 69/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3702 - mse: 0.2219 \n","Epoch 69: val_loss improved from 0.21394 to 0.19731, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3784 - mse: 0.2277 - val_loss: 0.1973 - val_mse: 0.0403\n","Epoch 70/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3928 - mse: 0.2218 \n","Epoch 70: val_loss did not improve from 0.19731\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3942 - mse: 0.2235 - val_loss: 0.1998 - val_mse: 0.0572\n","Epoch 71/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3300 - mse: 0.1866 \n","Epoch 71: val_loss did not improve from 0.19731\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3415 - mse: 0.1939 - val_loss: 0.2066 - val_mse: 0.0569\n","Epoch 72/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3177 - mse: 0.1680 \n","Epoch 72: val_loss improved from 0.19731 to 0.17965, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3226 - mse: 0.1744 - val_loss: 0.1796 - val_mse: 0.0473\n","Epoch 73/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3165 - mse: 0.1835 \n","Epoch 73: val_loss did not improve from 0.17965\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3222 - mse: 0.1872 - val_loss: 0.2088 - val_mse: 0.0610\n","Epoch 74/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3035 - mse: 0.1604 \n","Epoch 74: val_loss did not improve from 0.17965\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3045 - mse: 0.1643 - val_loss: 0.1846 - val_mse: 0.0563\n","Epoch 75/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2798 - mse: 0.1526 \n","Epoch 75: val_loss did not improve from 0.17965\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2879 - mse: 0.1583 - val_loss: 0.2272 - val_mse: 0.0778\n","Epoch 76/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3008 - mse: 0.1573 \n","Epoch 76: val_loss did not improve from 0.17965\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3007 - mse: 0.1611 - val_loss: 0.2009 - val_mse: 0.0604\n","Epoch 77/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2702 - mse: 0.1326 \n","Epoch 77: val_loss improved from 0.17965 to 0.16761, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2745 - mse: 0.1407 - val_loss: 0.1676 - val_mse: 0.0411\n","Epoch 78/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2418 - mse: 0.1218 \n","Epoch 78: val_loss did not improve from 0.16761\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2421 - mse: 0.1253 - val_loss: 0.1935 - val_mse: 0.0577\n","Epoch 79/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2720 - mse: 0.1425 \n","Epoch 79: val_loss improved from 0.16761 to 0.15347, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2721 - mse: 0.1466 - val_loss: 0.1535 - val_mse: 0.0435\n","Epoch 80/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2454 - mse: 0.1374 \n","Epoch 80: val_loss improved from 0.15347 to 0.15302, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2465 - mse: 0.1382 - val_loss: 0.1530 - val_mse: 0.0470\n","Epoch 81/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2144 - mse: 0.1108 \n","Epoch 81: val_loss did not improve from 0.15302\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2179 - mse: 0.1136 - val_loss: 0.1882 - val_mse: 0.0668\n","Epoch 82/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2378 - mse: 0.1213 \n","Epoch 82: val_loss did not improve from 0.15302\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2336 - mse: 0.1213 - val_loss: 0.1768 - val_mse: 0.0637\n","Epoch 83/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2014 - mse: 0.0947 \n","Epoch 83: val_loss improved from 0.15302 to 0.13559, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2062 - mse: 0.1010 - val_loss: 0.1356 - val_mse: 0.0393\n","Epoch 84/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2174 - mse: 0.1134 \n","Epoch 84: val_loss did not improve from 0.13559\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2183 - mse: 0.1115 - val_loss: 0.1414 - val_mse: 0.0517\n","Epoch 85/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1930 - mse: 0.0963 \n","Epoch 85: val_loss did not improve from 0.13559\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1974 - mse: 0.0997 - val_loss: 0.1524 - val_mse: 0.0564\n","Epoch 86/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1892 - mse: 0.0911 \n","Epoch 86: val_loss did not improve from 0.13559\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1915 - mse: 0.0939 - val_loss: 0.1362 - val_mse: 0.0412\n","Epoch 87/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1798 - mse: 0.0840 \n","Epoch 87: val_loss did not improve from 0.13559\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1823 - mse: 0.0872 - val_loss: 0.1622 - val_mse: 0.0529\n","Epoch 88/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1645 - mse: 0.0653 \n","Epoch 88: val_loss did not improve from 0.13559\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1682 - mse: 0.0715 - val_loss: 0.1380 - val_mse: 0.0455\n","Epoch 89/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1561 - mse: 0.0672 \n","Epoch 89: val_loss improved from 0.13559 to 0.12882, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1597 - mse: 0.0722 - val_loss: 0.1288 - val_mse: 0.0525\n","Epoch 90/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1556 - mse: 0.0794 \n","Epoch 90: val_loss improved from 0.12882 to 0.12604, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1619 - mse: 0.0830 - val_loss: 0.1260 - val_mse: 0.0402\n","Epoch 91/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1676 - mse: 0.0836 \n","Epoch 91: val_loss did not improve from 0.12604\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1678 - mse: 0.0845 - val_loss: 0.1426 - val_mse: 0.0418\n","Epoch 92/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1768 - mse: 0.0792 \n","Epoch 92: val_loss did not improve from 0.12604\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1739 - mse: 0.0805 - val_loss: 0.1284 - val_mse: 0.0493\n","Epoch 93/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1473 - mse: 0.0665 \n","Epoch 93: val_loss improved from 0.12604 to 0.12568, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1501 - mse: 0.0681 - val_loss: 0.1257 - val_mse: 0.0403\n","Epoch 94/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1557 - mse: 0.0722 \n","Epoch 94: val_loss improved from 0.12568 to 0.11662, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1555 - mse: 0.0719 - val_loss: 0.1166 - val_mse: 0.0376\n","Epoch 95/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1303 - mse: 0.0544 \n","Epoch 95: val_loss improved from 0.11662 to 0.11654, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1338 - mse: 0.0583 - val_loss: 0.1165 - val_mse: 0.0369\n","Epoch 96/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1386 - mse: 0.0608 \n","Epoch 96: val_loss improved from 0.11654 to 0.10687, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1411 - mse: 0.0635 - val_loss: 0.1069 - val_mse: 0.0376\n","Epoch 97/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1281 - mse: 0.0570 \n","Epoch 97: val_loss did not improve from 0.10687\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1318 - mse: 0.0592 - val_loss: 0.1069 - val_mse: 0.0425\n","Epoch 98/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1256 - mse: 0.0581 \n","Epoch 98: val_loss improved from 0.10687 to 0.10022, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1271 - mse: 0.0589 - val_loss: 0.1002 - val_mse: 0.0317\n","Epoch 99/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1297 - mse: 0.0588 \n","Epoch 99: val_loss did not improve from 0.10022\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1308 - mse: 0.0592 - val_loss: 0.1242 - val_mse: 0.0485\n","Epoch 100/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1257 - mse: 0.0531 \n","Epoch 100: val_loss did not improve from 0.10022\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1254 - mse: 0.0545 - val_loss: 0.1079 - val_mse: 0.0397\n","Epoch 101/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1140 - mse: 0.0497 \n","Epoch 101: val_loss did not improve from 0.10022\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1140 - mse: 0.0504 - val_loss: 0.1111 - val_mse: 0.0353\n","Epoch 102/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1212 - mse: 0.0489 \n","Epoch 102: val_loss did not improve from 0.10022\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1212 - mse: 0.0510 - val_loss: 0.1033 - val_mse: 0.0393\n","Epoch 103/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1113 - mse: 0.0484 \n","Epoch 103: val_loss improved from 0.10022 to 0.09798, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1114 - mse: 0.0490 - val_loss: 0.0980 - val_mse: 0.0346\n","Epoch 104/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1049 - mse: 0.0445 \n","Epoch 104: val_loss did not improve from 0.09798\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1068 - mse: 0.0467 - val_loss: 0.0981 - val_mse: 0.0370\n","Epoch 105/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1105 - mse: 0.0476 \n","Epoch 105: val_loss improved from 0.09798 to 0.09316, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1107 - mse: 0.0486 - val_loss: 0.0932 - val_mse: 0.0322\n","Epoch 106/150\n","\u001b[1m17/32\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0987 - mse: 0.0423 \n","Epoch 106: val_loss did not improve from 0.09316\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1026 - mse: 0.0454 - val_loss: 0.0979 - val_mse: 0.0371\n","Epoch 107/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1108 - mse: 0.0484 \n","Epoch 107: val_loss did not improve from 0.09316\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1108 - mse: 0.0496 - val_loss: 0.0966 - val_mse: 0.0426\n","Epoch 108/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1038 - mse: 0.0456 \n","Epoch 108: val_loss did not improve from 0.09316\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1068 - mse: 0.0480 - val_loss: 0.0945 - val_mse: 0.0314\n","Epoch 109/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1127 - mse: 0.0467 \n","Epoch 109: val_loss did not improve from 0.09316\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1109 - mse: 0.0463 - val_loss: 0.0997 - val_mse: 0.0471\n","Epoch 110/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0953 - mse: 0.0402 \n","Epoch 110: val_loss improved from 0.09316 to 0.09303, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0983 - mse: 0.0422 - val_loss: 0.0930 - val_mse: 0.0318\n","Epoch 111/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1070 - mse: 0.0461 \n","Epoch 111: val_loss improved from 0.09303 to 0.08762, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1074 - mse: 0.0472 - val_loss: 0.0876 - val_mse: 0.0324\n","Epoch 112/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0990 - mse: 0.0440 \n","Epoch 112: val_loss did not improve from 0.08762\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1002 - mse: 0.0454 - val_loss: 0.0949 - val_mse: 0.0398\n","Epoch 113/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0948 - mse: 0.0405 \n","Epoch 113: val_loss did not improve from 0.08762\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0967 - mse: 0.0424 - val_loss: 0.0901 - val_mse: 0.0355\n","Epoch 114/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0988 - mse: 0.0442 \n","Epoch 114: val_loss improved from 0.08762 to 0.08529, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0986 - mse: 0.0451 - val_loss: 0.0853 - val_mse: 0.0368\n","Epoch 115/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0832 - mse: 0.0331 \n","Epoch 115: val_loss did not improve from 0.08529\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0872 - mse: 0.0365 - val_loss: 0.0860 - val_mse: 0.0316\n","Epoch 116/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0951 - mse: 0.0400 \n","Epoch 116: val_loss improved from 0.08529 to 0.08350, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0963 - mse: 0.0413 - val_loss: 0.0835 - val_mse: 0.0333\n","Epoch 117/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0901 - mse: 0.0379 \n","Epoch 117: val_loss did not improve from 0.08350\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0939 - mse: 0.0409 - val_loss: 0.0854 - val_mse: 0.0322\n","Epoch 118/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0974 - mse: 0.0412 \n","Epoch 118: val_loss did not improve from 0.08350\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0972 - mse: 0.0420 - val_loss: 0.0903 - val_mse: 0.0421\n","Epoch 119/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0915 - mse: 0.0402 \n","Epoch 119: val_loss did not improve from 0.08350\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0941 - mse: 0.0414 - val_loss: 0.0907 - val_mse: 0.0387\n","Epoch 120/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0992 - mse: 0.0424 \n","Epoch 120: val_loss did not improve from 0.08350\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0993 - mse: 0.0433 - val_loss: 0.0877 - val_mse: 0.0303\n","Epoch 121/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0871 - mse: 0.0343 \n","Epoch 121: val_loss did not improve from 0.08350\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0890 - mse: 0.0368 - val_loss: 0.0871 - val_mse: 0.0329\n","Epoch 122/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0922 - mse: 0.0371 \n","Epoch 122: val_loss improved from 0.08350 to 0.08103, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0940 - mse: 0.0392 - val_loss: 0.0810 - val_mse: 0.0321\n","Epoch 123/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0908 - mse: 0.0397 \n","Epoch 123: val_loss did not improve from 0.08103\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0922 - mse: 0.0411 - val_loss: 0.0870 - val_mse: 0.0386\n","Epoch 124/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0900 - mse: 0.0396 \n","Epoch 124: val_loss did not improve from 0.08103\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0904 - mse: 0.0408 - val_loss: 0.0910 - val_mse: 0.0397\n","Epoch 125/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0843 - mse: 0.0339 \n","Epoch 125: val_loss improved from 0.08103 to 0.07995, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0865 - mse: 0.0372 - val_loss: 0.0800 - val_mse: 0.0329\n","Epoch 126/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0822 - mse: 0.0350 \n","Epoch 126: val_loss did not improve from 0.07995\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0870 - mse: 0.0387 - val_loss: 0.0890 - val_mse: 0.0361\n","Epoch 127/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0885 - mse: 0.0370 \n","Epoch 127: val_loss did not improve from 0.07995\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0892 - mse: 0.0386 - val_loss: 0.0818 - val_mse: 0.0306\n","Epoch 128/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0862 - mse: 0.0354 \n","Epoch 128: val_loss did not improve from 0.07995\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0882 - mse: 0.0377 - val_loss: 0.0885 - val_mse: 0.0422\n","Epoch 129/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0832 - mse: 0.0360 \n","Epoch 129: val_loss improved from 0.07995 to 0.07987, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0866 - mse: 0.0382 - val_loss: 0.0799 - val_mse: 0.0320\n","Epoch 130/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0818 - mse: 0.0353 \n","Epoch 130: val_loss did not improve from 0.07987\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0852 - mse: 0.0382 - val_loss: 0.0806 - val_mse: 0.0340\n","Epoch 131/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0821 - mse: 0.0348 \n","Epoch 131: val_loss improved from 0.07987 to 0.07902, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0839 - mse: 0.0368 - val_loss: 0.0790 - val_mse: 0.0303\n","Epoch 132/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0795 - mse: 0.0316 \n","Epoch 132: val_loss improved from 0.07902 to 0.07356, saving model to best_model_keras_tuner_epochs_100.keras\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0814 - mse: 0.0340 - val_loss: 0.0736 - val_mse: 0.0290\n","Epoch 133/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0801 - mse: 0.0351 \n","Epoch 133: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0835 - mse: 0.0373 - val_loss: 0.0833 - val_mse: 0.0343\n","Epoch 134/150\n","\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0801 - mse: 0.0326 \n","Epoch 134: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0828 - mse: 0.0354 - val_loss: 0.0787 - val_mse: 0.0356\n","Epoch 135/150\n","\u001b[1m18/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0770 - mse: 0.0328 \n","Epoch 135: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0800 - mse: 0.0355 - val_loss: 0.0795 - val_mse: 0.0376\n","Epoch 136/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0782 - mse: 0.0359 \n","Epoch 136: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0806 - mse: 0.0375 - val_loss: 0.0746 - val_mse: 0.0323\n","Epoch 137/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0787 - mse: 0.0346 \n","Epoch 137: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0803 - mse: 0.0363 - val_loss: 0.0840 - val_mse: 0.0377\n","Epoch 138/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0735 - mse: 0.0288 \n","Epoch 138: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0780 - mse: 0.0330 - val_loss: 0.0755 - val_mse: 0.0318\n","Epoch 139/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0790 - mse: 0.0362 \n","Epoch 139: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0801 - mse: 0.0377 - val_loss: 0.0778 - val_mse: 0.0322\n","Epoch 140/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0798 - mse: 0.0338 \n","Epoch 140: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0822 - mse: 0.0361 - val_loss: 0.0834 - val_mse: 0.0330\n","Epoch 141/150\n","\u001b[1m21/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0826 - mse: 0.0351 \n","Epoch 141: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0846 - mse: 0.0373 - val_loss: 0.0765 - val_mse: 0.0319\n","Epoch 142/150\n","\u001b[1m20/32\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0765 - mse: 0.0321 \n","Epoch 142: val_loss did not improve from 0.07356\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0801 - mse: 0.0348 - val_loss: 0.0792 - val_mse: 0.0332\n"]}],"source":["# Rebuild the model using the best hyperparameters and compile it\n","model = tuner.hypermodel.build(best_hps)\n","model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint(f'best_model_keras_tuner_epochs_100.keras', monitor='val_loss', save_best_only=True, verbose=1)\n","\n","# Train the model\n","history = model.fit(\n","    X_train_scaled, y_train_log,\n","    validation_data=(X_val_scaled, y_val_log),\n","    epochs=150,\n","    batch_size=32,\n","    callbacks=[early_stopping, model_checkpoint],\n","    verbose=1\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Now, we evaluate the optimal model on the validation data. To do this, we predict the values ​​of `y` for `X_val_scaled` using `model.predict()`, then calculate the mean square error (MSE) between the predictions (`y_pred_log_test2.flatten()`) and the true ones. values ​​(`y_val_log`) using `mean_squared_error()`. The results are then saved in a `results` dictionary under the `'Keras_Tuner`` key, including the MSE of the optimal model and the training history (`history.history`)."]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T15:09:36.503929Z","iopub.status.busy":"2024-06-02T15:09:36.503032Z","iopub.status.idle":"2024-06-02T15:09:37.186062Z","shell.execute_reply":"2024-06-02T15:09:37.184965Z","shell.execute_reply.started":"2024-06-02T15:09:36.503895Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n"]}],"source":["# Évaluer le modèle optimal sur les données de validation\n","y_pred_log_test2 = model.predict(X_val_scaled)\n","mse_test2 = mean_squared_error(y_val_log, y_pred_log_test2.flatten())\n","\n","results['Keras_Tuner'] = {\n","    'optimal_model_keras_tuner_mse': {'MSE': mse_test2, 'history': history.history}\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Resultat"]},{"cell_type":"markdown","metadata":{},"source":["Show the result of our test:"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T15:09:39.093211Z","iopub.status.busy":"2024-06-02T15:09:39.092826Z","iopub.status.idle":"2024-06-02T15:09:39.105712Z","shell.execute_reply":"2024-06-02T15:09:39.104492Z","shell.execute_reply.started":"2024-06-02T15:09:39.093182Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["      Optimizer                  Configuration       MSE\n","0          Adam                 epochs_150_mse  0.081190\n","1          Adam            epochs_100_loss_mse  0.104574\n","2          Adam            epochs_100_loss_mae  0.258640\n","3          Adam          epochs_100_loss_huber  0.068115\n","4          Adam            lr_0.001_epochs_100  0.104574\n","5          Adam             lr_0.01_epochs_100  0.056995\n","6          Adam              lr_0.1_epochs_100  0.078273\n","7          Adam       unique_params_epochs_100  0.056995\n","8           SGD                 epochs_150_mse  0.041406\n","9           SGD            epochs_100_loss_mse  0.064181\n","10          SGD            epochs_100_loss_mae  0.089661\n","11          SGD          epochs_100_loss_huber  0.091117\n","12          SGD            lr_0.001_epochs_100  0.105312\n","13          SGD             lr_0.01_epochs_100  0.064181\n","14          SGD       unique_params_epochs_100  0.068041\n","15      RMSprop                 epochs_150_mse  0.057422\n","16      RMSprop            epochs_100_loss_mse  0.097950\n","17      RMSprop            epochs_100_loss_mae  0.081648\n","18      RMSprop          epochs_100_loss_huber  0.163229\n","19      RMSprop            lr_0.001_epochs_100  0.097950\n","20      RMSprop             lr_0.01_epochs_100  0.115676\n","21      RMSprop              lr_0.1_epochs_100  0.084147\n","22      RMSprop       unique_params_epochs_100  0.064741\n","23        Nadam                 epochs_150_mse  0.074337\n","24        Nadam            epochs_100_loss_mse  0.109846\n","25        Nadam            epochs_100_loss_mae  0.245510\n","26        Nadam          epochs_100_loss_huber  0.107030\n","27        Nadam            lr_0.001_epochs_100  0.109846\n","28        Nadam             lr_0.01_epochs_100  0.063538\n","29        Nadam              lr_0.1_epochs_100  0.115879\n","30        Nadam       unique_params_epochs_100  0.063538\n","31  Keras_Tuner  optimal_model_keras_tuner_mse  0.029001\n"]}],"source":["# Créer une liste pour stocker les résultats\n","results_of_the_test = []\n","\n","# Parcourir les résultats et ajouter chaque entrée à la liste\n","for opt in results:\n","    for config in results[opt]:\n","        mse_value = results[opt][config]['MSE']\n","        results_of_the_test.append([opt, config, mse_value])\n","\n","# Convertir la liste en DataFrame pandas\n","result_table = pd.DataFrame(results_of_the_test, columns=[\"Optimizer\", \"Configuration\", \"MSE\"])\n","\n","# Afficher le DataFrame\n","print(result_table)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T15:09:48.528358Z","iopub.status.busy":"2024-06-02T15:09:48.527981Z","iopub.status.idle":"2024-06-02T15:09:48.535740Z","shell.execute_reply":"2024-06-02T15:09:48.534469Z","shell.execute_reply.started":"2024-06-02T15:09:48.528326Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best result:\n","Optimizer: Keras_Tuner, Configuration: optimal_model_keras_tuner_mse, MSE: 0.029000900045229698\n"]}],"source":["# Initialiser les variables pour stocker le meilleur résultat\n","best_optimizer = None\n","best_config = None\n","best_mse = float('inf')\n","\n","# Parcourir les résultats pour trouver le meilleur optimiseur et configuration\n","for opt in results:\n","    for config in results[opt]:\n","        mse_value = results[opt][config]['MSE']\n","        if mse_value < best_mse:\n","            best_mse = mse_value\n","            best_optimizer = opt\n","            best_config = config\n","\n","# Afficher le meilleur résultat\n","print(\"Best result:\")\n","print(f\"Optimizer: {best_optimizer}, Configuration: {best_config}, MSE: {best_mse}\")\n"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T15:09:54.241454Z","iopub.status.busy":"2024-06-02T15:09:54.241061Z","iopub.status.idle":"2024-06-02T15:09:56.342764Z","shell.execute_reply":"2024-06-02T15:09:56.341690Z","shell.execute_reply.started":"2024-06-02T15:09:54.241407Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA+0AAAK9CAYAAABRvo1QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeVxP2f8H8Nen7fNp+7Rp1a5VEhKThiwR0VhKGDPKNowlhrIMWscyhIx9mYkxzNh9Q0xEmCxDhCENiRAKLaL9c35/9OtOV4uilffz8fh8v59777nnvs8592M69557roAxxkAIIYQQQgghhJAmR6qxAyCEEEIIIYQQQkjlqNNOCCGEEEIIIYQ0UdRpJ4QQQgghhBBCmijqtBNCCCGEEEIIIU0UddoJIYQQQgghhJAmijrthBBCCCGEEEJIE0WddkIIIYQQQgghpImiTjshhBBCCCGEENJEUaedEEIIIYQQQghpoqjTTgghhJAPtn37dlhZWUFWVhaqqqqNHU69CAoKgkAgqNM8fXx8YGxsXKd5fgyMjY3h4+PT2GEQQkiTQJ12QghpApKTkzFhwgSYmppCJBJBLBbDyckJq1atQl5eXmOHR2ohNjYWQ4YMgY6ODuTk5KClpQV3d3fs37+/sUOrN7dv34aPjw9atWqFzZs3Y9OmTQ1y3Li4OAwePBja2toQCoUwNjbGhAkTkJqa+t55vnnzBkFBQYiNja27QJu5rVu3QiAQQCQS4fHjxxW2d+/eHW3atGmEyAgh5NMg09gBEELIp+7IkSMYOnQohEIhRo0ahTZt2qCwsBB//fUX/P39cfPmzQbrBJEPExgYiJCQEJibm2PChAkwMjLCixcvEBUVBQ8PD+zYsQNffvllY4dZ52JjYyGRSLBq1SqYmZk1yDFXr16NadOmwdTUFFOnToWuri4SExOxZcsW7Nq1C1FRUejSpUut833z5g2Cg4MBlHZGy5s/fz7mzJlTF+FzNm/eDIlEUqd51peCggIsWbIEq1evbuxQCCHkk0KddkIIaUQpKSkYPnw4jIyMcPLkSejq6nLbJk+ejLt37+LIkSONGOGHy8/Ph5ycHKSkPu7BXXv37kVISAg8PT2xc+dOyMrKctv8/f3x559/oqioqE6O9ebNGygoKNRJXnUhPT0dAOp0WHx1ZYyLi8P06dPx+eef49ixY7x03377LZycnODp6YmbN29CTU2tzmKSkZGBjEzd/ulU/jxpCMXFxZBIJJCTk6v1vu3atcPmzZsxd+5c6Onp1UN0hBBCKvNx/wVFCCFN3NKlS5Gbm4uff/6Z12EvY2ZmhmnTpnHLxcXFCA0NRatWrbjhwN9//z0KCgp4+xkbG2PAgAH466+/0KlTJ4hEIpiamuLXX3/l0ly+fBkCgQDbtm2rcNw///wTAoEAhw8f5tY9fvwYY8aM4YYi29jY4JdffuHtFxsbC4FAgD/++APz589Hy5YtoaCggJycHADAnj170Lp1a4hEIrRp0wYHDhyo9JleiUSC8PBw2NjYQCQSQVtbGxMmTEBmZmaty1kmKysL3333HYyNjSEUCqGvr49Ro0bh+fPnXJqCggIEBgbCzMwMQqEQBgYGmDVrVoX6rcyCBQugrq6OX375pdKOmKurKwYMGADgv+HG9+/fr7T+yg/NLht6HB8fj27dukFBQQHff/89BgwYAFNT00pjcXR0RMeOHXnrfvvtN9jb20NeXh7q6uoYPnw4Hj58yEtz584deHh4QEdHByKRCPr6+hg+fDiys7OrLLexsTECAwMBAJqamhAIBAgKCuK2r1u3DjY2NhAKhdDT08PkyZORlZXFy6OqMlYlNDSUO3ff7ti3atUKS5cuxZMnT7Bx40ZuvY+PD5SUlHDv3j24urpCUVERenp6CAkJAWMMAHD//n1oamoCAIKDgyEQCHjlqeyZdoFAgClTpnDntry8PBwdHXHjxg0AwMaNG2FmZgaRSITu3btXaPO3z//u3btzx337s3XrVi5dVlYWpk+fDgMDAwiFQpiZmeHHH3/k3bW/f/8+BAIBwsLCEB4ezv27cevWLQCljzXU5lGC77//HiUlJViyZMk700ZERKBnz57Q0tKCUChE69atsX79+grpGGP44YcfoK+vDwUFBfTo0QM3b96skO7ly5fw8/ODra0tlJSUIBaL0a9fP1y7do2Xruw3tHv3bgQHB6Nly5ZQVlaGp6cnsrOzUVBQgOnTp0NLSwtKSkoYPXp0jX7fhBDSmOhOOyGENKJDhw7B1NS0xsN4x40bh23btsHT0xMzZ87ExYsXsXjxYiQmJuLAgQO8tHfv3oWnpyfGjh0Lb29v/PLLL/Dx8YG9vT1sbGzQsWNHmJqaYvfu3fD29ubtu2vXLqipqcHV1RUA8OzZM3z22WdcB0VTUxNHjx7F2LFjkZOTg+nTp/P2Dw0NhZycHPz8/FBQUAA5OTkcOXIEw4YNg62tLRYvXozMzEyMHTsWLVu2rFDOCRMmYOvWrRg9ejR8fX2RkpKCNWvW4OrVq4iLi+N1it9VTgDIzc1F165dkZiYiDFjxqBDhw54/vw5IiMj8ejRI7Ro0QISiQRffPEF/vrrL3zzzTewtrbGjRs3sHLlSvz77784ePBgle1y584d3L59G2PGjIGysnKN2rI2Xrx4gX79+mH48OH46quvoK2tDXt7e4waNQqXLl2Cg4MDl/bBgwe4cOECli1bxq1buHAhFixYAC8vL4wbNw4ZGRlYvXo1unXrhqtXr0JVVRWFhYVwdXVFQUEBpk6dCh0dHTx+/BiHDx9GVlYWVFRUKo0tPDwcv/76Kw4cOID169dDSUkJbdu2BVDayQ0ODoaLiwu+/fZbJCUlYf369bh06VKFdqysjJV58+YNYmJi0LVrV5iYmFSaZtiwYfjmm29w+PBh3nD2kpIS9O3bF5999hmWLl2KY8eOITAwEMXFxQgJCYGmpibWr1+Pb7/9FoMHD8aQIUMAgCtPVc6ePYvIyEhMnjwZALB48WIMGDAAs2bNwrp16zBp0iRkZmZi6dKlGDNmDE6ePFllXvPmzcO4ceN463777Tf8+eef0NLS4urA2dkZjx8/xoQJE2BoaIhz585h7ty5ePLkCcLDw3n7R0REID8/H9988w2EQiHU1dUBANbW1nB2dq7x8/smJiYYNWoUNm/ejDlz5lR7t339+vWwsbHBF198ARkZGRw6dAiTJk2CRCLh6gkAAgIC8MMPP8DNzQ1ubm64cuUK+vTpg8LCQl5+9+7dw8GDBzF06FCYmJjg2bNn2LhxI5ydnXHr1q0KsSxevBjy8vKYM2cO7t69i9WrV0NWVhZSUlLIzMxEUFAQLly4gK1bt8LExAQBAQE1qgNCCGkUjBBCSKPIzs5mANjAgQNrlD4hIYEBYOPGjeOt9/PzYwDYyZMnuXVGRkYMADtz5gy3Lj09nQmFQjZz5kxu3dy5c5msrCx7+fIlt66goICpqqqyMWPGcOvGjh3LdHV12fPnz3nHHj58OFNRUWFv3rxhjDF26tQpBoCZmppy68rY2toyfX199urVK25dbGwsA8CMjIy4dWfPnmUA2I4dO3j7Hzt2rML6mpYzICCAAWD79+9nb5NIJIwxxrZv386kpKTY2bNneds3bNjAALC4uLgK+5b53//+xwCwlStXVpmmvIiICAaApaSk8NaX1d+pU6e4dc7OzgwA27BhAy9tdnZ2hXIyxtjSpUuZQCBgDx48YIwxdv/+fSYtLc0WLlzIS3fjxg0mIyPDrb969SoDwPbs2VOjMpQXGBjIALCMjAxuXXp6OpOTk2N9+vRhJSUl3Po1a9YwAOyXX355ZxkrU/Y7mDZtWrXp2rZty9TV1bllb29vBoBNnTqVWyeRSFj//v2ZnJwcF3tGRgYDwAIDA6ssZ3kAmFAo5LXlxo0bGQCmo6PDcnJyuPVz586t0O7e3t688/9tcXFxTFZWlvd7DA0NZYqKiuzff//lpZ0zZw6TlpZmqampjDHGUlJSGAAmFotZenp6hbwBMGdn5yqPXabsfL106RJLTk5mMjIyzNfXl9vu7OzMbGxsePu8/ftnjDFXV1dmamrKLZedI/379+d+h4wx9v333zMAzNvbm1uXn5/PO4/KyicUCllISAi3ruw31KZNG1ZYWMitHzFiBBMIBKxfv368PBwdHautf0IIaQpoeDwhhDSSsiHjNb0zGxUVBQCYMWMGb/3MmTMBoMKz761bt0bXrl25ZU1NTVhaWuLevXvcumHDhqGoqIg3s3l0dDSysrIwbNgwAKXDV/ft2wd3d3cwxvD8+XPu4+rqiuzsbFy5coV3bG9vb8jLy3PLaWlpuHHjBkaNGgUlJSVuvbOzM2xtbXn77tmzByoqKujduzfvWPb29lBSUsKpU6dqXc59+/bBzs4OgwcPrlCvZcOd9+zZA2tra1hZWfGO27NnTwCocNzyatuWtSUUCjF69GjeurLhwbt37+aGdwOloyQ+++wzGBoaAgD2798PiUQCLy8vXrl0dHRgbm7OlavsTvqff/6JN2/efHDMJ06cQGFhIaZPn86bz2D8+PEQi8UVztfKyliZV69eAXh3XSsrK3PtUt6UKVO472UjRwoLC3HixIl3HrsqvXr14g1x79y5MwDAw8ODF2fZ+vLnZnWePn0KT09PtGvXDuvWrePW79mzB127doWamhqvTV1cXFBSUoIzZ87w8vHw8OCG/ZfHGKv1LPmmpqb4+uuvsWnTJjx58qTKdOV//9nZ2Xj+/DmcnZ1x79497nGLsnNk6tSpvMcO3h65A5SeH2XnUUlJCV68eAElJSVYWlpW+PcHAEaNGsUbydG5c2cwxjBmzBheus6dO+Phw4coLi6uWQUQQkgjoE47IYQ0ErFYDOC/Tsi7PHjwAFJSUhVm59bR0YGqqioePHjAW1/WaStPTU2N91y4nZ0drKyssGvXLm7drl270KJFC66zmpGRgaysLGzatAmampq8T1knq2wisjJvD1sui62ymcXfXnfnzh1kZ2dDS0urwvFyc3MrHKsm5UxOTn7nK6nu3LmDmzdvVjimhYVFpWUsr7ZtWVstW7asdOKwYcOG4eHDhzh//jyA0nLGx8dzF1yA0nIxxmBubl6hbImJiVy5TExMMGPGDGzZsgUtWrSAq6sr1q5dW+3z7NUpa3NLS0veejk5OZiamlY4X6sq49vKOsHvqutXr15V6NhLSUlVmAegrH3ffta8Nt4+B8sugBgYGFS6/u25GSpTXFwMLy8vlJSUYP/+/RAKhdy2O3fu4NixYxXa08XFBcC7f48fav78+SguLq722fa4uDi4uLhAUVERqqqq0NTU5OYpKDunys4Bc3Nz3r6ampoVJhCUSCRYuXIlzM3NIRQK0aJFC2hqauL69euVnqO1aROJRPLe5zkhhDQEeqadEEIaiVgshp6eHv75559a7ff2RFhVkZaWrnR9+buyQGnHb+HChXj+/DmUlZURGRmJESNGcLNkl01s9dVXX1V49r3M28/8lr/LVlsSiQRaWlrYsWNHpdvfvmNY03LW5Li2trZYsWJFpdvf/mO/PCsrKwDgJh97l6rasKSkpNL1VdWnu7s7FBQUsHv3bnTp0gW7d++GlJQUhg4dyqWRSCQQCAQ4evRopXVVfuTD8uXL4ePjg//973+Ijo6Gr68vFi9ejAsXLkBfX79GZXtfNT1nzMzMICMjg+vXr1eZpqCgAElJSRUm46svVZ2DH3Ju+vv74/z58zhx4kSFupdIJOjduzdmzZpV6b5lFyLKfMjvsTKmpqb46quvsGnTpkpfgZecnIxevXrBysoKK1asgIGBAeTk5BAVFYWVK1e+1yvuFi1ahAULFmDMmDEIDQ2Furo6pKSkMH369Erzq482IYSQxkKddkIIaUQDBgzApk2bcP78eTg6Olab1sjICBKJBHfu3IG1tTW3/tmzZ8jKyoKRkdF7xTBs2DAEBwdj37590NbWRk5ODoYPH85t19TUhLKyMkpKSrg7ebVVFtvdu3crbHt7XatWrXDixAk4OTnVWWejVatW77w40qpVK1y7dg29evWq8YWRMhYWFrC0tMT//vc/rFq1itcRrkzZXcS3Z1F/++7zuygqKmLAgAHYs2cPVqxYgV27dqFr1668SblatWoFxhhMTEwqdOYqY2trC1tbW8yfPx/nzp2Dk5MTNmzYgB9++KFWsZW1eVJSEu/udmFhIVJSUt77XFJUVESPHj1w8uRJPHjwoNLzfvfu3SgoKOBm6y8jkUhw7949Xj38+++/AMANb69t29eHP/74A+Hh4QgPD4ezs3OF7a1atUJubu5712FdmD9/Pn777Tf8+OOPFbYdOnQIBQUFiIyM5N3xfvsRk7K2u3PnDu8cycjIqDAaYe/evejRowd+/vln3vqsrCy0aNHig8tDCCFNGQ2PJ4SQRjRr1iwoKipi3LhxePbsWYXtycnJWLVqFQDAzc0NACrMDF12Z7h///7vFYO1tTVsbW2xa9cu7Nq1C7q6uujWrRu3XVpaGh4eHti3b1+lHd+MjIx3HkNPTw9t2rTBr7/+itzcXG796dOnK9ydLhsSHBoaWiGf4uLiCh3dmvDw8MC1a9cqzLAP/HeHzcvLC48fP8bmzZsrpMnLy8Pr16+rPUZwcDBevHiBcePGVfp8bHR0NPcKvVatWgEA79njkpISbNq0qeaF+n/Dhg1DWloatmzZgmvXrvGGxgPAkCFDIC0tjeDg4Ap3ExljePHiBYDS5/LfjtvW1hZSUlLv9UosFxcXyMnJ4aeffuId9+eff0Z2dvZ7n69AaYeRMQYfHx/k5eXxtqWkpGDWrFnQ1dXFhAkTKuy7Zs0a7jtjDGvWrIGsrCx69eoFANwr5N7nPKsL//zzD8aNG4evvvqK97rH8ry8vHD+/Hn8+eefFbZlZWXV+Pns2r7yrbxWrVrhq6++wsaNG/H06VPetrK72eXbPTs7GxEREbx0Li4ukJWVxerVq3lp3/43rizPt8/fPXv24PHjx+8VPyGENCd0p50QQhpRq1atsHPnTgwbNgzW1tYYNWoU2rRpg8LCQpw7dw579uyBj48PgNLnz729vbFp0yZkZWXB2dkZf//9N7Zt24ZBgwahR48e7x3HsGHDEBAQAJFIhLFjx/ImDgOAJUuW4NSpU+jcuTPGjx+P1q1b4+XLl7hy5QpOnDiBly9fvvMYixYtwsCBA+Hk5ITRo0cjMzMTa9asQZs2bXgdeWdnZ0yYMAGLFy9GQkIC+vTpA1lZWdy5cwd79uzBqlWr4OnpWavy+fv7Y+/evRg6dCjGjBkDe3t7vHz5EpGRkdiwYQPs7Ozw9ddfY/fu3Zg4cSJOnToFJycnlJSU4Pbt29i9ezf+/PPPaodbDxs2DDdu3MDChQtx9epVjBgxAkZGRnjx4gWOHTuGmJgY7Ny5EwBgY2ODzz77DHPnzsXLly+hrq6OP/74470mw3Jzc4OysjL8/Py4CyzltWrVCj/88APmzp2L+/fvY9CgQVBWVkZKSgoOHDiAb775Bn5+fjh58iSmTJmCoUOHwsLCAsXFxdi+fXuledaEpqYm5s6di+DgYPTt2xdffPEFkpKSsG7dOjg4OOCrr76qdZ5lunXrhrCwMMyYMQNt27aFj48PdHV1cfv2bWzevBkSiQRRUVEVnosWiUQ4duwYvL290blzZxw9ehRHjhzB999/zz12IS8vj9atW2PXrl2wsLCAuro62rRp8845EepK2TwR3bp1w2+//cbb1qVLF5iamsLf3x+RkZEYMGAA93rD169f48aNG9i7dy/u379fo7vPtX3l29vmzZuH7du3IykpiXu9IgD06dMHcnJycHd3x4QJE5Cbm4vNmzdDS0uLN3mdpqYm/Pz8uFfkubm54erVqzh69GiF+AcMGICQkBCMHj0aXbp0wY0bN7Bjx44KcxQQQshHqWEnqyeEEFKZf//9l40fP54ZGxszOTk5pqyszJycnNjq1atZfn4+l66oqIgFBwczExMTJisrywwMDNjcuXN5aRgrfRVa//79KxzH2dm50lc83blzhwFgANhff/1VaYzPnj1jkydPZgYGBkxWVpbp6OiwXr16sU2bNnFpyl63VNVrw/744w9mZWXFhEIha9OmDYuMjGQeHh7MysqqQtpNmzYxe3t7Ji8vz5SVlZmtrS2bNWsWS0tLe69yvnjxgk2ZMoW1bNmSycnJMX19febt7c17jV1hYSH78ccfmY2NDRMKhUxNTY3Z29uz4OBglp2dXWmZ3hYTE8MGDhzItLS0mIyMDNPU1GTu7u7sf//7Hy9dcnIyc3FxYUKhkGlra7Pvv/+eHT9+vNJXvr39Oq23jRw5kgFgLi4uVabZt28f+/zzz5mioiJTVFRkVlZWbPLkySwpKYkxxti9e/fYmDFjWKtWrZhIJGLq6uqsR48e7MSJE+8sc2WvfCuzZs0aZmVlxWRlZZm2tjb79ttvWWZmJi9NTcpYmTNnzrCBAweyFi1aMFlZWWZoaMjGjx/P7t+/XyGtt7c3U1RUZMnJyaxPnz5MQUGBaWtrs8DAwAqvEjt37hyzt7dncnJyvNe/VfXKt8mTJ/PWlb1qbdmyZbz1lf0+3n7lW9lrDCv7REREcOlevXrF5s6dy8zMzJicnBxr0aIF69KlCwsLC+NedVZVHOVjr+0r395W9iq9t9svMjKStW3blolEImZsbMx+/PFH9ssvv1R45V1JSQkLDg5murq6TF5ennXv3p39888/zMjIqMIr32bOnMmlc3JyYufPn6/wW6/q36CqylDduUsIIU2FgDGaeYMQQkjjadeuHTQ1NXH8+PHGDoV8xHx8fLB3717eqA5CCCGkOaBn2gkhhDSIoqKiCsO/Y2Njce3aNXTv3r1xgiKEEEIIaeLomXZCCCEN4vHjx3BxccFXX30FPT093L59Gxs2bICOjg4mTpzY2OERQgghhDRJ1GknhBDSINTU1GBvb48tW7YgIyMDioqK6N+/P5YsWQINDY3GDo8QQgghpEmiZ9oJIYQQQgghhJAmip5pJ4QQQgghhBBCmijqtBNCCCGEEEIIIU0UPdMOQCKRIC0tDcrKyhAIBI0dDiGEEEIIIYSQjxxjDK9evYKenh6kpKq+n06ddgBpaWkwMDBo7DAIIYQQQgghhHxiHj58CH19/Sq3U6cdgLKyMoDSyhKLxY0cTdWKiooQHR2NPn36QFZWtrHDIe+B2vDjQO3Y/FEbfhyoHZs/asOPA7Vj80dt2DhycnJgYGDA9UerQp12gBsSLxaLm3ynXUFBAWKxmH5MzRS14ceB2rH5ozb8OFA7Nn/Uhh8Hasfmj9qwcb3rEW2aiI4QQgghhBBCCGmiqNNOCCGEEEIIIYQ0UdRpJ4QQQgghhBBCmih6pp0QQgghpAExxlBcXIySkpLGDqXRFRUVQUZGBvn5+VQfzRi1Y/NHbVg/pKWlISMj88GvFadOOyGEEEJIAyksLMSTJ0/w5s2bxg6lSWCMQUdHBw8fPvzgP2pJ46F2bP6oDeuPgoICdHV1IScn9955UKedEEIIIaQBSCQSpKSkQFpaGnp6epCTk/vk/ziWSCTIzc2FkpISpKToqc3mitqx+aM2rHuMMRQWFiIjIwMpKSkwNzd/77qlTjshhBBCSAMoLCyERCKBgYEBFBQUGjucJkEikaCwsBAikYg6Cs0YtWPzR21YP+Tl5SErK4sHDx5w9fs+qEUIIYQQQhoQ/UFMCCGfjrr4N5/+q0EIIYQQQgghhDRR1GknhBBCCCGEEEKaKOq0E0IIIYSQBmdsbIxVq1Y1dhifnKCgILRr166xwyANIDY2FgKBAFlZWY0dCvlA1GknhBBCCCFVEggE1X6CgoLeK99Lly5h/PjxHxRb9+7dMX369A/Kg9SdmzdvwsPDA8bGxhAIBAgPD6+QJigoqMI5ZGVlxUuTn5+PyZMnQ0NDA0pKSvDw8MCzZ88aqBSEND00ezwhhBBCCKnSkydPuO+7du1CQEAAkpKSuHVKSkrcd8YYSkpKICPz7j8xNTU1IZFIkJOTU7cBk0bz5s0bmJqaYujQofjuu++qTGdjY4MTJ05wy2+fL9999x2OHDmCPXv2QEVFBVOmTMGQIUMQFxdXb7ET0pTRnXZCCCGEEFIlHR0d7qOiogKBQMAt3759G8rKyjh69Cjs7e0hFArx119/ITk5GQMHDoS2tjaUlJTg4ODA66QBFYfHCwQCbNmyBYMHD4aCggLMzc0RGRn5QbHv27cPNjY2EAqFMDY2xvLly3nb161bB3Nzc4hEImhra8PT05PbtnfvXtja2kJeXh4aGhpwcXHB69evPyierKwsjBs3DpqamhCLxejZsyeuXbvGbS8bur5x40bu1YBeXl7Izs7m0kgkEoSEhEBfXx9CoRDt2rXDsWPHeMd59OgRRowYAXV1dSgqKqJjx464ePEiL8327dthbGwMFRUVDB8+HK9evfrgsjs4OGDZsmUYPnw4hEJhlelkZGR451WLFi24bdnZ2fj555+xYsUK9OzZE/b29oiIiMC5c+dw4cKFd8ZQNiT8zz//RPv27SEvL4+ePXsiPT0dR48ehbW1NcRiMb788ku8efOmxmXesmULrK2tIRKJYGVlhXXr1lUbh0QiweLFi2FiYgJ5eXnY2dlh7969FeI8cuQI2rZtC5FIhM8++wz//PMPL593ncMFBQWYPXs2DAwMIBQKYWZmhp9//pmXJj4+Hh07doSCggK6dOnCu+h27do19OjRAyoqKjA0NISDgwMuX778znomDYvutBNCCCGENCL31X8h41VBgx9XU1mIQ1M/r5O85syZg7CwMJiamkJNTQ0PHz6Em5sbFi5cCKFQiF9//RXu7u5ISkqCoaFhlfkEBwdj6dKlWLZsGVavXo2RI0fiwYMHUFdXr3VM8fHx8PLyQlBQEIYNG4Zz585h0qRJ0NDQgI+PDy5fvgxfX19s374dXbp0wcuXL3H27FkApaMLRowYgaVLl2Lw4MF49eoVzp49C8bYe9cRAAwdOhTy8vI4evQoVFRUsHHjRvTq1Qv//vsvV8a7d+9i9+7dOHToEHJycjB27FhMmjQJO3bsAACsWrUKy5cvx8aNG9G+fXv88ssv+OKLL3Dz5k2Ym5sjNzcXzs7OaNmyJSIjI6Gjo4MrV65AIpFwcSQnJ+PgwYM4fPgwMjMz4eXlhSVLlmDhwoX1Vvby7ty5Az09PYhEIjg6OmLx4sXceREfH4+ioiK4uLhw6a2srGBoaIjz58/js88+q9ExgoKCsGbNGu7Ch5eXF4RCIXbu3Inc3FwMHjwYq1evxuzZs99Z5h07diAgIABr1qxB+/btcfXqVYwfPx6Kiorw9vau9PiLFy/Gb7/9hg0bNsDc3BxnzpzBV199BU1NTTg7O3Pp/P39sWrVKujo6OD777+Hu7s7/v33X8jKyr7zHAaAUaNG4fz58/jpp59gZ2eHlJQUPH/+nBfLvHnzsHz5cmhqamLixIkYM2YMN2ph5MiRaN++PdauXYu8vDzcvXsXsrKyNWtI0mCo004IIYQQ0ogyXhXgaU5+Y4fxQUJCQtC7d29uWV1dHXZ2dtxyaGgoDhw4gMjISEyZMqXKfHx8fDBixAgAwKJFi/DTTz/h77//Rt++fWsd04oVK9CrVy8sWLAAAGBhYYFbt25h2bJl8PHxQWpqKhQVFTFgwAAoKyvDyMgI7du3B1DaaS8uLsaQIUNgZGQEALC1ta11DOX99ddf+Pvvv5Gens7dhQ4LC8PBgwexd+9efPPNNwBKn+f+9ddf0bJlSwDA6tWr0b9/fyxfvhw6OjoICwvD7NmzMXz4cADAjz/+iFOnTiE8PBxr167Fzp07kZGRgUuXLnEXAszMzHixSCQSbN26FcrKygCAr7/+GjExMVynva7LXl7nzp2xdetWWFpa4smTJwgODkbXrl3xzz//QFlZGU+fPoWcnBxUVVV5+2lra+Pp06c1Ps4PP/wAJycnAMDYsWMxd+5cJCcnw9TUFADg6emJU6dOcZ326socGBiI5cuXY8iQIQAAExMT3Lp1Cxs3bqy0015QUIBFixbhxIkTcHR0BACYmprir7/+wsaNG3md9sDAQO63s23bNujr6+PAgQPw8vJ65zn877//Yvfu3Th+/Dh3kaOsfOUtXLiQO+acOXPQv39/5OfnQyQSITU1Ff7+/rCyskJOTg7at29fJ+8VJ3WLOu2EEEIIIY1IU7nqYcTN5bgdO3bkLefm5iIoKAhHjhzhOkR5eXlITU2tNp+2bdty3xUVFSEWi5Genv5eMSUmJmLgwIG8dU5OTggPD0dJSQl69+4NIyMjmJqaom/fvujbty83NN/Ozg69evWCra0tXF1d0adPH3h6ekJNTa3SY9nY2ODBgwcAgK5du+Lo0aMV0ly7dg25ubnQ0NDgrc/Ly0NycjK3bGhoyHXYAcDR0RESiQRJSUlQUFBAWloa1xktX66yYfYJCQlo3759taMTjI2NuQ47AOjq6nL1XNuy11a/fv24723btkXnzp1hZGSE3bt3Y+zYsXVyjLK8y2hra0NBQYHXodXW1sbff/8NoPoyv379GsnJyRg7dixv4sTi4mKoqKhUeuy7d+/izZs3vAtZAFBYWMhdGCpT1qkHSi92WVpaIjExEcC7z+GEhARIS0vzLgK8qy50dXUBAOnp6TA0NMSMGTMwbtw4bN++HU5OTvjqq69gbm5ebX6k4VGnnRBCCCGkEdXVEPXGpKioyFv28/PD8ePHERYWBjMzM8jLy8PT0xOFhYXV5vP2sFyBQMAb1l2XlJWVceXKFcTGxiI6OhoBAQEICgrCpUuXoKqqiuPHj+PcuXOIjo7G6tWrMW/ePFy8eBEmJiYV8oqKikJRUREAQF5evtLj5ebmQldXF7GxsRW2vX1X+UNUdfzyqqtnaWnpWpX9Q6mqqsLCwgJ3794FUDqHQmFhIbKysnj18uzZM+jo6NQ43/JlFAgE711mBQUFAMDmzZvRuXNnXh7S0tKVHjs3NxcAcOTIEd4FGADVPutfWzVpa6BiXQDgyh4UFIQvv/wShw8fxuHDh7FkyRL88ccfGDx4cJ3FST4cjX0ghBBCCCF1Ki4uDj4+Phg8eDBsbW2ho6OD+/fvN2gM1tbWFWYbj4uLg4WFBdfZkpGRgYuLC5YuXYrr16/j/v37OHnyJIDSzo2TkxOCg4Nx9epVyMnJ4cCBA5Uey8jICGZmZjAzM6vQSSvToUMHPH36FDIyMlzask/5idhSU1ORlpbGLV+4cAFSUlKwtLSEWCyGnp5epeVq3bo1gNK7qgkJCXj58mUta+w/tSn7h8rNzUVycjJ3B9je3h6ysrKIiYnh0iQlJSE1NZV3V7quVVVmbW1t6Onp4d69exXaraqLGK1bt4ZQKERqamqFfQwMDHhpy0+ul5mZiX///RfW1tYA3n0O29raQiKR4PTp0x9UdgsLC0yfPh379+/H4MGDERER8UH5kbpHd9oJIYQQQkidMjc3x/79++Hu7g6BQIAFCxbU2x3zjIwMJCQk8Nbp6upi5syZcHBwQGhoKIYNG4bz589jzZo13Kzfhw8fxr1799CtWzeoqakhKioKEokElpaWuHjxImJiYtCnTx9oaWnh4sWLyMjI4DpT78PFxQWOjo4YNGgQli5dCgsLC6SlpeHIkSMYPHgw94iBSCSCt7c3wsLCkJOTA19fX3h5eXF3mf39/REYGIhWrVqhXbt2iIiIQEJCAjdR3YgRI7Bo0SIMGjQIixcvhq6uLq5evQo9Pb0adXo/pOyFhYW4ffs29/3x48dISEiAkpIS91y9n58f3N3dYWRkhLS0NAQGBkJaWpqby0BFRQVjx47FjBkzoK6uDrFYjKlTp8LR0bHGk9DV1rvKHBwcDF9fX6ioqKBv374oKCjA5cuXkZmZiRkzZlTIT1lZGX5+fvjuu+8gkUjw+eefIzs7G3FxcRCLxbzn4ENCQqChoQFtbW3MmzcPLVq0wKBBgwDgneewsbExvL29MWbMGG4iugcPHiA9PR1eXl7vLHdeXh78/f3h6ekJIyMjJCUl4fLly/Dw8KiDWiV1iTrthBBCCCGkTq1YsQJjxoxBly5d0KJFC8yePbve3se+c+dO7Ny5k7cuNDQU8+fPx+7duxEQEIDQ0FDo6uoiJCSEm3VbVVUV+/fvR1BQEPLz82Fubo7ff/8dNjY2SExMxJkzZxAeHo6cnBwYGRlh+fLlvOexa0sgECAqKgrz5s3D6NGjkZGRAR0dHXTr1g3a2tpcOjMzMwwZMgRubm54+fIlBgwYwHu9mK+vL7KzszFz5kykp6ejdevWiIyM5J5DlpOTQ3R0NGbOnAk3NzcUFxejdevWWLt2bY3iFIvF7132tLQ03jPbYWFhCAsLg7OzM/dYQNnr6F68eAFNTU18/vnnuHDhAjQ1Nbn9Vq5cCSkpKXh4eKCgoACurq7vfMXah3hXmceNGwcFBQUsW7YM/v7+UFRUhK2tLaZPn15lnqGhodDU1MTixYtx7949qKqqokOHDvj+++956ZYsWYJp06bhzp07aNeuHQ4dOgQ5OTkApaMzqjuHAWD9+vX4/vvvMWnSJLx48QKGhoYVjlEVaWlpvHjxAqNGjcKzZ8+goaGBIUOGIDg4uHYVSOqdgNXl+xuaqZycHKioqCA7Oxtisbixw6lSUVERoqKi4ObmRq9iaKaoDT8O1I7NH7Xhx6G5tWN+fj5SUlJgYmICkUjU2OE0CRKJBDk5ORCLxTRjNUqfLz548GCFkQNNHbVj7cXGxqJHjx7IzMys0zkN3he1Yf2p7t/+mvZDqUUIIYQQQgghhJAmijrtzcTp3/+H/fM3IzM6D5nPXzR2OIQQQggh5BOjpKRU6UcsFuPcuXMNEsPEiROrjGPixIkNEgMhDY2eaW8m0m88xvNsKwDA9T/PwsVnaCNHRAghhBBC6lJQUBCCgoIaO4wqVTVsXyKR8N77Xp9CQkLg5+dX6bam/Jjr27p37w56SpnUFHXamwl1S22kny/9/vLfjMYNhhBCCCGEfHLKZoB/W9nz0A1BS0sLWlpaDXIsQpoKGh7fTLTr2w0ClAAA3mSrNm4whBBCCCGEEEIaBHXamwkNbU2I5R4CAF6X6ODezduNHBEhhBBCCCGEkPpGnfZmRL5FIff99sm/GzESQgghhBBCCCENgTrtzYiunSn3PefBm0aMhBBCCCGEEEJIQ6BOezNi17MLZAWlnfXc1zooKSpu5IgIIYQQQgghhNQn6rQ3I3IiIRSEqQCAAibGtdMN8z5MQgghhJC6ZmxsjFWrVjV2GJ+coKAgtGvXrrHDqDexsbEQCATIyspq7FDI//Px8cGgQYMaO4xmjTrtzQwTF3DfH/yd1IiREEIIIeRTIBAIqv2873vFL126hPHjx39QbN27d8f06dM/KA9Sd27evAkPDw8YGxtDIBAgPDy80nRr166FsbExRCIROnfujL//5s/VlJ+fj8mTJ0NDQwNKSkrw8PDAs2fPGqAEhDRN1GlvZqRbqnDf36QLGjESQgghhHwKnjx5wn3Cw8MhFot56/z8/Li0jDEUF9fs8T1NTU0oKCjUV9ikEbx58wampqZYsmQJdHR0Kk2za9cuzJgxA4GBgbhy5Qrs7Ozg6uqK9PR0Ls13332HQ4cOYc+ePTh9+jTS0tIwZMiQhioGIU0OddqbGXlNdShIPQcAvMrXR25WTiNHRAghhJCPmY6ODvdRUVGBQCDglm/fvg1lZWUcPXoU9vb2EAqF+Ouvv5CcnIyBAwdCW1sbSkpKcHBwwIkTJ3j5vj08XiAQYMuWLRg8eDAUFBRgbm6OyMjID4p93759sLGxgVAohLGxMZYvX87bvm7dOpibm0MkEkFbWxuenp7ctr1798LW1hby8vLQ0NCAi4sLXr9+/UHxZGVlYdy4cdDU1IRYLEbPnj1x7do1bnvZ0PWNGzfCwMAACgoK8PLyQnZ2NpdGIpEgJCQE+vr6EAqFaNeuHY4dO8Y7zqNHjzBixAioq6tDUVERHTt2xMWLF3lptm/fDmNjY6ioqGD48OF49erVB5fdwcEBy5Ytw/DhwyEUCitNs2LFCowfPx6jR49G69atsWHDBigoKOCXX34BAGRnZ+Pnn3/GihUr0LNnT9jb2yMiIgLnzp3DhQsX3l3JlWiM86BsSPiiRYugra0NVVVVhISEoLi4GP7+/lBXV4e+vj4iIiK4fQoLCzFlyhTo6upCJBLByMgIixcv5ra/6/ypzMOHD+Hl5QVVVVWoq6tj4MCBuH//foU4Q0JCYGZmBlVVVUycOBGFhf+9taqgoAC+vr7Q0tKCSCTC559/jkuXLvGOc/PmTQwYMABisRjKysro2rUrkpOTeWnCwsKgq6sLDQ0NTJ48GUVFRTVqAwLINHYApHakpAVQUH6ON9ktUAI5xB+LhfPwLxo7LEIIIYS8r43OQG76u9PVNSUtYMLpOslqzpw5CAsLg6mpKdTU1PDw4UO4ublh4cKFEAqF+PXXX+Hu7o6kpCQYGhpWmU9wcDCWLl2KZcuWYfXq1Rg5ciQePHgAdXX1WscUHx8PLy8vBAUFYdiwYTh37hwmTZoEDQ0N+Pj44PLly/D19cX27dvRpUsXvHz5EmfPngVQOrpgxIgRWLp0KQYPHoxXr17h7NmzYIy9dx0BwNChQyEvL4+jR49CRUUFGzduRK9evfDvv/9yZbx79y52796NQ4cOIScnB2PHjsWkSZOwY8cOAMCqVauwfPlybNy4Ee3bt8cvv/yCL774Ajdv3oS5uTlyc3Ph7OyMli1bIjIyEjo6Orhy5QokEgkXR3JyMg4ePIjDhw8jMzMTXl5eWLJkCRYuXFhvZQdKO6Xx8fGYO3cut05KSgouLi44f/48gNJ2KyoqgouLC5fGysoKhoaGOH/+PD777LNaHbMxz4OTJ09CX18fZ86cQVxcHMaOHYtz586hW7duuHjxInbt2oUJEyagd+/e0NfXx08//YTIyEjs3r0bhoaGePjwIR4+fMjlV5Pzp7yioiK4urrC0dERZ8+ehYyMDH744Qf07dsX169fh5ycHAAgJiYGQqEQhw4dwvPnzzF27FhoaGhg4cKFAIBZs2Zh37592LZtG4yMjLB06VK4urri7t27UFdXx+PHj9GtWzd0794dJ0+ehFgsRlxcHG/UzalTp6Crq4tTp07h7t27GDZsGNq1a4fx48dX2wbk/zHCsrOzGQCWnZ3d2KFUq7CwkB08eJAd27iTrZkQw9ZMiGF75q9r7LBILZS1YWFhYWOHQj4AtWPzR234cWhu7ZiXl8du3brF8vLy+BvCrBgLFDf8J8yq1mWIiIhgKioq3PKpU6cYAHbw4MF37mtjY8NWr17NLRsZGbEVK1awzMxMVlJSwgCw+fPnc9tzc3MZAHb06NEq83R2dmbTpk2rdNuXX37JevfuzVvn7+/PWrduzRhjbN++fUwsFrOcnJwK+8bHxzMA7P79++8sV02dPXuWicVilp+fz1vfqlUrtnHjRsYYY4GBgUxaWpo9evSI23706FEmJSXFnjx5whhjTE9Pjy1cuJCXh4ODA5s0aRJjjLGNGzcyZWVl9uLFi0rjCAwMZAoKCrxy+/v7s86dOzPG3q/sJSUlXDuWMTIyYitXruSle/z4MQPAzp07x1vv7+/POnXqxBhjbMeOHUxOTq7CMRwcHNisWbPeGUvZOZmZmckYa7zzwNvbmxkZGfHqxNLSknXt2pVbLi4uZoqKiuz3339njDE2depU1rNnTyaRSCrkV5Pz523bt29nlpaWvPwKCgqYvLw8+/PPP7k41dXV2atXr7g2XL9+PVNSUmIlJSUsNzeXycrKsh07dnB5FBYWMj09PbZ06VLGGGNz585lJiYmVf5bXFYXxcXF3LqhQ4eyYcOGMcaqb4OPQZX/9rOa90NpeHwzZOPiCKD0aumbTKXGDYYQQgghH0ZJC1DWa/iPkladFaFjx4685dzcXPj5+cHa2hqqqqpQUlJCYmIiUlNTq82nbdu23HdFRUWIxWLes861kZiYCCcnJ946Jycn3LlzByUlJejduzeMjIxgamqKr7/+Gjt27MCbN6Wv1rWzs0OvXr1ga2uLoUOHYvPmzcjMzKzyWDY2NlBSUoKSkhL69etXaZpr164hNzeXm1yt7JOSksIbRmxoaIiWLVtyy46OjpBIJEhKSkJOTg7S0tIqLVdiYiIAICEhAe3bt692dIKxsTGUlZW5ZV1dXa6ea1v2pq4hz4O32djYQErqv+6WtrY2bG1tuWVpaWloaGhwde/j44OEhARYWlrC19cX0dHRXNqanj/lXbt2DXfv3oWysjKXXl1dHfn5+bx97OzsePNLODo6Ijc3Fw8fPkRycjKKiop4dSgrK4tOnTrxzrmuXbtCVla22rqQlpbmlsufc9W1ASlFw+ObIW2DlhDLnkZOkQFeFbfEo7sp0DczaeywCCGEEPI+6miIemNSVFTkLfv5+eH48eMICwuDmZkZ5OXl4enpyXtOtjJv/9EvEAh4w7rrkrKyMq5cuYLY2FhER0cjICAAQUFBuHTpElRVVXH8+HGcO3cO0dHRWL16NebNm4eLFy/CxKTi31xRUVHc87ny8vKVHi83Nxe6urqIjY2tsE1VVbXOylXV8currp6lpaVrVfbaaNGiBaSlpSvMBP/s2TNu4jodHR0UFhYiKyuLVy/l09SlujwP3lZZPVdX9x06dEBKSgqOHj2KEydOwMvLCy4uLti7d+97nT+5ubmwt7fnHq0oT1NT853x19SHnnPvagNCE9E1W/Lq/119+uc4va+dEEIIIU1HXFwcfHx8MHjwYNja2kJHR4c3+VVDsLa2RlxcXIW4LCwsuDt+MjIycHFxwdKlS3H9+nXcv38fJ0+eBFDaqXByckJwcDCuXr0KOTk5HDhwoNJjGRkZwczMDGZmZry75OV16NABT58+hYyMDJe27NOiRQsuXWpqKtLS0rjlCxcuQEpKCpaWlhCLxdDT06u0XK1btwZQOlohISEBL1++rGWN/ac2Za8NOTk52NvbIyYmhlsnkUgQExMDR0dHAIC9vT1kZWV5aZKSkpCamsqlqY2GPA/qglgsxrBhw7B582bs2rUL+/btw8uXL2t8/pTXoUMH3LlzB1paWhX2UVH5741U165dQ15eHrd84cIFKCkpwcDAAK1atYKcnByvDouKinDp0iXeOXf27FnexHK1VV0bELrT3mxptzXEs+Ol37NTaAZ5QgghhDQd5ubm2L9/P9zd3SEQCLBgwYJ6u2OekZGBhIQE3jpdXV3MnDkTDg4OCA0NxbBhw3D+/HmsWbMG69atAwAcPnwY9+7dQ7du3aCmpoaoqChIJBJYWlri4sWLiImJQZ8+faClpYWLFy8iIyMD1tbW7x2ni4sLHB0dMWjQICxduhQWFhZIS0vDkSNHMHjwYO4RA5FIBG9vb4SFhSEnJwe+vr7w8vLi7jL7+/sjMDAQrVq1Qrt27RAREYGEhATubuqIESOwaNEiDBo0CIsXL4auri6uXr0KPT29GnV6P6TshYWFuH37Nvf98ePHSEhIgJKSEszMzAAAM2bMgLe3Nzp27IhOnTohPDwcr1+/xujRowEAKioqGDt2LGbMmAF1dXWIxWJMnToVjo6OtZ6EDkCTOw+qs2LFCujq6qJ9+/aQkpLCnj17oKOjA1VV1RqfP+WNHDkSy5Ytw8CBA7k3Djx48AD79+/HrFmzoK+vD6C0rcaNG4dp06bh+fPnCAwMxJQpUyAlJQVFRUV8++233Iz3hoaGWLp0Kd68eYOxY8cCAKZMmYLVq1dj+PDhmDt3LlRUVHDhwgV06tQJlpaW7yx3dW1ASlGnvZmyd3XGreNnUAwRcnO1ISkpgVS550QIIYQQQhrLihUrMGbMGHTp0gUtWrTA7NmzkZNTPzcZdu7ciZ07d/LWhYaGYv78+di9ezcCAgIQGhoKXV1dhISEwMfHB0DpkOL9+/cjKCgI+fn5MDc3x++//w4bGxskJibizJkzCA8PR05ODoyMjLB8+fIqn1evCYFAgKioKMybNw+jR49GRkYGdHR00K1bN2hra3PpzMzMMGTIELi5ueHly5cYMGAA18EEAF9fX2RnZ2PmzJlIT09H69atERkZCXNzcwCld7Ojo6Mxc+ZMuLm5obi4GK1bt8batWtrFKdYLH7vsqelpaF9+/bcclhYGMLCwuDs7MwN6x42bBgyMjIQEBCAp0+fcq+sK18HK1euhJSUFDw8PFBQUABXV1deHdRGhw4dmtR5UB1lZWUsXboUd+7cgbS0NBwcHBAVFcU9F1+T86c8BQUFnDlzBrNnz8aQIUPw6tUrtGzZEr169YJYLObS9erVC+bm5ujfvz8KCwsxYsQIBAUFcduXLFkCiUSCr7/+Gq9evULHjh3x559/Qk1NDQCgoaGBkydPwt/fH87OzpCWlka7du0qzCVQleragJQSMFYH729o5nJycqCiooLs7GzeCdzUFBUVISoqCm5ubpCVlcXO7zYhM6/0qmW3rxRh+3nnRo6QvMvbbUiaJ2rH5o/a8OPQ3NoxPz8fKSkpMDExgUgkauxwmgSJRIKcnByIxWLehF2fqqCgIBw8eLDCyIGmjtqx+fLx8UFWVhb2799PbVhPqvu3v6b9UGqRZkxR97/mu3fuRiNGQgghhBBCCCGkPlCnvRkzcWzDfX/9pH6eEyOEEEIIIQQA71Vj5T9isRjnzjXMxMgTJ06sMo6JEyc2SAxA1XWhpKSEs2fPNlgc5NNAz7Q3Y226OODS7/9DvkQVr/L0kZvzCkpi5XfvSAghhBBCmpygoCDes8RNTVXD9iUSCe+97/UpJCQEfn5+lW5ryMdcq3uEoao3CDRFW7duBYB6myiS1A3qtDdjUtLSUFJ+hvxsVRRDhEuRx9HjqyGNHRYhhBBCCPkIlc0A/7ayZ9obgpaWFrS0tBrkWNWpqi4IqQ80PL6Za2GtyX3P+OdZI0ZCCCGEEEIIIaSuUae9mes00AXSKAAAvMrRRklRcSNHRAghhBBCCCGkrlCnvZlTVlOFWCEVAJAvUUX88TONHBEhhBBCCCGEkLpCnfaPgKqJPPc99WJSI0ZCCCGEEEIIIaQuUaf9I9C2X1cApTM+vn7RcLNmEkIIIYQQQgipX9Rp/wjom5lARe4hACC3WBe3LyU0bkCEEEIIIe9gbGyMVatWNXYYn5ygoCC0a9euscOoN7GxsRAIBMjKymrsUEg59+/fh0AgqPZVeaRq1Gn/SCjp/jcBXVLMpUaMhBBCCCEfE4FAUO3nfd8rfunSJYwfP/6DYuvevTumT5/+QXmQunPz5k14eHjA2NgYAoEA4eHhlaZbu3YtjI2NIRKJ0LlzZ/z999+87fn5+Zg8eTI0NDSgpKQEDw8PPHtGb0kiny7qtH8kLHp04L6/eiLdiJEQQggh5GPy5MkT7hMeHg6xWMxb5+fnx6VljKG4uGZvstHU1ISCgkJ9hU0awZs3b2BqaoolS5ZAR0en0jS7du3CjBkzEBgYiCtXrsDOzg6urq5IT0/n0nz33Xc4dOgQ9uzZg9OnTyMtLQ1DhgxpqGIQ0uRQp/0j0fozeyjKPAUA5BQY4Mn9h40cESGEEEI+Bjo6OtxHRUUFAoGAW759+zaUlZVx9OhR2NvbQygU4q+//kJycjIGDhwIbW1tKCkpwcHBASdOnODl+/bweIFAgC1btmDw4MFQUFCAubk5IiMjPyj2ffv2wcbGBkKhEMbGxli+fDlv+7p162Bubg6RSARtbW14enpy2/bu3QtbW1vIy8tDQ0MDLi4ueP369QfFk5WVhXHjxkFTUxNisRg9e/bEtWvXuO1lQ9c3btwIAwMDKCgowMvLC9nZ2VwaiUSCkJAQ6OvrQygUol27djh27BjvOI8ePcKIESOgrq4ORUVFdOzYERcvXuSl2b59O4yNjaGiooLhw4fj1atXH1x2BwcHLFu2DMOHD4dQKKw0zYoVKzB+/HiMHj0arVu3xoYNG6CgoIBffvkFAJCdnY2ff/4ZK1asQM+ePWFvb4+IiAicO3cOFy5ceHclV6IxzgMfHx8MGjQIixYtgra2NlRVVRESEoLi4mL4+/tDXV0d+vr6iIiI4O03e/ZsWFhYQEFBAaampliwYAGKiop4af73v/+hQ4cOEIlEMDU1RXBw8Dsvlm3ZsgXW1tYQiUSwsrLCunXruG3379+HtLQ09u3bh88//xwikQht2rTB6dOneXmcPn0anTp1glAohK6uLubMmcM7rkQiwdKlS2FmZgahUAhDQ0MsXLiQl8e9e/fQo0cPKCgowM7ODufPn+e2PXjwAO7u7lBTU4OioiJsbGwQFRX1zrr+FFCn/SOipF76DzqDNK4eiW3cYAghhBDyyZgzZw6WLFmCxMREtG3bFrm5uXBzc0NMTAyuXr2Kvn37wt3dHampqdXmExwcDC8vL1y/fh1ubm4YOXIkXr58+V4xxcfHw8vLC8OHD8eNGzcQFBSEBQsWYOvWrQCAy5cvw9fXFyEhIUhKSsKxY8fQrVs3AKWjC0aMGIExY8YgMTERsbGxGDJkCBhj7xVLmaFDhyI9PR1Hjx5FfHw8OnTogF69evHKePfuXezevRuHDh3CsWPHcPXqVUyaNInbvmrVKixfvhxhYWG4fv06XF1d8cUXX+DOnTsAgNzcXDg7O+Px48eIjIzEtWvXMGvWLEgkEi6P5ORkHDx4EIcPH8bhw4dx+vRpLFmypF7LDgCFhYWIj4+Hi4sLt05KSgouLi5c5y0+Ph5FRUW8NFZWVjA0NOR18GqqMc+DkydPIi0tDWfOnMGKFSsQGBiIAQMGQE1NDRcvXsTEiRMxYcIEPHr0iNtHWVkZW7duxa1bt7Bq1Sps3rwZK1eu5LafPXsWo0aNwrRp03Dr1i1s3LgRW7durdA5Lm/Hjh0ICAjAwoULkZiYiEWLFmHBggXYtm0bL11AQAC+++47XL16FY6OjnB3d8eLFy8AAI8fP4abmxscHBxw7do1rF+/Hj///DN++OEHbv+5c+diyZIlWLBgAW7duoWdO3dCW1ubd4x58+bBz88PCQkJsLCwwIgRI7iO/+TJk1FQUIAzZ87gxo0b+PHHH6GkpFSjuv7oMcKys7MZAJadnd3YoVSrsLCQHTx4kBUWFla6/XxkNFszIYatmRDDds7Y0MDRkZp4VxuS5oHasfmjNvw4NLd2zMvLY7du3WJ5eXm89V6HvFjP3T0b/ON1yKvWZYiIiGAqKirc8qlTpxgAdvDgwXfua2Njw1avXs0tGxkZsRUrVrDMzExWUlLCALD58+dz23NzcxkAdvTo0SrzdHZ2ZtOmTat025dffsl69+7NW+fv789at27NGGNs3759TCwWs5ycnAr7xsfHMwDs/v377yxXTZ09e5aJxWKWn5/PW9+qVSu2ceNGxhhjgYGBTFpamj169IjbfvToUSYlJcWePHnCGGNMT0+PLVy4kJeHg4MDmzRpEmOMsY0bNzJlZWX24sWLSuMIDAxkCgoKvHL7+/uzzp07M8ber+wlJSVcO5YxMjJiK1eu5KV7/PgxA8DOnTvHW+/v7886derEGGNsx44dTE5OrsIxHBwc2KxZs94ZS9k5mZmZyRhrvPPA29ubGRkZ8erE0tKSde3alVsuLi5mioqK7Pfff68yn2XLljF7e3tuuVevXmzRokW8NNu3b2e6urpV5tGqVSu2c+dO3rrQ0FDm6OjIGGMsJSWFAWCBgYFcvEVFRUxfX5/9+OOPjDHGvv/+e2ZpackkEgmXx9q1a5mSkhIrKSlhOTk5TCgUss2bN1caQ9kxtmzZwq27efMmA8ASExMZY4zZ2tqyoKCgKsvRXFX1bz9jNe+HyjTKlQJSL+xdu+Ofo0dQIBEj57UBcrNyoKRKr4AjhBBCmrLnec+R/ib93QmbsI4dO/KWc3NzERQUhCNHjuDJkycoLi5GXl7eO++0t23blvuuqKgIsVjMe9a5NhITEzFw4EDeOicnJ4SHh6OkpAS9e/eGkZERTE1N0bdvX/Tt25cbmm9nZ4devXrB1tYWrq6u6NOnDzw9PaGmplbpsWxsbPDgwQMAQNeuXXH06NEKaa5du4bc3FxoaGjw1ufl5SE5OZlbNjQ0RMuWLbllR0dHSCQSJCUlQUFBAWlpaXBycqpQrrJh9gkJCWjfvj3U1dWrrBtjY2MoKytzy7q6ulw917bsTV1Dngdvs7GxgZTUfwObtbW10aZNG25ZWloaGhoavHN8165d+Omnn5CcnIzc3FwUFxdDLP7v7/lr164hLi6Od2e9pKQE+fn5ePPmTYV5Il6/fo3k5GSMHTuWN/FjcXExVFRUeGkdHBy47zIyMujYsSMSExO5enR0dIRAIODVY25uLh49eoSnT5+ioKAAvXr1qrZOyv/GdXV1AQDp6emwsrKCr68vvv32W0RHR8PFxQUeHh689J8y6rR/RGTlZKGs/AQF2WIUQ4S/D51Az69p0g5CCCGkKWsh36LZH1dRUZG37Ofnh+PHjyMsLAxmZmaQl5eHp6cnCgsLq81HVlaWtywQCHjDuuuSsrIyrly5gtjYWERHRyMgIABBQUG4dOkSVFVVcfz4cZw7dw7R0dFYvXo15s2bh4sXL8LExKRCXlFRUdxzx/Ly8pUeLzc3F7q6uoiNja2wTVVVtc7KVdXxy6uunqWlpWtV9tpo0aIFpKWlK8wE/+zZM27iOh0dHRQWFiIrK4tXL+XT1KW6PA/eVlk9V1f358+fx8iRIxEcHAxXV1eoqKjgjz/+4D2Dn5ubi+Dg4Eon5hOJRBXW5ebmAgA2b96Mzp0787ZJS9fd5NU1Oe8Afp2UXQAoK/+4cePg6uqKI0eOIDo6GosXL8by5csxderUOouzuaJO+0emRWstPP//x30ybtKrMQghhJCmbteAXY0dQp2Li4uDj48PBg8eDKC043D//v0GjcHa2hpxcXEV4rKwsOA6KzIyMnBxcYGLiwsCAwOhqqqKkydPYsiQIRAIBHBycoKTkxMCAgJgZGSEAwcOYMaMGRWOZWRk9M54OnTogKdPn0JGRgbGxsZVpktNTUVaWhr09PQAABcuXICUlBQsLS0hFouhp6eHuLg4ODs788rVqVMnAKV3Mrds2YKXL19We7e9OrUpe23IycnB3t4eMTExGDRoEIDSDltMTAymTJkCALC3t4esrCxiYmLg4eEBAEhKSkJqaiocHR1rfcyGPA8+1Llz52BkZIR58+Zx68pGcJTp0KEDkpKSYGZmVqM8tbW1oaenh3v37mHkyJHVpr18+TL69esHoPROfHx8PNcu1tbW2LdvHxhjXGc7Li4OysrK0NfXh5aWFuTl5RETE4Nx48bVuMxvMzAwwMSJEzFx4kTMnTsXmzdvpk47qNP+0en0RS/cOX8eJRAiN0cHJUXFkJalZiaEEEJIwzE3N8f+/fvh7u4OgUCABQsW1Nsd84yMDCQkJPDW6erqYubMmXBwcEBoaCiGDRuG8+fPY82aNdys2YcPH8a9e/fQrVs3qKmpISoqChKJBJaWlrh48SJiYmLQp08faGlp4eLFi8jIyIC1tfV7x+ni4gJHR0cMGjQIS5cuhYWFBdLS0nDkyBEMHjyYe8RAJBLB29sbYWFhyMnJga+vL7y8vLi7zP7+/ggMDESrVq3Qrl07REREICEhATt27AAAjBgxAosWLcKgQYOwePFi6Orq4urVq9DT06tRp/dDyl5YWIjbt29z3x8/foyEhAQoKSlxncwZM2bA29sbHTt2RKdOnRAeHo7Xr19j9OjRAAAVFRWMHTsWM2bMgLq6OsRiMaZOnQpHR0d89tlnta73pnYeVMfc3Bypqan4448/4ODggCNHjuDAgQO8NAEBARgwYAAMDQ3h6ekJKSkpXLt2Df/88w9vUrjygoOD4evrCxUVFfTt2xcFBQW4fPkyMjMzeRcftmzZgjZt2sDGxgYrV65EZmYmxowZAwCYNGkSwsPDMXXqVEyZMgVJSUkIDAzEjBkzICUlBZFIhNmzZ2PWrFmQk5ODk5MTMjIycPPmTYwdO7ZG5Z8+fTr69esHCwsLZGZm4tSpU/VW181OPT1v36x8LBPRldn53QZuQroLR040UHSkJprbpEmkctSOzR+14cehubVjdZMRNRdVTURXNulXmZSUFNajRw8mLy/PDAwM2Jo1aypMGlfZRHQHDhzg5aOiosIiIiKqjMfZ2ZkBqPAJDQ1ljDG2d+9e1rp1ayYrK8sMDQ3ZsmXLuH3Pnj3LnJ2dmZqaGpOXl2dt27Zlu3btYowxduvWLebq6so0NTWZUChkFhYWvEn03ldOTg6bOnUq09PTY7KysszAwICNHDmSpaamMsZKJ4mzs7Nj69atY3p6ekwkEjFPT0/28uVLLo+SkhIWFBTEWrZsyWRlZZmdnV2Fyfru37/PPDw8mFgsZgoKCqxjx47s4sWLvGOUt3LlSmZkZPTeZS+biC45ObnS9nB2dualX716NTM0NGRycnKsU6dO7MKFC7zteXl5bNKkSUxNTY0pKCiwwYMHcxPxvUtl52RjnAfe3t5s4MCBvHWVTZz49oR9/v7+TENDgykpKbFhw4axlStX8n5zjDF27Ngx1qVLFyYvL8/EYjHr1KkT27RpU7Xx7Nixg7Vr147JyckxNTU11q1bN7Z//37G2H+TxG3evJl16tSJycnJsdatW7OTJ0/y8oiNjWUODg5MTk6O6ejosNmzZ7OioiJue0lJCfvhhx+YkZERV9dlk+aVHePq1atc+szMTAaAnTp1ijHG2JQpU1irVq2YUChkmpqa7Ouvv2bPnz+vtlzNQV1MRCdgrA7e39DM5eTkQEVFBdnZ2byJHpqaoqIiREVFwc3NrcLzMOUdWfMr7v+jDwDQ1voXniETGypE8g41bUPStFE7Nn/Uhh+H5taO+fn5SElJgYmJSaXPnn6KJBIJcnJyIBaLeRN2faqCgoJw8ODBCiMHmjpqx+bt/v37MDExwZkzZ+Dk5ERtWMeq+7e/pv3QRm2RM2fOwN3dHXp6ehAIBDh48GCVaSdOnAiBQIDw8HDe+pcvX2LkyJEQi8VQVVXF2LFjuQkXPlXt+ztDgBIAwKsXapCUlDRyRIQQQgghhBBC3kejdtpfv34NOzs7rF27ttp0Bw4cwIULF7gJOcobOXIkbt68iePHj+Pw4cM4c+YMvvnmm/oKuVnQMzGCiqh00oo3JZq4EvNXI0dECCGEEEKaOyUlpUo/YrEY586da5AYJk6cWGUcEyc23OjSqmJQUlLC2bNnGywO8mlo1BnK+vXrx81QWJXHjx9j6tSp+PPPP9G/f3/etsTERBw7dgyXLl3iJu9YvXo13NzcEBYWVmknHwAKCgpQUFDALefk5AAoHWZX9rqOpqgstprEqGIsg6zSeUCQcvYW7Hp0qc/QSA3Vpg1J00Xt2PxRG34cmls7FhUVgTEGiURSb5OyNTdlT2mW1cunLiAgAAEBAU22Lq5cuVLpesYYxGJxg7RjUFBQlTO3i8XiBqu7quoCAFq2bNlk27AyhoaGKC4uxqtXr+i3WA8kEgkYYygqKqrwmr2a/verSU8rLpFI8PXXX8Pf3x82NjYVtp8/fx6qqqpchx0onZlTSkoKFy9e5F4z8rbFixcjODi4wvro6GgoKCjUXQHqyfHjx9+ZpkBDDgIUg0EGr160wKFDhyEtTc+nNBU1aUPS9FE7Nn/Uhh+H5tKOMjIy0NHRQW5u7jvfV/6pefXqVWOHQGpAS0ur2u0N0Y4ikajaOSHKbsbVt+rqoqnfCKwO/RbrXmFhIfLy8nDmzBkUFxfztr1586ZGeTTpTvuPP/4IGRkZ+Pr6Vrr96dOnFX4wMjIyUFdXx9OnT6vMd+7cubwrdDk5OTAwMECfPn2a/ER0x48fR+/evWs04c6eKxHIzDNDnkQDllKF6OTWqwGiJNWpbRuSponasfmjNvw4NLd2zM/Px8OHD6GkpEQT0f0/xhhevXoFZWVl7t3PpPmhdmz+qA3rT35+PuTl5dGtW7dKJ6KriSbbaY+Pj8eqVatw5cqVOj9xhEIhhEJhhfWysrLN4j/6NY1TzVwBmddLv6f9fQ+yg/rWc2SkpprLuUaqR+3Y/FEbfhyaSzuWlJRAIBBASkqKZmf+f2XDcMvqhTRP1I7NH7Vh/ZGSkoJAIKj0v1U1/W9Xk22Rs2fPIj09HYaGhpCRkYGMjAwePHiAmTNnwtjYGACgo6OD9PR03n7FxcV4+fIldHR0GiHqpqWThyukUTr8LidLF0WFzXOYDiGEEEIIIYR8qppsp/3rr7/G9evXkZCQwH309PTg7++PP//8EwDg6OiIrKwsxMfHc/udPHkSEokEnTt3bqzQmwwNbU2oKJbOIp8vUcGFyOhGjogQQgghhBBCSG006vD43Nxc3L17l1tOSUlBQkIC1NXVYWhoCA0NDV56WVlZ6OjowNLSEgBgbW2Nvn37Yvz48diwYQOKioowZcoUDB8+vMqZ4z81GlYqePn/1zTSLqcCno0bDyGEEEIIIYSQmmvUO+2XL19G+/bt0b59ewDAjBkz0L59ewQEBNQ4jx07dsDKygq9evWCm5sbPv/8c2zatKm+Qm52Og/pAxnkAwBysvVQkJfXyBERQgghhADGxsZYtWpVY4fxyQkKCkK7du0aO4x6ExsbC4FAgKysrEaNw8fHB4MGDfqgPO7fvw+BQICEhIQ6iak5+BTLXBON2mnv3r07GGMVPlu3bq00/f379zF9+nTeOnV1dezcuROvXr1CdnY2fvnlFygpKdV/8M2EioY6VJRSAQCFTBnnD9AQeUIIIYTUnEAgqPYTFBT0XvleunQJ48eP/6DYunfvXuFvQ9J4bt68CQ8PDxgbG0MgECA8PLzSdGvXroWxsTFEIhE6d+6Mv//+m7c9Pz8fkydPhoaGBpSUlODh4YFnz541QAkIaZqa7DPtpO60aNOC+/4sIa0RIyGEEEJIc/PkyRPuEx4eDrFYzFvn5+fHpWWMVXgPcVU0NTWhoKBQX2GTRvDmzRuYmppiyZIlVU4KvWvXLsyYMQOBgYG4cuUK7Ozs4Orqyptc+rvvvsOhQ4ewZ88enD59GmlpaRgyZEhDFeOjV1hY2NghkFqiTvsn4LMhrpAVvAEAZL8ywJvc3EaOiBBCCCHNhY6ODvdRUVGBQCDglm/fvg1lZWUcPXoU9vb2EAqF+Ouvv5CcnIyBAwdCW1sbSkpKcHBwwIkTJ3j5vj08XiAQYMuWLRg8eDAUFBRgbm6OyMjID4p93759sLGxgVAohLGxMZYvX87bvm7dOpibm0MkEkFbWxuenv9N/rN3717Y2tpCXl4eGhoacHFxwevXrz8onqysLIwbNw6ampoQi8Xo2bMnrl27xm0vG7q+ceNGGBgYQEFBAV5eXsjOzubSSCQShISEQF9fH0KhEO3atcOxY8d4x3n06BFGjBgBdXV1KCoqomPHjrh48SIvzfbt22FsbAwVFRUMHz4cr169+uCyOzg4YNmyZRg+fHilr1cGgBUrVmD8+PEYPXo0WrdujQ0bNkBBQQG//PILACA7Oxs///wzVqxYgZ49e8Le3h4RERE4d+4cLly48O5KrkRjngdhYWHQ1dWFhoYGJk+ejKKi/97mJBAIcPDgQV56VVXVCqOOb9++jS5dukAkEqFNmzY4ffo0b/s///yDfv36QUlJCdra2vj666/x/Plzbnv37t0xZcoUTJ8+HS1atICrq2uV8W7ZsgXW1tYQiUSwsrLCunXruG1lQ9f/+OOPauM5ffo0OnXqBKFQCF1dXcyZM4d3MU8ikWDp0qUwMzODUCiEoaEhFi5cyMvj3r176NGjBxQUFGBnZ4fz589z2x48eAB3d3eoqalBUVERNjY2iIqKqrJMHwPqtH8ClMTKECs/BAAUMQWc33vsHXsQQgghhNTcnDlzsGTJEiQmJqJt27bIzc2Fm5sbYmJicPXqVfTt2xfu7u5ITU2tNp/g4GB4eXnh+vXrcHNzw8iRI/Hy5cv3iik+Ph5eXl4YPnw4bty4gaCgICxYsIDrEF2+fBm+vr4ICQlBUlISjh07hm7dugEoHV0wYsQIjBkzBomJiYiNjcWQIUPAGHuvWMoMHToU6enpOHr0KOLj49GhQwf06tWLV8a7d+9i9+7dOHToEI4dO4arV69i0qRJ3PZVq1Zh+fLlCAsLw/Xr1+Hq6oovvvgCd+7cAVA60bOzszMeP36MyMhIXLt2DbNmzeLeww0AycnJOHjwIA4fPozDhw/j9OnTWLJkSb2WHSi9wxsfHw8XFxdunZSUFFxcXLhOWXx8PIqKinhprKysYGhoyOu41VRjngenTp1CcnIyTp06hW3btmHr1q1VPgZcHX9/f8ycORNXr16Fo6Mj3N3d8eLFCwClF4J69uyJ9u3b4/Llyzh27BiePXsGLy8vXh7btm2DnJwc4uLisGHDhkqPs3v3bgQFBWHhwoVITEzEokWLsGDBAmzbtq3G8Tx+/Bhubm5wcHDAtWvXsH79evz888/44YcfuP3nzp2LJUuWYMGCBbh16xZ27twJbW1t3jHmzZsHPz8/JCQkwMLCAiNGjOA6/pMnT0ZBQQHOnDmDGzdu4Mcff/z4H49mhGVnZzMALDs7u7FDqVZhYSE7ePAgKywsrPW+J3ccYGsmxLA1E2LY7zPX10N0pCY+pA1J00Ht2PxRG34cmls75uXlsVu3brG8vDze+ntDPNi/3Zwb/HNviEetyxAREcFUVFS45VOnTjEA7ODBg+/c18bGhq1evZpbNjIyYitWrGCZmZmspKSEAWDz58/ntufm5jIA7OjRo1Xm6ezszKZNm1bpti+//JL17t2bt87f35+1bt2aMcbYvn37mFgsZjk5ORX2jY+PZwDY/fv331mumjp79iwTi8UsPz+ft75Vq1Zs48aNjDHGAgMDmbS0NHv06BG3/ejRo0xKSoo9efKEMcaYnp4eW7hwIS8PBwcHNmnSJMYYYxs3bmTKysrsxYsXlcYRGBjIFBQUeOX29/dnnTt3Zoy9X9lLSkq4dixjZGTEVq5cyUv3+PFjBoCdO3eOt97f35916tSJMcbYjh07mJycXIVjODg4sFmzZr0zlrJzMjMzkzHWeOeBt7c3MzIyYsXFxdy6oUOHsmHDhnHLANiBAwd4+6moqLCIiAjGGGMpKSkMAFuyZAm3vaioiOnr67Mff/yRMcZYaGgo69OnDy+Phw8fMgAsKSmJMVb6O2nfvn218ZaUlDATExP222+/8daHhoYyR0fHGsfz/fffM0tLSyaRSLg0a9euZUpKSqykpITl5OQwoVDINm/eXGkcZcfYsmULt+7mzZsMAEtMTGSMMWZra8uCgoKqLU9TUtW//YzVvB/aqK98Iw2ny2BX3D0bjUKmjOxcQ2S/eAkVDfXGDosQQgj55BU/f47iZj7JVseOHXnLubm5CAoKwpEjR/DkyRMUFxcjLy/vnXfa27Zty31XVFSEWCzmPetcG4mJiRg4cCBvnZOTE8LDw1FSUoLevXvDyMgIpqam6Nu3L/r27csNzbezs0OvXr1ga2sLV1dX9OnTB56enlBTU6v0WDY2Nnjw4AEAoGvXrjh69GiFNNeuXUNubm6FVxrn5eUhOTmZWzY0NETLli25ZUdHR0gkEiQlJUFBQQFpaWlwcnKqUK6yYfYJCQlo37491NWr/jvP2NgYysrK3LKuri5Xz7Ute1PXkOfB22xsbCAtLc0t6+rq4saNG7Uug6OjI/ddRkYGHTt2RGJiIoDS8+rUqVOV3mlOTk6GhYUFAMDe3r7aY7x+/RopKSkYP348JkyYwK0vLi6GiopKjeNJTEyEo6MjBAIBl8bJyQm5ubl49OgRnj59ioKCAvTq1avaeMr/W6CrqwsASE9Ph5WVFXx9ffHtt98iOjoaLi4u8PDw4KX/GFGn/RMhlJeHikoaMrIsUQwRLuz7E67fjGjssAghhJBPnkyLFu9O1MSPq6ioyFv28/PD8ePHERYWBjMzM8jLy8PT0/OdE2DJysrylgUCAW9Yd11SVlbGlStXEBsbi+joaAQEBCAoKAiXLl2Cqqoqjh8/jnPnziE6OhqrV6/GvHnzcPHiRZiYmFTIKyoqintWWV5evtLj5ebmQldXF7GxsRW2qaqq1lm5qjp+edXVs7S0dK3KXhstWrSAtLR0hZngnz17xk1cp6Ojg8LCQmRlZfHqpXyaulSX58Hb3nU+CwSCCkPtyz/zXhO5ublwd3fHjz/+WGFbWWcXqPgbrSwfANi4cSOvUw6Ad+HhQ9Xk/AT4dVd2AaCs7saNGwdXV1ccOXIE0dHRWLx4MZYvX46pU6fWWZxNDXXaPyF6DsbIOF76/UUiTUZHCCGENAUm+/Y2dgh1Li4uDj4+Phg8eDCA0g7B/fv3GzQGa2trxMXFVYjLwsKC64TIyMjAxcUFLi4uCAwMhKqqKk6ePIkhQ4ZAIBDAyckJTk5OCAgIgJGREQ4cOIAZM2ZUOJaRkdE74+nQoQOePn0KGRkZGBsbV5kuNTUVaWlp0NPTAwBcuHABUlJSsLS0hFgshp6eHuLi4uDs7MwrV6dOnQCU3qHcsmULXr58We3d9urUpuy1IScnB3t7e8TExHDvMJdIJIiJicGUKVMAlN4RlpWVRUxMDDw8PAAASUlJSE1NrdCZrImGPA9qS1NTE0+ePOGW79y5gzdv3lRId+HCBe45++LiYsTHx3P11aFDB+zbtw/GxsaQkXn/rp22tjZ0dXWRkpKCr7/+utq01cVjbW2Nffv2gTHGdbbj4uKgrKwMfX19aGlpQV5eHjExMRg3btx7x2tgYICJEydi4sSJmDt3LjZv3kyddvJxcPyiN+6c3I83JS2QlWeMB4l3YGRt3thhEUIIIeQjY25ujv3798Pd3R0CgQALFiyotzvmGRkZSEhI4K3T1dXFzJkz4eDggNDQUAwbNgznz5/HmjVruNmwDx8+jHv37qFbt25QU1NDVFQUJBIJLC0tcfHiRcTExKBPnz7Q0tLCxYsXkZGRAWtr6/eO08XFBY6Ojhg0aBCWLl0KCwsLpKWl4ciRIxg8eDD3iIFIJIK3tzfCwsKQk5MDX19feHl5cXeZ/f39ERgYiFatWqFdu3aIiIhAQkICduzYAQAYMWIEFi1ahEGDBmHx4sXQ1dXF1atXoaenV6NO74eUvbCwELdv3+a+P378GAkJCVBSUoKZmRkAYMaMGfD29kbHjh3RqVMnhIeH4/Xr1xg9ejQAQEVFBWPHjsWMGTOgrq4OsViMqVOnwtHREZ999lmt672pnQfl9ezZE2vWrIGjoyNKSkowe/bsCnfngdL32pubm8Pa2horV65EZmYmxowZA6B0UrbNmzdjxIgRmDVrFtTV1XH37l388ccf2LJlS63uks+ZMwdz5syBqqoq+vbti4KCAly+fBmZmZm8ixTVxTNp0iSEh4dj6tSpmDJlCpKSkhAYGIgZM2ZASkoKIpEIs2fPxqxZsyAnJwcnJydkZGTg5s2bGDt2bI3inD59Ovr16wcLCwtkZmbi1KlTddYmTRV12j8h0rIyEGtn4k1aCzBI48qBU9RpJ4QQQkidW7FiBcaMGYMuXbqgRYsWmD17NnJycurlWDt37sTOnTt560JDQzF//nzs3r0bAQEBCA0Nha6uLkJCQuDj4wOgdEj6/v37ERQUhPz8fJibm+P333+HjY0NEhMTcebMGYSHhyMnJwdGRkZYvnw5+vXr995xCgQCREVFYd68eRg9ejQyMjKgo6ODbt268WbONjMzw5AhQ+Dm5oaXL19iwIABvNdu+fr6Ijs7GzNnzkR6ejpat26NyMhImJuX/k0nJyeH6OhozJw5E25ubiguLkbr1q2xdu3aGsUpFovfu+xpaWlo3749txwWFoawsDA4OztzjwUMGzYMGRkZCAgIwNOnT7lX1pWvg5UrV0JKSgoeHh4oKCiAq6srrw5qo0OHDk3qPChv+fLlGD16NLp27Qo9PT2sWrUK8fHxFdItWbIES5YsQUJCAszMzBAZGYkW//94S9nIi9mzZ6NPnz4oKCiAkZER+vbtCymp2r0obNSoUVBXV8fy5cvh7+8PRUVF2NraYvr06TWOp2XLloiKioK/vz/s7Oygrq6OsWPHYv78+dz+CxYsgIyMDAICApCWlgZdXV1MnDixxnGWlJRg8uTJePToEcRiMfr27YuVK1fWqqzNjYC9/SDFJygnJwcqKirIzs6GWCxu7HCqVFRUhKioKLi5uVV6Fa4m/r1yA8c3ZQAAFGWeYtSqYZCqw+dUSPXqog1J46N2bP6oDT8Oza0d8/PzkZKSAhMTE4hEosYOp0mQSCTIycmBWCyudQfjYxQUFISDBw9WGDnQ1FE7Nn81acP79+/DxMQEV69eRbt27Ro2wGasun/7a9oPpV/VJ8aigy1UhCkAgNfFOrj0Z2zjBkQIIYQQQgghpErUaf8EqZvLcd8fnP23ESMhhBBCCCHNhZKSUqUfsViMc+fONUgMEydOrDKO2gyx/lBVxaCkpISzZ882WBzk00DPtDcTR/f8gRs3b6KwqBi6qmJ0cu7x3nl1Ge6GhwsuopiJkJWlj9ysHCipNt3HAgghhBBCPgVBQUEICgpq7DCqVNWwfYlEwnvve30KCQmBn59fpdsa8jHX6h5haNmyZYPF0ZCMjY0rvKKONAzqtDcTmS9f4g2kAVlp5GRmflBeqi00oCp+gOfZlihiiojbfYTe2U4IIYQQQqpVNgP828qeh24IWlpa0NLSapBjVaequiCkPtDw+Gai/OsaSkqKPzg//c7G3PcXt+id7YQQQgghhBDSFFGnvZmQlv5vUERx8Yd32j9z7w0F6dJZ5LPyS9/ZTgghhBBCCCGkaaFOezMhw7vTXvLB+UnLykBFJwsAuHe2E0IIIYQQQghpWqjT3kxIy5TrtNfBnXYAsB3gxH3PTlOCpA4uBhBCCCGEEEIIqTvUaW8myg+Pr4s77QBg3r4NVOmd7YQQQgghhBDSZFGnvZmQkS3XaS+uuzviahb0znZCCCGENDxjY2OsWrWqscNoUAKBAAcPHmzsMD5ZW7duhaqqamOH8VGKjY2FQCBAVlZWY4fyUaJOezPBu9MuqbtOe5dhbpAR5AMAsrL0UVRYVGd5E0IIIaT5EwgE1X7e973ily5dwvjx4z8otu7du2P69OkflEdzc/36dXTt2hUikQgGBgZYunTpO/dJTU1F//79oaCgAC0tLfj7+1eY2Dg2NhYdOnSAUCiEmZkZtm7dytt+5swZuLu7Q09Pjy4+VOLJkyf48ssvYWFhASkpqSrPyz179sDKygoikQi2traIioribWeMISAgALq6upCXl4eLiwvu3KEJoz911GlvJnh32uvw2XPVFhoQKzwEABQxRdy9eqPO8iaEEEJI8/fkyRPuEx4eDrFYzFvn5+fHpWWM1fgtN5qamlBQUKivsJuloqLqb57k5OSgT58+MDIyQnx8PJYtW4agoCBs2rSpyn1KSkrQv39/FBYW4ty5c9i2bRu2bt2KgIAALk1KSgr69++PHj16ICEhAdOnT8e4cePw559/cmlev34NOzs7rF279sML+hEqKCiApqYm5s+fDzs7u0rTnDt3DiNGjMDYsWNx9epVDBo0CIMGDcI///zDpVm6dCl++uknbNiwARcvXoSioiJcXV2Rn5/fUEUhTRB12psJmXJ32ut6wjih6n/fH96gK3mEEEII+Y+Ojg73UVFRgUAg4JZv374NZWVlHD16FPb29hAKhfjrr7+QnJyMgQMHQltbG0pKSnBwcMCJEyd4+b49PF4gEGDLli0YPHgwFBQUYG5ujsjIyA+Kfd++fbCxsYFQKISxsTGWL1/O275u3TqYm5tDJBJBW1sbnp6e3La9e/fC1tYW8vLy0NDQgIuLC16/fv1B8ZR3//59CAQC7Nq1C87OzhCJRNixY0e1++zYsQOFhYX45ZdfYGNjg+HDh8PX1xcrVqyocp/o6GjcunULv/32G9q1a4d+/fohNDQUa9euRWFhIQBgw4YNMDExwfLly2FtbY0pU6bA09MTK1eu5PLp168ffvjhBwwePPi9yltQUAA/Pz+0bNkSioqK6Ny5M2JjY7ntZUPXDx48yLWJq6srHj58yMtn/fr1aNWqFeTk5GBpaYnt27fztmdlZWHChAnQ1taGSCRCmzZtcPjwYV6aP//8E9bW1lBSUkLfvn3x5MkTbltsbCw6deoERUVFqKqqwsnJCQ8ePHhn+crO51GjRkFFRaXSNKtWrULfvn3h7+8Pa2trhIaGokOHDlizZg2A0ote4eHhmD9/PgYOHIi2bdvi119/RVpaWrUjGyQSCRYvXgwTExPIy8vDzs4Oe/fu5ZVJIBDgyJEjaNu2LUQiET777DPexQIAiIyMhK2tbZW/l4KCAsyePRsGBgbciIyff/6ZlyY+Ph4dO3aEgoICunTpgqSkJG7btWvX0KNHDygrK0MsFsPe3h6XL19+Z90S6rQ3GzKy/z17XlIiqdO8xQYa3PdXjzLrNG9CCCGEfPzmzJmDJUuWIDExEW3btkVubi7c3NwQExODq1evom/fvnB3d0dqamq1+QQHB8PLywvXr1+Hm5sbRo4ciZcvX75XTPHx8fDy8sLw4cNx48YNBAUFYcGCBdyw78uXL8PX1xchISFISkrCsWPH0K1bNwClowtGjBiBMWPGIDExEbGxsRgyZAgYY+8VS3XmzJmDadOmITExEa6urtWmPX/+PLp16wY5uf/+LnR1dUVSUhIyMyv/G+78+fOwtbWFtrY2b5+cnBzcvHmTS+Pi4sLbz9XVFefPn3/fYlUwZcoUnD9/Hn/88QeuX7+OoUOHom/fvryh32/evMHChQvx66+/Ii4uDllZWRg+fDi3/cCBA5g2bRpmzpyJf/75BxMmTMDo0aNx6lTpq4slEgn69euHuLg4/Pbbb7h16xaWLFkC6XKvTn7z5g3CwsKwfft2nDlzBqmpqdxokeLiYgwaNAjOzs64fv06zp8/j2+++QYCgaBO6uBd9ZySkoKnT5/y0qioqKBz587VtsXixYvx66+/YsOGDbh58ya+++47fPXVVzh9+jQvnb+/P5YvX45Lly5BU1MT7u7u3OiO+Ph4jB49GsOGDav09wIAo0aNwu+//46ffvoJiYmJ2LhxI5SUlHjHmDdvHpYvX47Lly9DRkYGY8aM4baNHDkS+vr6uHTpEuLj4zFnzhzIysrWrhI/UTLvTkKaAulyw+MlkrrttBvZWSHpQjoAoCCHruMQQgghDWn3okt4k1PY4MdVEMvB63uHOskrJCQEvXv35pbV1dV5Q4RDQ0Nx4MABREZGYsqUKVXm4+PjgxEjRgAAFi1ahJ9++gl///03+vbtW+uYVqxYgV69emHBggUAAAsLC9y6dQvLli2Dj48PUlNToaioiAEDBkBZWRlGRkZo3749gNJOe3FxMYYMGQIjIyMAgK2tba1jqInp06djyJAhNUr79OlTmJiY8NaVdcafPn0KNTW1Svcp32F/e5/q0uTk5CAvLw/y8vI1K0wVUlNTERERgdTUVOjp6QEA/Pz8cOzYMURERGDRokUASh8PWLNmDTp37gwA2LZtG6ytrfH333+jU6dOCAsLg4+PDyZNmgQAmDFjBi5cuICwsDD06NEDJ06cwN9//43ExERYWFgAAExNTXmxFBUVYcOGDWjVqhWA0osJISEhAEofP8jOzsaAAQO47dbW1h9U9vKqqufy7VC2rqo0bysoKMCiRYtw4sQJODo6Aigt819//YWNGzfC2dmZSxsYGMj9Trdt2wZ9fX0cOHAAXl5eWLlyJZydnTF//nxISUlV+L38+++/2L17N44fP85dVHi7bgFg4cKF3DHnzJmD/v37Iz8/HyKRCKmpqfD394eVlRUAwNzcvHYV+AmjHlozUf4qVF1ORAcApm2sICt4AwAoyK/4jz0hhBBC6s+bnEK8zipo8E9dXijo2LEjbzk3Nxd+fn6wtraGqqoqlJSUkJiY+M477W3btuW+KyoqQiwWIz09/b1iSkxMhJOTE2+dk5MT7ty5g5KSEvTu3RtGRkYwNTXF119/jR07duDNm9K/h+zs7NCrVy/Y2tpi6NCh2Lx5c5V3sgHAxsYGSkpKUFJSQr9+/WoV59t19zG6ceMGSkpKYGFhwdWTkpISTp8+jeTkZC6djIwMHBz+u5BkZWUFVVVVJCYmAqi6Tcu2JyQkQF9fn+uwV0ZBQYHrkAOArq4ud46pq6vDx8cHrq6ucHd3x6pVq3hD55uiu3fv4s2bN+jduzevbn/99Vde3QLgOvVAaVktLS25urt9+zZ3saRM+d9LQkICpKWleRcBKlP+N6yrqwsAXP3OmDED48aNg4uLC5YsWVIhPlI1utPeTEjL/Ndpr+s77dKyMlCQTUd2oTHeSDSQ/vgptFrq1OkxCCGEEFI5BbHcuxM18eMqKirylv38/HD8+HGEhYXBzMwM8vLy8PT05J6hrsrbQ2UFAkGd/91TRllZGVeuXEFsbCyio6MREBCAoKAgXLp0Caqqqjh+/DjOnTuH6OhorF69GvPmzcPFixcr3OkGgKioKG6YcW3vSr9dd9XR0dHBs2fPeOvKlnV0Kv/bTUdHB3///Xe1+1SVr1gs/uC77EDpRRxpaWnEx8fzhqoDqDC8+kPUJNbKzrHyjz1ERETA19cXx44dw65duzB//nwcP34cn3322QfHV1U9l2+HsnVlHd6y5Xbt2lWaZ25uLgDgyJEjaNmyJW+bUCj84JjL1PQ8KF+/ZY8VlP2Gg4KC8OWXX+LIkSM4evQoAgMD8ccff7z3PAmfEuq0NxOycvXXaQcAOaUC4P8fGbv7dwK0Btd+GBohhBBCaq+uhqg3JXFxcfDx8eH+GM/NzcX9+/cbNAZra2vExcVViMvCwoLrOMrIyMDFxQUuLi4IDAyEqqoqTp48iSFDhkAgEMDJyQlOTk4ICAiAkZERDhw4gBkzZlQ4VtkQ+vrm6OiIefPmoaioiOscHT9+HJaWlpUOjS/bZ+HChUhPT4eWlha3j1gsRuvWrbk0b7967Pjx47w7sx+iffv2KCkpQXp6Orp27VpluuLiYly+fBmdOnUCACQlJSErK4sbol7Wpt7e3tw+cXFxXDnatm2LR48e4d9//632bntN4m3fvj3mzp0LR0dH7Ny5s0467Y6OjoiJieG9Dq58PZuYmEBHRwcxMTFcJz0nJwcXL17Et99+W2merVu3hlAoRGpq6jvvgl+4cAGGhoYAgMzMTPz7779c3VpZWeHixYu89OV/L7a2tpBIJDh9+nSF5/Jrw8LCAhYWFvjuu+8wYsQIREREUKe9BqjT3kzI8O601/0kKAqa8lyn/Xny4zrPnxBCCCGfDnNzc+zfvx/u7u4QCARYsGBBvd0xz8jIQEJCAm+drq4uZs6cCQcHB4SGhmLYsGE4f/481qxZg3Xr1gEADh8+jHv37qFbt25QU1NDVFQUJBIJLC0tcfHiRcTExKBPnz7Q0tLCxYsXkZGRUafPN7+PL7/8EsHBwRg7dixmz56Nf/75B6tWreLN8n7gwAHMnTsXt2/fBgD06dMHrVu3xtdff42lS5fi6dOnmD9/PiZPnszdiZ04cSLWrFmDWbNmYcyYMTh58iR2796NI0eOcPnm5ubi7t273HJKSgoSEhKgrq4OfX39auO2sLDAyJEjMWrUKCxfvhzt27dHRkYGYmJi0LZtW/Tv3x9A6V3aqVOn4qeffoKMjAymTJmCzz77jOvE+/v7w8vLC+3bt4eLiwsOHTqE/fv3c28mcHZ2Rrdu3eDh4YEVK1bAzMwMt2/fhkAgqNG8CCkpKdi0aRO++OIL6OnpISkpCXfu3MGoUaNq0jzceZibm8udl3JyctxFhWnTpsHZ2RnLly9H//798ccff+Dy5cvcK/sEAgGmT5+OH374Aebm5jAxMcGCBQugp6eHQYMGVXpMZWVl+Pn54bvvvoNEIsHnn3+O7OxsxMXFQSwW8y5whISEQENDA9ra2pg3bx5atGjB5Ttjxgx07twZP/zwA4YPH17h92JsbAxvb2+MGTMGP/30E+zs7PDgwQOkp6fDy8vrnXWTl5cHf39/eHp6wsTEBI8ePcKlS5fg4eFRo7r95DHCsrOzGQCWnZ3d2KFU6cGdf1lgYCALDAxk4T8E13n+F46cYGsmxLA1E2LYH7PX1Xn+pFRhYSE7ePAgKywsbOxQyAegdmz+qA0/Ds2tHfPy8titW7dYXl5eY4fy3iIiIpiKigq3fOrUKQaAZWZm8tKlpKSwHj16MHl5eWZgYMDWrFnDnJ2d2bRp07g0RkZGbMWKFSwzM5OVlJQwAOzAgQO8fFRUVFhERESV8Tg7OzMAFT6hoaGMMcb27t3LWrduzWRlZZmhoSFbtmwZt+/Zs2eZs7MzU1NTY/Ly8qxt27Zs165djDHGbt26xVxdXZmmpiYTCoXMwsKCrV69+r3qrLzyZUxJSWEA2NWrV2uVx7Vr19jnn3/OhEIha9myJVuyZAlve0REBHv7T/z79++zfv36MXl5edaiRQs2c+ZMVlRUxEtz6tQp1q5dOyYnJ8dMTU0r1HtZW7/98fb2ZiUlJVw7VqWwsJAFBAQwY2NjJisry3R1ddngwYPZ9evXubhVVFTYvn37mKmpKRMKhczFxYU9ePCAl8+6deuYqakpk5WVZRYWFuzXX3/lbX/x4gUbPXo009DQYCKRiLVp04YdPnyYd4zyDhw4wNXX06dP2aBBg5iuri6Tk5NjRkZGLCAgoNpylVdZ/RgZGfHS7N69m1lYWDA5OTlmY2PDjhw5wtsukUjYggULmLa2NhMKhaxXr14sKSmp2uNKJBIWHh7OLC0tmaysLNPU1GSurq7s9OnTjLH/2u7QoUPMxsaGycnJsU6dOrFr165xeZSUlLBt27ZV+XthrPTfsO+++46rHzMzM/bLL7/wjlH+34KrV68yACwlJYUVFBSw4cOHMwMDAyYnJ8f09PTYlClTmvW/hzVV3b/9Ne2HChirh3dXNDM5OTlQUVFBdnY2xGJxY4dTqbTU+9j0y1YAgIq0AN8tCKzT/LOev8CO+VcBSEEs+xBfr/Z+5z6k9oqKihAVFQU3Nzd6xUUzRu3Y/FEbfhyaWzvm5+cjJSUFJiYmEIlEjR1OkyCRSJCTkwOxWAwpKZofubmqi3bcunUrpk+fjqysrLoNjiA2NhY9evRAZmYmVFVVK01Dv8X6U92//TXth1KLNBNycv9NJCGph+ssqi00oCj9HADwukgLRYVFdX4MQgghhBBCCCG1Q532ZkKm3B2E+hocIRRlAwBKIMSdK9fr5RiEEEIIIaRy/fr14722q/yn7F3mTdHZs2ehr68PsVhcaewfg/Kv9Xv7s2PHjsYOj3zkaCK6ZkJOWL932gFAqArgden3RzfuoPVn9vVyHEIIIYQQUtGWLVuQl5dX6TZ1dfUGjqbmOnbsiDNnzkBJSem9h1b7+PjAx8enbgOrQ+Vf6/c2bW3tBo6mdrp3715vN/1Iw6BOezMhI/vfu1Tr60enYqCBJ/8/cfyrx1n1cgxCCCGEEFK5t9+z3VzIy8vD1NT0o34euqFe60dIZT7OX9VHqPzw+Pq6027Y7r/XmBRk06lBCCGEEEIIIY2NembNhJSUFMBK329aX4NbWtlaQVbwBgCQX6BWT0chhBBCCCGEEFJT1GlvTv7/Dnt9PZIiJS0NBdl0AECeRAPpj5/Wz4EIIYQQQgghhNQIddqbEcH/d9brcxoJOaV87vudi1fr8UiEEEIIIYQQQt6FOu3NCiv3v/VDQUuB+/7iXlo9HokQQgghhBBCyLtQp70ZEfz//9dnp13LwpD7npdRWI9HIoQQQsinzNjYGKtWrWrsMBqUQCDAwYMHGzuMj9L9+/chEAiQkJDQ2KF8lOjcbVzUaW9GGqLTbtm5HYDSCe8K3yhUm5YQQgghHz+BQFDtJygo6L3yvXTpEsaPH/9BsXXv3h3Tp0//oDyam+vXr6Nr164QiUQwMDDA0qVL37lPamoq+vfvDwUFBWhpacHf3x/FxcXc9idPnuDLL7+EhYUFpKSkPrk6rQlfX1/Y29tDKBSiXbt2laapSdvs2bMHVlZWEIlEsLW1RVRUVD1HTj4G1GlvRgSVfKtrKhrqUJTOAAC8LtJGUWFRvR2LEEIIIU3fkydPuE94eDjEYjFvnZ+fH5eWMcbrDFZHU1MTCgp0g6C8oqLq/+7KyclBnz59YGRkhPj4eCxbtgxBQUHYtGlTlfuUlJSgf//+KCwsxLlz57Bt2zZs3boVAQEBXJqCggJoampi/vz5sLOzq7PyfGzGjBmDYcOGVbqtJm1z7tw5jBgxAmPHjsXVq1cxaNAgDBo0CP/8809DFYE0U9Rpb0a4O+3112cHAAhF2QCAEsjh3/jr9XswQgghhDRpOjo63EdFRQUCgYBbvn37NpSVlXH06FHuLuRff/2F5ORkDBw4ENra2lBSUoKDgwNOnDjBy/ft4fECgQBbtmzB4MGDoaCgAHNzc0RGRn5Q7Pv27YONjQ2EQiGMjY2xfPly3vZ169bB3NwcIpEI2tra8PT05Lbt3bsXtra2kJeXh4aGBlxcXPD69esPiqe8suHcu3btgrOzM0QiEXbs2FHtPjt27EBhYSF++eUX2NjYYPjw4fD19cWKFSuq3Cc6Ohq3bt3Cb7/9hnbt2qFfv34IDQ3F2rVrUVhY+ihkWVuMGjUKKioq71WeX3/9FTY2NhCJRLCyssK6desqlPWPP/5Aly5dIBKJ0KZNG5w+fZqXx+nTp9GpUycIhULo6upizpw5vItAEokES5cuhZmZGYRCIQwNDbFw4UJeHvfu3UOPHj2goKAAOzs7nD9/ntv24MEDuLu7Q01NDYqKirCxsanxne6ffvoJkydPhqmpaaXba9I2q1atQt++feHv7w9ra2uEhoaiQ4cOWLNmTbXH/t///ocOHTpAJBLB1NQUwcHBvHoRCARYv349+vXrB3l5eZiammLv3r28PG7cuIGePXty5/M333yD3NxcXprffvsNtra2XP1PmTKFt/358+dV/j4zMzMxcuRIaGpqQl5eHubm5oiIiKi+UkmNUae9GREI3v5SP4Rq/+X/+J879XosQgghhDR/c+bMwZIlS5CYmIi2bdsiNzcXbm5uiImJwdWrV9G3b1+4u7sjNTW12nyCg4Ph5eWF69evw83NDSNHjsTLly/fK6b4+Hh4eXlh+PDhuHHjBoKCgrBgwQJs3boVAHD58mX4+voiJCQESUlJOHbsGLp16wagdHTBiBEjMGbMGCQmJiI2NhZDhgwBq4f37s6ZMwfTpk1DYmIiXF1dq017/vx5dOvWDXJyctw6V1dXJCUlITMzs8p9bG1toa2tzdsnJycHN2/erJMy7NixA4sXL0ZoaCgSExOxaNEiLFiwANu2beOl8/f3x8yZM3H16lU4OjrC3d0dL168AAA8fvwYbm5ucHBwwLVr17B+/Xr8/PPP+OGHH7j9586diyVLlmDBggW4desWdu7cySsXAMybNw9+fn5ISEiAhYUFRowYwXVwJ0+ejIKCApw5cwY3btzAjz/+CCUlpTqpg5q0zfnz5+Hi4sLbz9XVlXdh4W1nz57FqFGjMG3aNNy6dQsbN27E1q1bK1ysWLBgATw8PHDt2jWMHDkSw4cPR2JiIgDg9evXcHV1hZqaGi5duoQ9e/bgxIkTvE75+vXr4e/vj/Hjx+PGjRuIjIyEmZkZ7xjV/T7L2uTo0aNITEzE+vXr0aJFi/eoSVIZmcYOgNQc15UWSEEikUBKqn6uuagYtMCTR6Xfcx5n1csxCCGEEFLqt7nT8Tqr8g5XfVJUVcNXi8PrJK+QkBD07t2bW1ZXV+cNsw4NDcWBAwcQGRlZ4e5deT4+PhgxYgQAYNGiRfjpp5/w999/o2/fvrWOacWKFejVqxcWLFgAALCwsMCtW7ewbNky+Pj4IDU1FYqKihgwYACUlZVhZGSE9u3bAyjttBcXF2PIkCEwMjICANja2tY6hpqYPn06hgwZUqO0T58+hYmJCW9dWaf16dOnUFNTq3Sftzu25fepC8HBwQgNDcWQIUMgJSUFExMTroPp7e3NpZsyZQo8PDwAlHYSjx07hp9//hmzZs3CunXrYGBggDVr1kAgEMDKygppaWmYPXs2AgIC8Pr1a6xatQpr1qzh8mzVqhU+//xzXix+fn7o378/F5eNjQ3u3r0LKysrpKamwsPDg2vLqu6av4+atE1VbVFdOwQHB2POnDlcmU1NTREaGopZs2YhMDCQSzd06FCMGzcOQOnv7fjx41i9ejXWrVuHnTt3Ij8/H7/++isUFRUBAGvWrIG7uzt+/PFHaGtrY9GiRZg8eTJ8fX25PoaDgwMvlup+n6mpqWjfvj06duwIoHT0Bqk71GlvRqTK3WEvLiqCnFBYL8cxbm+F2+efAQAKs2kwBiGEEFKfXmdlIvfli8YO44OU/aFeJjc3F0FBQThy5AjXAc7Ly3vnnfa2bdty3xUVFSEWi5Genv5eMSUmJmLgwIG8dU5OTggPD0dJSQl69+4NIyMjmJqaom/fvujbty839NfOzg69evWCra0tXF1d0adPH3h6elbaKQYAGxsbPHjwAADQtWtXHD16tMZxvl13zc3r16+RnJwMX19f3gR2xcXFFYbaOzo6ct9lZGTQsWNH7m5wYmIiHB0dISj3966TkxNyc3Px6NEjPH36FAUFBejVq1e18ZQ/h3R1dQEA6enpsLKygq+vL7799ltER0fDxcUFHh4evPRN0bVr1xAXF8e7s15SUoL8/Hy8efOGmxeifN2WLZfNpJ+YmAg7Ozuuww6U1q1EIkFSUhIEAgHS0tLg7OxcbSzV/T6//fZbeHh44MqVK+jTpw8GDRqELl26fFDZyX+o096MlB8VX1xUWG+ddhMbK8gK7qGIKSK/oPL/OBFCCCGkbiiqNs5/a+vyuOU7A0Dp3c7jx48jLCwMZmZmkJeXh6enJ/cMdVVkZWV5ywKBABKJpM7iLE9ZWRlXrlxBbGwsoqOjERAQgKCgIFy6dAmqqqo4fvw4zp07h+joaKxevRrz5s3DxYsXK9xNBYCoqChuEjl5eflaxfF23VVHR0cHz549460rW9bR0alyn7///rtW+9RG2XPR4eHh6N69O28kqLS09AfnX6am9Vr+HCq7AFB2Do0bNw6urq44cuQIoqOjsXjxYixfvhxTp0794Phq0jZVpamuHXJzcxEcHFzpaAyRSPShYQN4v7oF+L/Pfv364cGDB4iKisLx48fRq1cvTJ48GWFhYXUS46eOOu3NiJRAinvfW2FBARSUlOvnONLSUJDLQHaBIvIkGkh/+BhaBi3r5ViEEELIp66uhqg3JXFxcfDx8cHgwYMBlHY87t+/36AxWFtbIy4urkJcFhYWXGdSRkYGLi4ucHFxQWBgIFRVVXHy5EkMGTIEAoEATk5OcHJyQkBAAIyMjHDgwAHMmDGjwrHKhtDXN0dHR8ybNw9FRUVcB+r48eOwtLSschSAo6MjFi5ciPT0dGhpaXH7iMVitG7d+oNj0tbWhp6eHh48eAAzM7NqH9+8cOECN29AcXEx4uPjucclrK2tsW/fPjDGuM52XFwclJWVoa+vDy0tLcjLyyMmJoYbBv4+DAwMMHHiREycOBFz587F5s2b66TTXpO2cXR0RExMDG9EwvHjxyvcJS+vQ4cOSEpKqvB8+dsuXLiAUaNG8ZbLHvewtrbG1q1b8fr1a+4iUVxcHKSkpGBpaQllZWUYGxvj9OnT3KMF70NTUxPe3t7w9vZG165d4e/vT532OkKd9mZEIBBwnfbid7wS5EMJFfOBgtLvdy5dp047IYQQQmrM3Nwc+/fvh7u7OwQCARYsWFBvd8wzMjK4YcBldHV1MXPmTDg4OCA0NBTDhg3D+f9j777Do6j2/4G/Z/umN9IgJAFCld5BAekgCogi5SooX8sPUBG9IldFigVREClX9KpYriiKgMhFpAgigvQAUkILnSSUJJu6dX5/LJndhTQgyexs3q/n4fHs7uzOZ+ZskHfOmTPbt2PBggXSquarV6/GqVOn0KVLF4SGhmLNmjVwOBxo0KABduzYgY0bN6J3796IjIzEjh07cPnyZTRq1KhSjqG8RowYgWnTpmHMmDGYNGkS/v77b3z44Yf44IMPpG1WrFiByZMn4+jRowCA3r17o3Hjxnj00Ucxa9YspKWl4bXXXsO4ceOgd5u1WXQOc3NzpXOq0+nKFezfeOMNTJgwAZGRkejXrx/MZjN2796NzMxMj19yLFy4EElJSWjUqBE++OADZGZm4oknngAAjB07FnPnzsWzzz6L8ePHIyUlBW+88QYmTpwIlUoFg8GASZMm4eWXX4ZOp0Pnzp1x+fJlHDp0CGPGjCnX+ZswYQL69euH+vXrIzMzE5s2bSp3n544cQK5ublIS0tDQUGBdL4aN24MnU5Xrr55/vnn0bVrV8yePRv33XcfvvvuO+zevbvUW/ZNmTIFAwYMQO3atfHQQw9BpVJh//79+Pvvvz0W6fvhhx/Qpk0b3H333fjmm2+wc+dOfPbZZwCAkSNH4o033sCoUaMwdepUXL58Gc8++yweffRR6Rr7KVOmYOzYsYiLi0P//v2Rk5ODP//8s9y/0JgyZQpat26NJk2awGw2Y/Xq1bL/vPgUkcTs7GwRgJidnS13KaWaM32q+MYbb4hvvPGGeOFMaqXua/XcxeKCpzeKC57eKP4069NK3Vd1YrFYxJUrV4oWi0XuUugOsB+Vj33oG5TWjwUFBeLhw4fFgoICuUu5bYsXLxaDg4Olx5s2bRIBiJmZmR7bpaamivfee69oNBrFuLg4ccGCBWLXrl3F559/XtomPj5enDNnjpiZmSna7XYRgLhixQqPzwkODhYXL15cYj1du3YV4RzS8PgzY8YMURRFcdmyZWLjxo1FrVYr1q5dW3zvvfek9/7xxx9i165dxdDQUNFoNIrNmjUTly5dKoqiKB4+fFjs06ePWKNGDVGv14v169cX58+ff1vnzJ37MaampooAxH379t3SZ+zfv1+8++67Rb1eL9asWVOcOXOmx+uLFy8Wb/wn/unTp8V+/fqJRqNRjIiIEF988UXRarXeVNuNf+Lj48tVk91uFz/55BOxRYsWok6nE0NDQ8UuXbqIy5cv9zjWJUuWiO3atRN1Op3YuHFj8bfffvP4nM2bN4tt27YVdTqdGB0dLU6aNMmjTrvdLr755ptifHy81Kdvv/22xz7cz2dmZqYIQNy0aZMoiqI4fvx4sW7duqJerxdr1KghPvroo+KVK1fKdYwlfddSU1OlbcrqG1EUxe+//16sX7++qNPpxCZNmoj/+9//ytz32rVrxU6dOolGo1EMCgoS27VrJ37yySfS6wDEhQsXir169RL1er2YkJAgfZeLHDhwQLz33ntFg8EghoWFiU8++aSYk5MjvW6328U5c+aIDRo0ELVarRgTEyM+++yzHvso7edzxowZYqNGjUSj0SiGhYWJAwcOFE+dOlXmsVUHpf3dX94cKohiJdy7QmFMJhOCg4ORnZ2NoKAgucsp0YdvTUem1flb6if+MRK16yVV2r52rd2EnSudX42I4BQ88u7/q7R9VSdWqxVr1qxB//79b7ouiJSD/ah87EPfoLR+LCwsRGpqKhITEyvsWlSlczgcMJlMCAoKqrS74lDlK6sfT58+jcTEROzbtw8tWrSo+gJ9nCAIWLFiBQYNGnTbn8GfxcpT2t/95c2h7BEFUQmu7rKWsZDLnWrYviUE2AEA5vyKuX8lERERERER3RqGdgVRqdxu+War3GvaA0ND4K9x3sIh1xqFXFNOpe6PiIiIqLrr168fAgICiv3z9ttvy1ZXSTUFBATgjz/+kK2uivLMM8+UeHzPPPOM3OURcSE6JXGfqmK1VG5oBwC9nwm5phiI0ODItj1o27dbpe+TiIiIqLr69NNPUVBQUOxrYWFhVVyNy40L/bmrWbPsxYoTEhLgzVfkTp8+HS+99FKxr3nzpbNFvPncUsVgaFcQ99Bur+SRdgDwq6HDVZOznXYkFWBoJyIiIqo05QnAcijrdmOVdWeAqhIZGSndDo/IG3F6vIKoVWqpba3kW74BQGSDOKmdn15Y6fsjIiIiIiIiTwztCuIx0m61Vfr+GnVu41qMLs+/0vdHREREREREnhjaFUStdnWXzVq5q8cDQHB4mLQYXZ41Cvm5uZW+TyIiIiIiInJhaFcQldo1Pd5mr/yRdsC5GB0AOKDF4W17qmSfRERERERE5MTQriBq99BeBdPjAcBYQye10w6nVsk+iYiIiIiIyImhXUHcF6KzV9FIu/tidHlpxd+ChIiIiOhWJSQk4MMPP5S7jColCAJWrlwpdxmKlJCQgLlz58pdhk/q1q0bXnjhBbnLoFIwtCuIWlP1I+2NOrUG4LyNhyXPr0r2SURERN5DEIRS/0ydOvW2PnfXrl148skn76i2bt26YcKECXf0GUpz4MAB3HPPPTAYDIiLi8OsWbPKfM9zzz2H1q1bQ6/Xo0WLFpVfpMJ88skn6NatG4KCgiAIArKysm7a5tq1axg5ciSCgoIQEhKCMWPGIPeG9Z5up2+IykPW0L5lyxbcf//9iI2Nvek3j1arFZMmTULTpk3h7++P2NhYPPbYY7h48aLHZ5TnB8hXuE+Pr6qR9pCIcARcX4wu1xqNgrz8KtkvEREReYdLly5Jf+bOnYugoCCP51566SVpW1EUYbOV798oNWrUgJ8fBwTclXVLX5PJhN69eyM+Ph579uzBe++9h6lTp+KTTz4p87OfeOIJPPLIIxVVqk/Jz89H37598a9//avEbUaOHIlDhw5h/fr1WL16NbZs2YKnnnpKev1O+oaoLLKG9ry8PDRv3hwLFy686bX8/Hzs3bsXr7/+Ovbu3Yvly5cjJSUFDzzwgMd2Zf0A+RK1RiO17TZ7le1Xb8wG4FyM7sj23VW2XyIiIpJfdHS09Cc4OBiCIEiPjx49isDAQPzyyy/SSO7WrVtx8uRJDBw4EFFRUQgICEDbtm2xYcMGj8+9cXq8IAj49NNPMXjwYPj5+SEpKQmrVq26o9p//PFHNGnSBHq9HgkJCZg9e7bH6//+97+RlJQEg8GAqKgoPPTQQ9Jry5YtQ9OmTWE0GhEeHo6ePXsiLy/vjupxd/r0aQiCgKVLl6Jr164wGAz45ptvSn3PN998A4vFgs8//xxNmjTBsGHD8Nxzz2HOnDmlvm/evHkYN24c6tSpc1u1bt26Fffccw+MRiPi4uLw3HPPeZyLOnXq4L333sOIESPg7++PmjVr3vTv+7Nnz2LgwIEICAhAUFAQhg4divT0dI9tfv75Z7Rt2xYGgwEREREYPHiwx+v5+fl44oknEBgYiNq1a3sEYovFgvHjxyMmJgYGgwHx8fF45513ynV8EyZMwCuvvIIOHToU+/qRI0ewdu1afPrpp2jfvj3uvvtuzJ8/H9999500oHi7ffP333+jX79+CAgIQFRUFB599FFcuXJFer1bt24YP348xo8fj+DgYEREROD111+HKIrSNpmZmXjssccQGhoKPz8/9OvXD8ePH/fYz59//olu3brBz88PoaGh6NOnDzIzM6XXHQ4HpkyZgoiICERHR3vMoBFFEVOnTkXt2rWh1+sRGxuL5557rlznliqGrKG9X79+ePPNN2/6gQSA4OBgrF+/HkOHDkWDBg3QoUMHLFiwAHv27MHZs2cBlO8HyJd4LERnr7rQbqyhldqXDp2qsv0SERGRMrzyyiuYOXMmjhw5gmbNmiE3Nxf9+/fHxo0bsW/fPvTt2xf333+/9G+4kkybNg1Dhw7FgQMH0L9/f4wcORLXrl27rZr27NmDoUOHYtiwYTh48CCmTp2K119/HV988QUAYPfu3Xjuuecwffp0pKSkYO3atejSpQsA5+yC4cOH44knnsCRI0ewefNmPPjggx5BqaK88soreP7553HkyBH06dOn1G23b9+OLl26QKdzLRTcp08fpKSkeASwinTy5En07dsXQ4YMwYEDB7B06VJs3boV48eP99hu/vz5aN68Ofbt2ycd0/r16wE4A+HAgQNx7do1/P7771i/fj1OnTrlMfL/v//9D4MHD0b//v2xb98+bNy4Ee3atfPYx+zZs9GmTRvs27cPY8eOxf/7f/8PKSkpAJy/mFi1ahW+//57pKSk4JtvvkFCQkKFnIPt27cjJCQEbdq0kZ7r2bMnVCoVduzYIW1zq32TlZWF7t27o2XLlti9ezfWrl2L9PR0DB061GO7L7/8EhqNBjt37sSHH36IOXPm4NNPP5VeHz16NHbv3o1Vq1Zh+/btEEUR/fv3l2ZuJCcno0ePHmjcuDG2b9+OrVu34v7774fdLU989dVX8Pf3x/bt2zFr1ixMnz5d6r8ff/wRH3zwAT7++GMcP34cK1euRNOmTe/wrNKt0JS9iffIzs6GIAgICQkBUPYPUHG/DAAAs9kMs9ksPTaZnLc1s1qtZU5LkpNK5XafdlvV1RperybOX8/quWn5Xn2OvF3RueM5VDb2o/KxD32D0vrRarVCFEU4HA44HA7p+csLk2HPqfpjUAdqUWNci1t6T1HdN/536tSp6NGjh7RdSEiIxz/qp02bhhUrVuCnn37CuHHjpOeLQnDRf0eNGiUFuTfffBPz5s3DX3/9hb59+5ZYU9E5vdHs2bPRvXt3vPrqqwCAevXq4dChQ3jvvffw2GOP4fTp0/D390f//v0RGBiIuLg4NG/eHA6HAxcuXIDNZsOgQYNQu3ZtAECTJk08jvl2FfV/0ec8//zzGDRokMfrJbl06RISExM9tqlRowYA4OLFiwgODi5130Xn+VaO4e2338aIESOkkdW6deti7ty5uPfee7Fw4UIYDAYAQLt27fDyyy9DEATUq1cPW7duxZw5c9CjRw+sX78eBw8exMmTJxEX51zk+IsvvkDTpk2xY8cOtG3bFm+99RYeeeQRvPHGG9K+mzZt6lFrv3798MwzzwAA/vnPf+KDDz7Axo0bkZSUhDNnziApKQmdOnWCIAjSfm7lWN2/1+7vu3TpEiIjIz2eU6lUCAsLw8WLF+FwOG6rb+bPn48WLVrgzTfflJ779NNPER8fj6NHj6J+/foAgLi4OMyePRuCICApKQkHDhzABx98gDFjxuD48eNYtWoV/vjjD3Tq1AkA8PXXXyM+Ph7Lly/Hww8/jHfffRdt2rTBggULpP00atTI45ibNm2KSZMmITAwEElJSViwYAE2bNiAHj164MyZM4iOjkb37t2h1WpRq1YttGnT5o5/FqoLh8MBURRhtVo9BmGB8v//SzGhvbCwEJMmTcLw4cMRFBQEAEhLS0NkZKTHdhqNBmFhYUhLSyvxs9555x1MmzbtpufXrVvn1ddWpZ0952qnpWHNmjVVsl+ruRBAOAAVzDl+VbZfX1b0m0tSNvaj8rEPfYNS+lGj0SA6Ohq5ubmwWCzS8zaTGWJO1axV404UHdLARXkVFhZCFEXpffn5zrVuGjRo4PFZubm5ePfdd7Fu3TqkpaXBbrejoKAAx48fl7ZzOBzSIEpOTg4AZ7B2/5zAwECcPXu2xDptNhssFkuxrx86dAj9+/f3eK1ly5b48MMPkZmZifbt26NWrVqoW7cuevTogR49emDAgAHw8/NDYmIiunbtiubNm6N79+649957MXDgQGng6EYdO3bEuXPOf6d16NABy5YtK/EcFhQUwGQySWswNWzYsNz9YLfbbzreos/Jzc0t83PMZjPsdvst9fu+fftw6NAhLFmyRHqu6BclBw8eRIMGDeBwONCuXTupHwHnuf7oo49gMpmQnJyMmjVrIjg4WNp3rVq1EBwcjH379qFBgwZITk7GyJEjS6zN4XCgfv36Hq/XqFED58+fh8lkwkMPPYTBgwejQYMG6NGjB/r06YPu3buX+zgB1/c5JyfHY7CssLAQDsfNPy+iKKKwsBAmk+m2+mbPnj3YvHmzlG3cHTx4ENHR0bDZbGjVqpXHuW3evDnmzJmDzMxM7NmzBxqNBo0aNZL2odVqUa9ePezfvx99+vTBvn37MHDgwFJ/jho2bCgdOwBERETgwoULMJlM6NOnDz744APUqVMHPXv2RK9evdC3b19oNIqJkrKyWCwoKCjAli1bblrzo+g7VxZFnGmr1YqhQ4dCFEV89NFHd/x5kydPxsSJE6XHJpMJcXFx6N27d7E/NN5iyy8iLu1NBgCEh4Whf//+VbbvJVt/QK49Gnm2GAzq2gFGf+/95YY3s1qtWL9+PXr16gWtVlv2G8grsR+Vj33oG5TWj4WFhTh37hwCAgKk0UkAMAfpYReq/opFdaD2lv/dYzAYIAiC9L6iwY7o6GiPz5o0aRI2bNiAWbNmoV69ejAajRg6dKjHe1UqFfR6PQBnOAeAoKAgj89RqVTQ6XQl1qnRaEp8Xa1WQ6/Xe7xmNBql/YSGhmLfvn3YvHkz1q9fj3fffRfvvfceduzYgdDQUGzcuBHbtm3D+vXr8dlnn+Gtt97C9u3bkZiYeNO+1qxZI42YGY3GUs9r0esBAQEAgMjIyHL3Q82aNZGZmemxfdG15fXq1Svzc/R6PdRq9S31e0FBAZ566ik8++yzN71Wu3Zt6HQ6KeAGBgZCEAQAzu+KSqVCUFCQR9udIAgwGAwICgqC0WiU2sVRqVQIDAz0eF2j0UCrdX6P77nnHpw6dQq//PILNm7ciCeeeAI9evTADz/8UO5jLfo+37if+Ph4XLlyxeM5m82GzMxMJCQkICgo6Lb6prCwEAMGDMDMmTNvei0mJgb+/v4ex1jE/XtcVHNQUJDHKK7799/f3/+mnwV3Go0G/v7+0rELggCtVit9Vxo3boyUlBRs2LABGzZswD//+U/8+9//xqZNmxTxd6/cCgsLYTQa0aVLF4+/+wGU+xdoXh/aiwL7mTNn8Ntvv3l82aKjo5GRkeGxvc1mw7Vr1xAdHV3iZ+r1eul/Eu60Wq1Xf/Hcr5FxOMQqrVXvl43cnGg4oMWJ3QfQquc9VbZvX+Tt3zUqH/aj8rEPfYNS+tFut0MQBKhUKo9RvKhnW8lY1a0pqru4/7of07Zt2zB69GgMGTIEgHOk8fTp0+jWrZvHdkUBr+i/N35OSc+5KzqnN2rUqBG2bdvm8dr27dtRv3596fui0+nQu3dv9O7dG1OnTkVISIh0/ToA3HPPPbjnnnvwxhtvID4+Hj/99JPHwE+R4oJ8SYqOp6RzV5pOnTrh1Vdfhd1ul45h48aNaNCgAcLDw8t8v/t5Lq9WrVrhyJEj0lTtkuzatcujL3bs2IFGjRpBpVKhcePGOHfuHC5cuCBNWz98+DCysrJw1113QaVSoVmzZti0aRPGjBlTav031u7+XEhICIYPH47hw4fj4YcfRt++fZGVlYWwsLByHWtJfdK5c2dkZWVh3759aN26NQBg8+bNcDgc6NixI1Qq1W31TevWrfHjjz+iTp06pY5a79y506OenTt3IikpCVqtFk2aNIHNZsOuXbuk6fFXr15FSkoKmjRpIp3b3377DdOnTy/zHBSdz6JbOhbt19/fHwMHDsTAgQMxfvx4NGzYEIcOHUKrVsr5+0suReezuP9Xlff/XV59n/aiwH78+HFs2LDhpi98x44dkZWVhT179kjP/fbbb3A4HGjfvn1Vl1vpNG6dandU3UJ0ABejIyIiovJLSkrC8uXLkZycjP3792PEiBGVdv3r5cuXkZyc7PEnPT0dL774IjZu3IgZM2bg2LFj+PLLL7FgwQLpFnWrV6/GvHnzkJycjDNnzuCrr76Cw+FAgwYNsGPHDrz99tvYvXs3zp49i+XLl+Py5cvSdcByGTFiBHQ6HcaMGYNDhw5h6dKl+PDDDz1+kbBixQppqnOREydOIDk5GWlpaSgoKJDOk/tlGiWZNGkStm3bhvHjxyM5ORnHjx/HTz/9dNNCdDt27MB7772HY8eOYeHChfjhhx/w/PPPA3CuOdW0aVOMHDkSe/fuxc6dO/HYY4+ha9eu0tpUb7zxBr799lu88cYbOHLkCA4ePIh333233Odmzpw5+Pbbb3H06FEcO3YMP/zwA6Kjo0u8pMFdWloakpOTceLECQDOqenJycnSIoiNGjVC37598eSTT2Lnzp34888/MX78eAwbNgyxsbEAytc3Nxo3bhyuXbuG4cOHY9euXTh58iR+/fVXPP744x6LxJ09exYTJ05ESkoKvv32W8yfP186t0lJSRg4cCCefPJJbN26Ffv378c//vEP1KxZEwMHDgTgnGW8a9cujB07FgcOHMDRo0fx0UcfeaxSX5ovvvgCn332Gf7++2+cOnUK//3vf2E0GhEfH1+u99Odk3WkPTc3V/rhAIDU1FQkJycjLCwMMTExeOihh7B3716sXr0adrtduk49LCwMOp3O4wdo0aJFsFqtN/0A+RKNxhWcHVUc2msk1ZIWo8tL473aiYiIqGRz5szBE088gU6dOiEiIgKTJk265evny2vJkiUe11sDwIwZM/Daa6/h+++/x5QpUzBjxgzExMRg+vTpGD16NADnqOzy5csxdepUFBYWIikpCd9++y2aNGmCI0eOYMuWLZg7dy5MJhPi4+Mxe/Zs9OvXr1KOobyCg4Oxbt06jBs3Dq1bt0ZERASmTJnicbvj7OxsaUX1Iv/3f/+H33//XXrcsmVLAM5/e5e1wnqzZs3w+++/49VXX8U999wDURRRt27dm+75Pn78eOzevRvTp09HUFAQ5syZI62GLwgCfvrpJzz77LPo0qULVCoV+vbti/nz50vv79atG3744QfMmDEDM2fORFBQkLSaf3kEBgZi1qxZOH78ONRqNdq2bYs1a9aUa1bBokWLPNa7Ktrv4sWLpe/LN998g/Hjx6NHjx5QqVQYMmQI5s2bJ72nPH1zo9jYWPz555+YNGkSevfuDbPZjPj4ePTt29ej7sceewwFBQVo164d1Go1nn/+eY/PXbx4MZ5//nkMGDAAFosFXbp0wZo1a6RR3Pr162PdunX417/+hXbt2sFoNKJ9+/YYPnx4uc5tSEgIZs6ciYkTJ8Jut6Np06b4+eefyzW7gyqGIFbGvSvKafPmzbj33ntven7UqFGYOnVqiVONNm3ahG7dugEArl27hvHjx+Pnn3/2+AEquk6oPEwmE4KDg5Gdne3V17Tv2/YnflrnXGwnJsAPT7/0cpXtOzPjCpZMSQagQpD2HB6dP6rK9u1LrFYr1qxZg/79+ytiKicVj/2ofOxD36C0fiwsLERqaioSExNvuq6xuipa3CsoKOiWpmyTd0lISMDTTz+NSZMmsR8rWLdu3dCiRQvMnTu3UvfDn8XKU9rf/eXNobKOtHfr1q3U+12W5/cJYWFhN/121VdptK7uqupbLIRGRsBfk4E8WzTyrFEwFxRAf30RDCIiIiIiIqoc/DWKgriPIshxX0SD0TmtzQ4dDm/fU8bWRERERHQr+vXrh4CAgGL/vP322z6zz6r0zTfflHh8TZo0kbs8onLx+tXjyUWtdV89vupDuzFCA1y/ReTFv0+iZfe7q7wGIiIiIl/16aefoqCgoNjXyrsCuhz7PHXqVKWtWXCnHnjggRIXqFbCZTWbN2+WuwTyAgztCuIxPV6GpQgikmrifKqznXcpr8r3T0REROTLatasWS32WZUCAwMRGBgodxlEd4TT4xVEJ/NIe8PObaS2OdevyvdPRERERERU3TC0K4hG5xbaZRhpD4+qAX91OgAgzxoNcwlTqYiIiIiIiKhiMLQriEYrb2gHAIMxG8D1xej+2idLDURERERERNUFQ7uCaHWuxTLKczu8ymCIUEvtS4dOylIDERERERFRdcHQriBavV5qyzXSHpoQJbXz03NlqYGIiIiIiKi6YGhXEI1GC1wP63KNtCe2bCy1Lbm8+QAREREREVFlYmhXmqLQLtPua9VLhE5w3u6t0BwqUxVERESkdAkJCfjwww/lLkNxpk6dihYtWshdBsmM34PK88UXXyAkJETuMjwwtCtO0Ui7PHtXqdUwai8DAAocYbiSliFPIURERFQlBEEo9c/UqVNv63N37dqFJ5988o5q69atGyZMmHBHn6E0L730EjZu3Ch3GeTDDh06hCFDhiAhIQGCIGDu3LnFbrdw4UIkJCTAYDCgffv22Llzp8frhYWFGDduHMLDwxEQEIAhQ4YgPT29Co7A9zC0K4wg80g7AOj8zVL75O4DMlZCREREle3SpUvSn7lz5yIoKMjjuZdeeknaVhRF2Gy2cn1ujRo14OfnV1ll+6yAgACEh4fLXUaVsFqtcpdQLeXn56NOnTqYOXMmoqOji91m6dKlmDhxIt544w3s3bsXzZs3R58+fZCR4RrQe+GFF/Dzzz/jhx9+wO+//46LFy/iwQcfrKrD8CkM7UrjBaHdEO669dyVk+dlrISIiIgqW3R0tPQnODgYgiBIj48ePYrAwED88ssvaN26NfR6PbZu3YqTJ09i4MCBiIqKQkBAANq2bYsNGzZ4fO6N0+MFQcCnn36KwYMHw8/PD0lJSVi1atUd1f7jjz+iSZMm0Ov1SEhIwOzZsz1e//e//42kpCQYDAZERUXhoYcekl5btmwZmjZtCqPRiPDwcPTs2RN5eXm3Xcvp06chCAKSk5Ol57KysiAIAjZv3gwA2Lx5MwRBwMaNG9GmTRv4+fmhU6dOSElJkd5z47Rou92OiRMnIiQkBOHh4Xj55ZcxatQoDBo0SNomISHhptHSFi1aeMySyMrKwv/93/+hRo0aCAoKQvfu3bF///5yHdu0adNwzz334OOPP0ZcXBz8/PwwdOhQZGdnS9vs2rULvXr1QkREBIKDg9G1a1fs3bvX43MEQcBHH32EBx54AP7+/njrrbdgt9sxZswYJCYmwmg0okGDBjddVjF69GgMGjQIb7/9NqKiohASEoLp06fDZrPhn//8J8LCwlCrVi0sXrxYeo/FYsH48eMRExMDg8GA+Ph4vPPOO+U63rLOVVEflXY+HA4Hpk+fjlq1akGv16NFixZYu3atx37Onz+P4cOHIywsDP7+/mjTpg127Njhsc3XX3+NhIQEBAcHY9iwYcjJyZFeu93vcNu2bfHee+9h2LBh0LsthO1uzpw5ePLJJ/H444+jcePGWLRoEfz8/PD5558DALKzs/HZZ59hzpw56N69O1q3bo3Fixdj27Zt+Ouvv0rct9lsxksvvYSaNWvC398f7du3l34+ANfU9ZUrV0o/u3369MG5c+c8Puejjz5C3bp1odPp0KBBA3z99dcer2dlZeHpp59GVFQUDAYD7rrrLqxevdpjm19//RWNGjVCQEAA+vbti0uXLkmvbd68Ge3atYO/vz9CQkLQuXNnnDlzpsxze7u4kphCyRnaQxOjce6Es51/uUDGSoiIiJTv448/Rm5u1d+RJSAgAE8//XSFfNYrr7yC999/H3Xq1EFoaCjOnTuH/v3746233oJer8dXX32F+++/HykpKahdu3aJnzNt2jTMmjUL7733HubPn4+RI0fizJkzCAsLu+Wa9uzZg6FDh2Lq1Kl45JFHsG3bNowdOxbh4eEYPXo0du/ejeeeew5ff/01OnXqhGvXruGPP/4A4JxdMHz4cMyaNQuDBw9GTk4O/vjjjypbCPjVV1/F7NmzUaNGDTzzzDN44okn8Oeffxa77ezZs/HFF1/g888/R6NGjTB79mysWLEC3bt3v6V9PvzwwzAajfjll18QHByMjz/+GD169MCxY8fKdf5TU1OxbNky/PzzzzCZTBgzZgzGjh2Lb775BgCQk5ODUaNGYf78+RBFEbNnz0b//v1x/PhxBAYGSp8zdepUzJw5E3PnzoVGo4HD4UCtWrXwww8/IDw8HNu2bcNTTz2FmJgYDB06VHrfb7/9hlq1amHLli34888/MWbMGGzbtg1dunTBjh07sHTpUjz99NPo1asXatWqhXnz5mHVqlX4/vvvUbt2bZw7d+6m4Hcn5+rEiRP4/vvvSzwfH374IWbPno2PP/4YLVu2xOeff44HHngAhw4dQlJSEnJzc9G1a1fUrFkTq1atQnR0NPbu3QuHwyHVcfLkSaxcuRKrV69GZmYmhg4dipkzZ+Ktt96q1O+wxWLBnj17MHnyZOk5lUqFnj17Yvv27QCcP39WqxU9e/aUtmnYsCFq166N7du3o0OHDsV+9vjx43H48GF89913iI2NxYoVK9C3b18cPHgQSUlJAJwzAd566y189dVX0Ol0GDt2LIYNGyb9jKxYsQLPP/885s6di549e2L16tV4/PHHUatWLdx7771wOBzo168fcnJy8N///hd169bF4cOHoVa7bm2dn5+P999/H19//TVUKhX+8Y9/4KWXXsI333wDm82GQYMG4cknn8S3334Li8WCnTt3QhCEOz63JWFoVxgBIkTIG9rrtGiCA+tPAQAsudoytiYiIqLS5ObmeoyOKdH06dPRq1cv6XFYWBiaN28uPZ4xYwZWrFiBVatWYfz48SV+zujRozF8+HAAwNtvv4158+Zh586d6Nu37y3XNGfOHPTo0QOvv/46AKB+/fo4fPgw3nvvPYwePRpnz56Fv78/BgwYgMDAQMTHx6Nly5YAnKHdZrPhwQcfRHx8PACgadOmt1zD7XrrrbfQtWtXAM5fiNx3330oLCyEwWC4adu5c+di8uTJ0rTjRYsW4ddff72l/W3duhU7d+5ERkaGNLL6/vvvY+XKlVi2bBmeeuqpMj+jsLAQX3zxBeLi4gAA8+fPx3333YfZs2cjOjr6pl8ifPLJJwgJCcHvv/+OAQMGSM+PGDECjz/+uMe206ZNk9qJiYnYvn07vv/+e4/QHhYWhnnz5kGlUqFBgwaYNWsW8vPz8a9//QsAMHnyZMycORNbt27FsGHDcPbsWSQlJeHuu++GIAhSP1fUuSosLMRXX32FmjVrFns+3n//fUyaNAnDhg0DALz77rvYtGkT5s6di4ULF2LJkiW4fPkydu3aJf0ioF69eh61OBwOfPHFF9IvPR599FFs3LhRCu2V9R2+cuUK7HY7oqKiPJ6PiorC0aNHAQBpaWnQ6XQ3LegWFRWFtLS0Yj/37NmzWLx4Mc6ePYvY2FgAzjUc1q5di8WLF+Ptt98G4LxsYsGCBWjfvj0A4Msvv0SjRo2wc+dOtGvXDu+//z5Gjx6NsWPHAgAmTpyIv/76C++//z7uvfdebNiwATt37sSRI0dQv359AECdOnU8arFarVi0aBHq1q0LwPnLhOnTpwMATCYTsrOzMWDAAOn1Ro0a3fqJvAWcHq80RWm9En+TU5aadROgV5kAAIWWW//NNxEREbkEBAQgMDCwyv8EBARU2DG0adPG43Fubi5eeuklNGrUCCEhIQgICMCRI0dw9uzZUj+nWbNmUtvf3x9BQUEe18jeiiNHjqBz584ez3Xu3BnHjx+H3W5Hr169EB8fjzp16uDRRx/FN998g/z8fABA8+bN0aNHDzRt2hQPP/ww/vOf/yAzM7PEfTVp0gQBAQEICAhAv379bqted+7nISYmBgCKPQ/Z2dm4dOmSFF4AQKPR3NQfZdm/fz9yc3OlBcOK/qSmpuLkyZPl+oxatWpJARUAOnbsCIfDIU3tT09Px5NPPomkpCQEBwcjKCgIubm5N30niqt94cKFaN26NWrUqIGAgAB88sknN72vSZMmUKlc0SYqKsojpKrVaoSHh0vncfTo0UhOTkaDBg3w3HPPYd26deU6zvKeq9q1a5d4PkwmEy5evFjs9/PIkSMAgOTkZLRs2bLUWQ4JCQkesxRiYmKk47vV77A3OHjwIOx2O+rXr+9xbn///XePc6vRaNC2bVvpccOGDRESEiKdu5J+9t3Pba1ataTAXhw/Pz8pkAOe5zYsLAyjR49Gnz59cP/99+PDDz/0mDpfGTjSrjBC0erxkC+0A4BRexVmcxAKHSFIP3seUbVryVoPERGRUlXUFHU5+fv7ezx+6aWXsH79erz//vuoV68ejEYjHnroIVgsllI/R6v1nMEnCILHdOCKFBgYiL1792Lz5s1Yt24dpkyZgqlTp2LXrl0ICQnB+vXrsW3bNqxbtw7z58/Hq6++ih07diAxMfGmz1qzZo20aJrRaCx2f0WB0n16ckkLrbmfh6Ipt3dyHlQq1U3Tot33nZubi5iYGI9rh4tU1K2vRo0ahatXr+LDDz9EfHw89Ho9OnbseNN34sbv0nfffYeXXnoJs2fPRseOHREYGIj33nvvpmu7i/vulPZ9atWqFVJTU/HLL79gw4YNGDp0KHr27Illy5aVehxVca6Akr9H7ko7PrVafUvf4VsREREBtVp900rw6enp0sJ10dHRsFgsyMrK8jgv7tvcKDc3F2q1Gnv27PGYqg6gQn/JeLvn1v1naPHixXjuueewdu1aLF26FK+99hrWr19f4rT/O8WRdoWRorqMI+3ADSvI7/lbxkqIiIjI2/z5558YPXo0Bg8ejKZNmyI6OhqnT5+u0hoaNWp003Xgf/75J+rXry8FAo1Gg549e2LWrFk4cOAATp8+jd9++w2A8x/pnTt3xrRp07Bv3z7odDqsWLGi2H3Fx8ejXr16qFevnsfoqrsaNWoAgMeInPuidLcjODgYMTExHgHWZrNhz549N+3bfb8mkwmpqanS41atWiEtLQ0ajUY6jqI/ERER5arl/PnzuHjxovT4r7/+kqaqA85z/9xzz6F///7S4oBXrlwp83P//PNPdOrUCWPHjkXLli1Rr169co/+lyUoKAiPPPII/vOf/2Dp0qX48ccfce3atVLfU95zdfbs2RLPR1BQEGJjY4v9fjZu3BiAc7ZFcnJymfWU5la+w7dCp9OhdevWHrcedDgc2LhxIzp27AgAaN26NbRarcc2KSkpOHv2rLTNjVq2bAm73Y6MjIybzq170LfZbNi9e7fH52ZlZUlT1Ev62Xc/t+fPn8exY8fu6Dy0bNkSkydPxrZt23DXXXdhyZIld/R5peFIu1IJAhx2O1Q3/BaqqhhqGIHrf4dcTb1Y+sZERERUrSQlJWH58uW4//77IQgCXn/99UobMb98+fJN4TcmJgYvvvgi2rZtixkzZuCRRx7B9u3bsWDBAvz73/8GAKxevRqnTp1Cly5dEBoaijVr1sDhcKBBgwbYsWMHNm7ciN69eyMyMhI7duzA5cuX7+i6VaPRiA4dOmDmzJlITExERkYGXnvttTs5dADA888/j5kzZyIpKQkNGzbEnDlzkJWV5bFN9+7d8cUXX+D+++9HSEgIpkyZ4jGS2bNnT3Ts2BGDBg3CrFmzUL9+fVy8eBH/+9//MHjw4HJNtzcYDBg9ejRmz54Nk8mE5557DkOHDpXCVlJSEr7++mu0adMGJpMJ//znP8s14pmUlISvvvoKv/76KxITE/H1119j165ddzxaPGfOHMTExKBly5ZQqVT44YcfEB0dXeZoeXnPlcFgwKhRo/D+++8Xez7++c9/4o033kDdunXRokULLF68GMnJydJCdcOHD8fbb7+NQYMG4Z133kFMTAz27duH2NjYEkOvuzv5DlssFhw+fFhqX7hwAcnJyQgICJCuq584cSJGjRqFNm3aoF27dpg7dy7y8vKk9QiCg4MxZswYTJw4EWFhYQgKCsKzzz6Ljh07ljgaXb9+fYwcORKPPfYYZs+ejZYtW+Ly5cvYuHEjmjVrhvvuuw+AcxT82Wefxbx586DRaDB+/Hh06NAB7dq1k87t0KFD0bJlS/Ts2RM///wzli9fLt3BomvXrujSpQuGDBmCOXPmoF69ejh69CgEQSjX+hmpqan45JNP8MADDyA2NhYpKSk4fvw4HnvssTLfe7sY2hXGfXzdYrHAUI6/7CpDRN2aOHv97iMFV8ylb0xERETVypw5c/DEE0+gU6dOiIiIwKRJk2AymSplX0uWLLlphGvGjBl47bXX8P3332PKlCmYMWMGYmJiMH36dIwePRqAcyrz8uXLMXXqVBQWFiIpKQnffvstmjRpgiNHjmDLli2YO3cuTCYT4uPjMXv27Du+Xv3zzz/HmDFj0Lp1a2mxtN69e9/RZ7744ou4dOkSRo0aBZVKhSeeeAKDBw/2uL3Y5MmTkZqaigEDBiA4OBgzZszwGGkXBAFr1qzBq6++iscffxyXL19GdHQ0unTpctNiYyVJTEzE4MGD0b9/f1y7dg0DBgyQfkECAJ999hmeeuoptGrVCnFxcXj77bfx0ksvlfm5Tz/9NPbt24dHHnkEgiBg+PDhGDt2LH755ZdbOEs3CwwMxKxZs3D8+HGo1Wq0bdsWa9as8bguvjjlPVf16tXDgw8+WOL5eO6555CdnY0XX3wRGRkZaNy4MVatWiWtkK7T6bBu3Tq8+OKL6N+/P2w2Gxo3boyFCxeW6/iCgoJu+zt88eJFaVFGwLnQ3vvvv4+uXbtKlwU88sgjuHz5MqZMmYK0tDTplnXu5+CDDz6ASqXCkCFDYDab0adPH49zUJzFixfjzTffxIsvvogLFy4gIiICHTp08Fis0M/PD5MmTcKIESNw4cIF3HPPPfjss8+k1wcNGoQPP/wQ77//Pp5//nkkJiZi8eLF6Natm7TNjz/+iJdeegnDhw9HXl4e6tWrh5kzZ5Z5bor2f/ToUXz55Ze4evUqYmJiMG7cuEq91EkQq+reFV7MZDIhODgY2dnZCAoKkrucElmtVsyaOgVWrXOlypcmvoCAoGBZasm4kIYfZjh/AxesP41/fPiELHUojdVqxZo1a9C/f/+brpUh5WA/Kh/70DcorR8LCwuRmpqKxMTEYlcBr44cDgdMJhOCgoLKDEtUfqNHj0ZWVhZWrlxZJft74403sHz5cuzfv5/9COdt61auXHnHlz9UJaX8LH7xxReYMGHCTbNJvFlpf/eXN4d6b49QsTxG2s3yjXBH1oyGQeVcgbLQEg6H3S5bLURERERERL6KoV1h3EO7rYwVWCubQee8qN0sBuLCqTOy1kJERERElcP9lnY3/im6BttXfPPNNyUea5MmTeQur0IUd2xBQUGoVasW/vjjD7nLo2LwmnalcUvtVplDuy7ABhQ626eTDyMuqY6s9RARERGRcwpxRXK/pd2NoqKi4O/vjxdeeKFC9ymXBx54wOO+9+7KexnO1KlTMXXq1AqsqmIVN23f4XAgNzdXWu3fW40ePVpal6I6YWhXGMEttdtsNhkrAfwi/YDrd+q4djq99I2JiIiISJHi4+NLfb2y7gwgh8DAQAQGBspdRqUqWgHeXdE17eVZ0Z+qHqfHK4z77dktFnlXbY+oV0tqm6/KO+pPRESkFFwDmIio+qiIv/MZ2hXG/Zp2u1XekfZ6rZtJbUs+V8ElIiIqTdHU2vz8fJkrISKiqlL0d/6d3OWE0+MVRnAbardZ5R3dDo+qAT/VVeQ7wpFvrQGH3Q6VWi1rTURERN5KrVYjJCQEGRkZAJz3+nX//3p15HA4YLFYUFhY6NW3maLSsR+Vj31Y8URRRH5+PjIyMhASEgL1HeQkhnaFcf+fe0kLglQlvT4T+QXhsIp+OH30OOo0aSh3SURERF4rOjoaAKTgXt2JooiCggIYjcZq/wsMJWM/Kh/7sPKEhIRIf/ffLoZ2hXH/IbLb5A/tukA7UOBsn92fwtBORERUCkEQEBMTg8jISK/45bvcrFYrtmzZgi5dutzR1FGSF/tR+diHlUOr1d7RCHsRhnaFcV893irzNe0A4B8VCFwfLMg6y1EDIiKi8lCr1RXyDzmlU6vVsNlsMBgMDAoKxn5UPvahd+MFCwojuPWYXeZbvgFAZP3aUtucKX89REREREREvoShXWEEt9TuDdPq6rdpBsB5b05Lvp+8xRAREREREfkYhnaF8bim3S7/yHZgaAj81VcAAPnWGrLfho6IiIiIiMiXMLQrjOct37wjIOv12QAAGww49fdRmashIiIiIiLyHQztCuM+Pd4bRtoBQBskSu1zB1NkrISIiIiIiMi3MLQrjMdIuxcsRAcAAdFBUjv7/FUZKyEiIiIiIvItDO0KI6hcod1ht8tYiUt0w0Spbc70jpqIiIiIiIh8AUO7wqhUbtPjvWSkvX6bphDgDOuWwgCZqyEiIiIiIvIdDO0K435Nu83ukLESF7+AAPhrLgNwriBvKTTLXBEREREREZFvYGhXGPeRdoeXLEQHAHqDCQBghw4pu5PlLYaIiIiIiMhHMLQrjOA+Pd5LRtoBQB/mquvCwRMyVkJEREREROQ7GNoVxiO0O7xn0beQhEipnXvRJGMlREREREREvoOhXWE8FqLzktXjAaBe22ZS25yjlbESIiIiIiIi38HQrjCe17R7z/T4uKQ6MKiyAAD55kivuR0dERERERGRkjG0K417aBe9J7QDgFF/BQBgEQOQeuiozNUQEREREREpH0O7wqjUaqntTQvRAYAuyDW6fnrvERkrISIiIiIi8g0M7QqjErx3pD0oLkxqZ529KmMlREREREREvoGhXWFUGtdIu8PhXaG9dvMGUtucJV8dREREREREvoKhXWncR9odooyF3CypxV3QCvkAgMLCsDK2JiIiIiIiorIwtCuM+zXt3jY9Xq3VwE+bAQAocIQj/ex5mSsiIiIiIiJSNoZ2hXEP7aLoXSPtAKAPLJTax3Yky1cIERERERGRD2BoVxiVyv2adu8L7X4xAVL76slLMlZCRERERESkfAztCuMx0i5jHSWJbVxHahdetZeyJREREREREZWFoV1hBI9r2r0vtjds3woqWAEA5oJAmashIiIiIiJSNoZ2hVGpVMD1sO6N17Qb/f3gf30xujxbJHIys+QtiIiIiIiISMEY2pXoelj3rrXjXfR+uQAAEWoc2b5X5mqIiIiIiIiUi6FdgQQUjbTLXEgJjDV0Ujs95YyMlRARERERESkbQ7sSFU2Pl7mMkkQk1ZLaBRmFpWxJREREREREpWFoVyBB7gLK0KBDK6ltzjXKWAkREREREZGyMbQrUFFo99aR9vCoGvBXX1+MzhoNS6FZ5oqIiIiIiIiUiaFdgVyh3XvH3A3GLACAHTqk7E6WtRYiIiIiIiKlYmhXIOGmhvfRh7q+WhcOnpCxEiIiIiIiIuWSNbRv2bIF999/P2JjYyEIAlauXOnxuiiKmDJlCmJiYmA0GtGzZ08cP37cY5tr165h5MiRCAoKQkhICMaMGYPc3NwqPIqqp4SR9pDESKmde9EkYyVERERERETKJWtoz8vLQ/PmzbFw4cJiX581axbmzZuHRYsWYceOHfD390efPn1QWOhakXzkyJE4dOgQ1q9fj9WrV2PLli146qmnquoQZCEINza8T722zaS2OUcrYyVERERERETKpZFz5/369UO/fv2KfU0URcydOxevvfYaBg4cCAD46quvEBUVhZUrV2LYsGE4cuQI1q5di127dqFNmzYAgPnz56N///54//33ERsbW2XHUpWEorAuCLDZrNBovC8UxyXVgUGVjEJHCPLNkXDY7VCp1XKXRUREREREpCiyhvbSpKamIi0tDT179pSeCw4ORvv27bF9+3YMGzYM27dvR0hIiBTYAaBnz55QqVTYsWMHBg8eXOxnm81mmM2uFc1NJuf0bavVCqvVWklHdOeKanMfXy/Iy4fBz0+egspg1F9FYUEILGIAju8/jDpNG8pdkuyK+tCbv2dUNvaj8rEPfQP7UfnYh76B/ah87EN5lPd8e21oT0tLAwBERUV5PB8VFSW9lpaWhsjISI/XNRoNwsLCpG2K884772DatGk3Pb9u3Tr4eWkAdmez2QCtc9T617W/QGvwznuhO3S5QIGzvfPXTTh67pS8BXmR9evXy10CVQD2o/KxD30D+1H52Ie+gf2ofOzDqpWfn1+u7bw2tFemyZMnY+LEidJjk8mEuLg49O7dG0FBQTJWVjqr1Yr169fDoNPBev0m7ffcfTfCIqNKf6NMfktfhuy9zrafRYf+/fvLW5AXKOrDXr16Qav1vssaqHzYj8rHPvQN7EflYx/6Bvaj8rEP5VE047ssXhvao6OjAQDp6emIiYmRnk9PT0eLFi2kbTIyMjzeZ7PZcO3aNen9xdHr9dDr9Tc9r9VqFfElFVQCYHe2RVH02prjWzbCib3XAACWbMFr65SDUr5rVDr2o/KxD30D+1H52Ie+gf2ofOzDqlXec+2192lPTExEdHQ0Nm7cKD1nMpmwY8cOdOzYEQDQsWNHZGVlYc+ePdI2v/32GxwOB9q3b1/lNVcVleDqNpvVImMlpavbvAlUuH59jMVf5mqIiIiIiIiUR9aR9tzcXJw4cUJ6nJqaiuTkZISFhaF27dqYMGEC3nzzTSQlJSExMRGvv/46YmNjMWjQIABAo0aN0LdvXzz55JNYtGgRrFYrxo8fj2HDhvnsyvEAoFK5lqKzWrw3tGt1WhjVmcizR6LAHsoV5ImIiIiIiG6RrKF99+7duPfee6XHRdeZjxo1Cl988QVefvll5OXl4amnnkJWVhbuvvturF27FgaDQXrPN998g/Hjx6NHjx5QqVQYMmQI5s2bV+XHUpWcI+3O+fE2i03eYsqg1eYA9kjYRCPSz11ETEKc3CUREREREREphqyhvVu3bhBFscTXBUHA9OnTMX369BK3CQsLw5IlSyqjPK+lUrmmx1ut5lK2lJ/GYAMKne3zR08ytBMREREREd0Cr72mnUrmPj3eZvPukXZdkGs6fOa5km/DR0RERERERDdjaFcgtcoVhG0Wq4yVlM0vIlBq513OkbESIiIiIiIi5WFoVyCV2m31eJt3h/aw2q5b71lzvHtWABERERERkbdhaFcgj5F2q3eH9poN6khta4GsSygQEREREREpDkO7AnmMtNu9e/Q6unYtaATnSnRWa4DM1RARERERESkLQ7sCKWmkXaVWw6C+BgAosIfBbvXuXzIQERERERF5E4Z2BVKrXaHdbrPLWEn5aHX5AAAHtDh7/KTM1RARERERESkHQ7sCqTVuI+1evhAdAGiNrl8sXDp+Wr5CiIiIiIiIFIahXYGUNtKuC9ZK7ezzl2WshIiIiIiISFkY2hXII7R7+UJ0ABAQGSK1C67lyVcIERERERGRwjC0K5Ba47p1ms3u/SPt4Qk1pbY1xyFjJURERERERMrC0K5AGrfQbrd5/0h7fOMkqW0162WshIiIiIiISFkY2hXIc3q894+0h0ZGQC/kAAAs1kCZqyEiIiIiIlIOhnYF0mhcC7s5FBDaAUCvyQIAFDhCUZCXL28xRERERERECsHQrkDu17Tb7cq4RlyrL7jeUuHM4WOy1kJERERERKQUDO0KpNG6RtqVMD0eADR+otTOOHVOxkqIiIiIiIiUg6FdgTQat2vaHcoYadeHGqS26dJVGSshIiIiIiJSDoZ2BdJodVLb4VDGSHtQdJjULswsKGVLIiIiIiIiKsLQrkDu17Q7FDLSHlknTmrb8gQZKyEiIiIiIlIOhnYF0rqNtCtlenx8k/oAnLVazYbSNyYiIiIiIiIADO2KpNG6RtpFhYR2v4AAGFVZAACzPUTWWoiIiIiIiJSCoV2BtDq91LY7xFK29C46jQkAYHYEIesKF6MjIiIiIiIqC0O7AmncrmkXRWWMtAOAxmCW2mePHJexEiIiIiIiImVgaFcgjd410u4QlTPSrg1wLUB3JfWCjJUQEREREREpA0O7Amk1WqntUND0eGOYv9TOTc+SrxAiIiIiIiKFYGhXIK3bSLuooJH24NgIqW3OtshYCRERERERkTIwtCuQRus20q6g0B6TlCC1bfn86hEREREREZWFyUmB3EO7gjI74hrUgwpWAIDV4idzNURERERERN6PoV2BVCoVcH3VeAeUk9q1Oi2M6kwAQKE9FA67XeaKiIiIiIiIvBtDu1Jdz+pKGmkHAK02BwBgFf1w+cIlmashIiIiIiLybgztCiVcT+0Ky+zQGGxS+9zRkzJWQkRERERE5P0Y2pVKVGZo1wWppfa1s2kyVkJEREREROT9GNoVSrj+X6WFdr+IQKmdf9kkYyVERERERETej6FdoYSyN/FKYbWjpbbFZCtlSyIiIiIiImJoVyhppF1QVnyv2aCO1LYVaGSshIiIiIiIyPsxtCuUUqfHR9euBY1QCACwWANkroaIiIiIiMi7MbQrlDTArrCRdpVaDYP6GgCgwB4Gu5VT5ImIiIiIiErC0K5QQtFYu6CCw+GQt5hbpNXlAwAc0OL8iVSZqyEiIiIiIvJeDO0KpXIbYLdZrfIVchu0RrvUvnicoZ2IiIiIiKgkDO0KJbhNi7dZLTJWcut0wVqpnXUuQ8ZKiIiIiIiIvBtDu0Kp3EK7xWyWsZJbFxgTJrXzL+fKWAkREREREZF3Y2hXKJXHSLuypsfHNEyU2pYcGQshIiIiIiLycgztCqVyu6jdYlHWSHvdpo2hgvMXDVazv8zVEBEREREReS+GdoUSBFfX2SzKGmnXGfTw01wFAOTbInjbNyIiIiIiohIwtCuU2m2k3WpR1kJ0AKDVOefF26HH6SMpMldDRERERETknRjaFUpQuY2025Q10g4AugDXveXPHz4pYyVERERERETei6FdodRuod2qsOnxAGCM8JPapvNXZKyEiIiIiIjIezG0K5TKLbTbFTjSHpYQI7ULrxXKWAkREREREZH3YmhXKJVKLbWtCrvlGwDUblJfalvytTJWQkRERERE5L0Y2hXKfXq8Eldfj0mIg07IAwBYLCHyFkNEREREROSlGNoVSq12jbTbrMpbPV6lVsOgvX7bN0c4crNMMldERERERETkfRjaFUqtdls93q68kXYA0BoKpPaJ5EMyVkJEREREROSdGNoVSuUx0q7M0K4Ldn39Mk6clbESIiIiIiIi78TQrlDu0+PtCh1pD4wOkdq56Vmy1UFEREREROStGNoVSq3RSG2ljrRH1qsttS3ZDhkrISIiIiIi8k4M7Qql8YGR9rrNmwBwhnVrgVHeYoiIiIiIiLwQQ7tCqdWukXa73S5jJbcvICQIfqprAIACWzgcCj0OIiIiIiKiysLQrlBqjdtCdDblhl2dLhsAYBX9cen0OZmrISIiIiIi8i4M7Qql0WiltpJHqHX+Vql95u8UGSshIiIiIiLyPgztCuUx0q7Qa9oBwBBqkNqZp9NkrISIiIiIiMj7MLQrlEbrGyPtwXE1pHbBlXwZKyEiIiIiIvI+DO0KpfFYiE65t0ur1bie1Lbk8etIRERERETkjilJoTRu92m3O5Q70h7fMAlqmAEAFkugzNUQERERERF5F68O7Xa7Ha+//joSExNhNBpRt25dzJgxA6IoStuIoogpU6YgJiYGRqMRPXv2xPHjx2WsumpotDqp7VBwaFdrNfDTXAEA5NsiYC4okLkiIiIiIiIi7+HVof3dd9/FRx99hAULFuDIkSN49913MWvWLMyfP1/aZtasWZg3bx4WLVqEHTt2wN/fH3369EFhYaGMlVc+j5F2u1jKlt5PZ8gDAIjQ4NSBIzJXQ0RERERE5D28OrRv27YNAwcOxH333YeEhAQ89NBD6N27N3bu3AnAOco+d+5cvPbaaxg4cCCaNWuGr776ChcvXsTKlSvlLb6SaXRuC9E5lHtNOwBo3WbFXzp2WrY6iIiIiIiIvI2m7E3k06lTJ3zyySc4duwY6tevj/3792Pr1q2YM2cOACA1NRVpaWno2bOn9J7g4GC0b98e27dvx7Bhw4r9XLPZDLPZLD02mUwAAKvVCqvVWux7vEFRbVarFVC5ft/icNi9uu6yGCP8gUvOdvaFq4o+lrJ49CEpFvtR+diHvoH9qHzsQ9/AflQ+9qE8ynu+BdH9AnEv43A48K9//QuzZs2CWq2G3W7HW2+9hcmTJwNwjsR37twZFy9eRExMjPS+oUOHQhAELF26tNjPnTp1KqZNm3bT80uWLIGfn1/lHEwFy7l6BSfOngMAGGwWNGrbXuaKbl9O6iVkH60PAAj2/xuBXeJlroiIiIiIiKhy5efnY8SIEcjOzkZQUFCJ23n1SPv333+Pb775BkuWLEGTJk2QnJyMCRMmIDY2FqNGjbrtz508eTImTpwoPTaZTIiLi0Pv3r1LPVlys1qtWL9+PXr16oX082dxYsl3AACDwYD+/fvLXN3tu5aegWVvpjgf2PwVfSxlce9DrVZb9hvIK7EflY996BvYj8rHPvQN7EflYx/Ko2jGd1m8OrT/85//xCuvvCJNc2/atCnOnDmDd955B6NGjUJ0dDQAID093WOkPT09HS1atCjxc/V6PfR6/U3Pa7VaRXxJtVotjH7+0mNRFBVRd0miatWEQbUThY5gmK2hij6W8lLKd41Kx35UPvahb2A/Kh/70DewH5WPfVi1ynuub2shunPnzuH8+fPS4507d2LChAn45JNPbufjSpSfnw+VyrNEtVotLbyWmJiI6OhobNy4UXrdZDJhx44d6NixY4XW4m00bh3s8N4rHMpNr80EABQ6QnA1/bLM1RAREREREXmH2wrtI0aMwKZNmwAAaWlp6NWrF3bu3IlXX30V06dPr7Di7r//frz11lv43//+h9OnT2PFihWYM2cOBg8eDAAQBAETJkzAm2++iVWrVuHgwYN47LHHEBsbi0GDBlVYHd5Ip3PNFPCF0K41uhYGPLXvkIyVEBEREREReY/bmh7/999/o127dgCc153fdddd+PPPP7Fu3To888wzmDJlSoUUN3/+fLz++usYO3YsMjIyEBsbi6efftrj819++WXk5eXhqaeeQlZWFu6++26sXbsWBoOhQmrwVu63fPPitQTLTR+iAbKc7aupF2SthYiIiIiIyFvcVmi3Wq3SNeEbNmzAAw88AABo2LAhLl26VGHFBQYGYu7cuZg7d26J2wiCgOnTp1foCL8S6PS+NdIeGBMGnHa28zNyZK2FiIiIiIjIW9zW9PgmTZpg0aJF+OOPP7B+/Xr07dsXAHDx4kWEh4dXaIFUPI1GC1wP674w0h7TIEFqm5nZiYiIiIiIANxmaH/33Xfx8ccfo1u3bhg+fDiaN28OAFi1apU0bZ6qQFFol7mMilC3eWMIsAEArGb/MrYmIiIiIiKqHm5reny3bt1w5coVmEwmhIaGSs8/9dRT8PPzq7DiqCxFI+0yl1EB9EYj/NRXkWePQoE1HHarDWqtV9+RkIiIiIiIqNLd1kh7QUEBzGazFNjPnDmDuXPnIiUlBZGRkRVaIJVMuB7WfSCzAwB0Oue8eBsMOH8iVeZqiIiIiIiI5HdboX3gwIH46quvAABZWVlo3749Zs+ejUGDBuGjjz6q0AKpZAJ8Z3o8AGj97FL74nGGdiIiIiIiotsK7Xv37sU999wDAFi2bBmioqJw5swZfPXVV5g3b16FFkglE67/11dCuy7YdRu7rHMZMlZCRERERETkHW4rtOfn5yMwMBAAsG7dOjz44INQqVTo0KEDzpw5U6EFUtl8JbQHRLnWRyi4lidjJURERERERN7htkJ7vXr1sHLlSpw7dw6//vorevfuDQDIyMhAUFBQhRZIJROkhlDaZooRkRArtS05DhkrISIiIiIi8g63FdqnTJmCl156CQkJCWjXrh06duwIwDnq3rJlywotkEpWlNVF+EZoj29SX2rbzHoZKyEiIiIiIvIOt3VPrYceegh33303Ll26JN2jHQB69OiBwYMHV1hxVDrpNy4+MtIeEhEOvcoEsyMIFitnbBAREREREd32jbCjo6MRHR2N8+fPAwBq1aqFdu3aVVhhVDahKKwLAhx2O1RqtbwFVQC9JgtmSxAKHCHINeUgIChQ7pKIiIiIiIhkc1vT4x0OB6ZPn47g4GDEx8cjPj4eISEhmDFjBhwOXotcVVRuI+wWi0XGSiqOVld4vaXC2cPHZK2FiIiIiIhIbrc10v7qq6/is88+w8yZM9G5c2cAwNatWzF16lQUFhbirbfeqtAiqXjuk+JtVgtgNMpWS0XRBADIdbYzTp1H4w6tZa2HiIiIiIhITrcV2r/88kt8+umneOCBB6TnmjVrhpo1a2Ls2LEM7VVEpRKA6xMbLGazvMVUEGOoEUhztnPSrspbDBERERERkcxua3r8tWvX0LBhw5ueb9iwIa5du3bHRVH5CG7T420+Mj0+qGaE1DZn+sYvIoiIiIiIiG7XbYX25s2bY8GCBTc9v2DBAjRr1uyOi6LyUQmu7rP6SGiPrhsvta35t/X1JCIiIiIi8hm3NT1+1qxZuO+++7BhwwbpHu3bt2/HuXPnsGbNmgotkEqmUrmNtNtsMlZSceIbJUHAJYhQw2rxk7scIiIiIiIiWd3WUGbXrl1x7NgxDB48GFlZWcjKysKDDz6IQ4cO4euvv67oGqkEKpWr+2xW3xhp1xn0MKqdl1gU2kLhsNtlroiIiIiIiEg+t32f9tjY2JsWnNu/fz8+++wzfPLJJ3dcGJXNc3q8VcZKKpZOm4N8ew1YRT9cuZiGyLiacpdEREREREQkC140rGBqte+NtAOA1uD6BcS5oydlrISIiIiIiEheDO0K5jHSbvWdkXZtkFpqXz1zScZKiIiIiIiI5MXQrmAqt5F2u813QrtfRKDUzr9skrESIiIiIiIied3SNe0PPvhgqa9nZWXdSS10i9Rq14i01eobq8cDQFjtaGCvCACwmHznuIiIiIiIiG7VLYX24ODgMl9/7LHH7qggKj/31ePtPnLLNwCo1bAeduI4AMBacNtrJRIRERERESneLSWixYsXV1YddBs0HiPtvjM9PiouFhrhAGyiEVZrYNlvICIiIiIi8lG8pl3BVG6h3W73nZF2lVot3au9wB7qU7ezIyIiIiIiuhUM7QrmPtJu86Fr2gFAq8sHADigxbmUEzJXQ0REREREJA+GdgVTqV1XN/jSSDsAaPwcUvvisdPyFUJERERERCQjhnYFU2vcRtp9aCE6ANCH6KR29qXLMlZCREREREQkH4Z2BdO4jbQ77HYZK6l4gdGhUrvwar6MlRAREREREcmHoV3B1Bq36fE+NtJeI6GW1LbmijJWQkREREREJB+GdoWwXMxF/u50RF7Uw55lBgBo3KfH2x0lvVWR4pvUl9o2s0HGSoiIiIiIiOTD0K4QhUeuIeenVMSd8Yct3TldXK3RSq87fGwhusDQEBhUWQAAsy1E1lqIiIiIiIjkwtCuEILONaouWpzXr2vcp8f72Eg7AOg12QCAQkcwcjKz5C2GiIiIiIhIBgztCqHSu0K7w+wM6Bqta6Td7vCthegAQKMvlNqnD6XIWAkREREREZE8GNoVQtC7uqr4kXbfC+26AEFqX069IGMlRERERERE8mBoV4hip8dr3a9p973p8YZwf6mdm5YpYyVERERERETyYGhXCPfp8aLZGdq17qFd9L3QHhwbIbXN2RYZKyEiIiIiIpIHQ7tCeI60F13TrpOe88WF6GIaJEptaz6/qkREREREVP0wCSmEUMxIu1rruqbdF0fa45LqQgUrAMBm8S9jayIiIiIiIt/D0K4QHtPjr1/TrtPppeccDt8L7VqdFkb1NQBAgT0MDh9cbI+IiIiIiKg0DO0K4THSXjQ93m31eIdDrPKaqoJWmwsAsIkGXDp9TuZqiIiIiIiIqhZDu0II2ptH2rU61zXtvjg9HgC0RpvUvpBySsZKiIiIiIiIqh5Du0IIKgHQOrur6Jp2jVtoF0XfHGnXBblmE1w7lyZjJURERERERFWPoV1Biq5rdxRd0653v6bdN0O7f2SQ1C64kitjJURERERERFWPoV1BBN31kfZibvnmm5EdCKsdI7UtJi5ER0RERERE1QtDu4IU3au9aHq8zuOadt+M7XGN60tta6G+lC2JiIiIiIh8D0O7gkgryNtFiDYHVGo1cD2s++o17ZE1o2FUOW/7lm+JhN1qK+MdREREREREvoOhXUGKpscDrhXki0K7b64d72QwOEO7VfTD8eS/Za6GiIiIiIio6jC0K0jR9HjAtRidgKKRdllKqhL6ENfBnd2fImMlREREREREVYuhXUGk6fFwXdcuTY+Xo6AqElI7XGqbzl2TsRIiIiIiIqKqxdCuIB4j7eaikXbfl9j2LqltydaUsiUREREREZFvYWhXEFUx17QXhXZfHmlPaJgEvWACAORbasBh563fiIiIiIioemBoV5Dipse7Qrvvjrmr1GoY9ZcBAGZHEM6knJC5IiIiIiIioqrB0K4gngvROdeLl6K672Z2AIA+yDW6nrqbK8gTEREREVH1wNCuIB63fCsaab8e1n15pB0AAmsFS+2sM1dkrISIiIiIiKjqMLQrSGnT4yEIcDh8927ttVs2lNrmTBkLISIiIiIiqkIM7QpS3H3aVUVD7YIAm9UiR1lVon6LptAK+QCAgsIwmashIiIiIiKqGgztClLcSLta5erCgvz8Kq+pqqi1GvjpMgAABY5wXDp9TuaKiIiIiIiIKh9Du4IIxdzyTaVyXcte6MOhHQD0gWapfXxHsnyFEBERERERVRGGdgVRuY20O6SRdtdz5oKCKq+pKgXEBErtqyfTZKyEiIiIiIioajC0K4j7Ne1F0+M1avfQXljlNVWl2LvqSm3zNXspWxIREREREfkGhnYF8Qjt16fHq9WuLjSbfTu0N2rfCmo4F9srLAguY2siIiIiIiLl8/rQfuHCBfzjH/9AeHg4jEYjmjZtit27d0uvi6KIKVOmICYmBkajET179sTx48dlrLgSaQSIEAG4psdrNK4gbyn07dCuM+jhr00HAOTZo3A1/bLMFREREREREVUurw7tmZmZ6Ny5M7RaLX755RccPnwYs2fPRmhoqLTNrFmzMG/ePCxatAg7duyAv78/+vTpg0IfDLCCIMCudoZ210i7Rnrd10faAUAf4FpsL+WvvTJWQkREREREVPk0ZW8in3fffRdxcXFYvHix9FxiYqLUFkURc+fOxWuvvYaBAwcCAL766itERUVh5cqVGDZsWJXXXNkcahGwu65p12pcXWi1mEt6m8/wizICmc725WPn5S2GiIiIiIioknl1aF+1ahX69OmDhx9+GL///jtq1qyJsWPH4sknnwQApKamIi0tDT179pTeExwcjPbt22P79u0lhnaz2Qyz2RVwTSYTAMBqtcJqtVbiEd0Zq9UK+/XZ8A6zHVarFWq3hegKCgq8uv6KUKN+bZw56mwXXLYo7niL6lVa3eSJ/ah87EPfwH5UPvahb2A/Kh/7UB7lPd+CKIpiJddy2wwGAwBg4sSJePjhh7Fr1y48//zzWLRoEUaNGoVt27ahc+fOuHjxImJiYqT3DR06FIIgYOnSpcV+7tSpUzFt2rSbnl+yZAn8/Pwq52AqSMMDQfDP00CEiL0dMnH20AFctTpH3aP89Iht0FjmCiuXzWxB+m8hEKFGgOYCQnoFyV0SERERERHRLcvPz8eIESOQnZ2NoKCSc41Xj7Q7HA60adMGb7/9NgCgZcuW+Pvvv6XQfrsmT56MiRMnSo9NJhPi4uLQu3fvUk+W3KxWK04f+gsAIEBAv159sCY3C1dPpAIAomNi0L9/fzlLrBLfblmKHFss8mzReKBzKwQEe2+f3chqtWL9+vXo1asXtFqt3OXQbWI/Kh/70DewH5WPfegb2I/Kxz6UR9GM77J4dWiPiYlB48aeI8eNGjXCjz/+CACIjo4GAKSnp3uMtKenp6NFixYlfq5er4der7/pea1W6/VfUofaNTFCLao8jsNus3t9/RVB75eDHBMgQo0Tuw+ibd9ucpd0y5TwXaOysR+Vj33oG9iPysc+9A3sR+VjH1at8p5rr149vnPnzkhJSfF47tixY4iPjwfgXJQuOjoaGzdulF43mUzYsWMHOnbsWKW1VhW7W2gXzXbodK7QbrVa5Cipyhlr6KR2+pHT8hVCRERERERUybw6tL/wwgv466+/8Pbbb+PEiRNYsmQJPvnkE4wbNw6A8xZoEyZMwJtvvolVq1bh4MGDeOyxxxAbG4tBgwbJW3wlcahcod1htkOrdwVYm9UmR0lVLqJ+Lamdn1EgYyVERERERESVy6unx7dt2xYrVqzA5MmTMX36dCQmJmLu3LkYOXKktM3LL7+MvLw8PPXUU8jKysLdd9+NtWvXSovY+RqPkXaLHXqd6zhttuoR2ht1bI19vxwAAJhzvXvhQCIiIiIiojvh1aEdAAYMGIABAwaU+LogCJg+fTqmT59ehVXJx+G6w5tzerzBNT2+uoT20MgI+KvTkWePQp41CuaCAuiNRrnLIiIiIiIiqnBePT2ebua+EJ3DYode7zbSbrfLUZIsDMZsAIAdOhzdmSxvMURERERERJWEoV1hblyITm90hXZ7dQrt4a4pBxf/PiljJURERERERJWHoV1h3BeiE8126PWuaeF2R/UJ7cFx4VI7/3KujJUQERERERFVHoZ2hbF7TI93QO/nHtodcpQki8i68VLbysxOREREREQ+iqFdYRw3TI83GKtnaE9s2gCA83itZi5CR0REREREvomhXWFuuuWb0XXLM4dDLO4tPskvIAB+qkwAQKEtVOZqiIiIiIiIKgdDu8K4X9PuMNuh0+sB0fmcQ6w+oR0AdDrnCvIWMQAZ5y7IXA0REREREVHFY2hXGLv7fdot1xeeE53TxKvRQDsAQGOwSu0zh47LWAkREREREVHlYGhXGI/7tJudoV0oGmmXpSL56IM1UvvqmYsyVkJERERERFQ5GNoV5sb7tAOuTqxmA+0IiAqW2vmXc2SshIiIiIiIqHIwtCuNAEAjAHBNjxeuvyRKreohPLGW1Laaqts8AyIiIiIiqg4Y2hVI0DkvbHfcONIuVK/Q7rztm5O1UC9jJURERERERJWDoV2BikK7NNJeFNZVKjiq0b3aQyLCYVA5V5A320LkLYaIiIiIiKgSMLQrkEp/PbRfH2lXq1wj7JbCQllqkotec/1e7Y4QZF25KnM1REREREREFYuhXYEEnbPbRKsDokOEym1afGF+vlxlyUJrMEvt038fk7ESIiIiIiKiisfQrkCC3nWzdtFih1rl6sbCwgI5SpKNNsh17FdOnZOxEiIiIiIioorH0K5ARde0A84p8mq1qxvNBdUrtPvVCJTauenZMlZCRERERERU8RjaFUjQu7rNYbFDrXKFeEs1G2kPj4+R2uZsm4yVEBERERERVTyGdgW6caRdo3E9Nlezhejim9SX2rZCrYyVEBERERERVTyGdgVyD+0Osx1qtetxYTUL7ZFxNaET8gAAFkuQzNUQERERERFVLIZ2Bbp5pF0jPbaazcW9xacZNNcAAPmOMOTn5spcDRERERERUcVhaFcglds17aLFDq3GNS3cbK5eI+0AoNUXXcevwum/U2SthYiIiIiIqCIxtCuQx/R4ix0ardtIu8UqR0my0ga42uknzshXCBERERERUQVjaFegG6fHa7U66bHFUv2mxxsj/KV2zqVMGSshIiIiIiKqWAztCuR+yzdnaHcfabfIUZKsQuIipbY5q/r90oKIiIiIiHwXQ7sC3Tg9XqvTS49t1uo3PT6uYT2pbc3XlLIlERERERGRsjC0K9CN0+N1erfp8dUwtNeslwANnAvwWSyBMldDRERERERUcRjaFUjQu4V2iwM695F2m02OkmSlUqthvH7btwJ7WLVcjI+IiIiIiHwTQ7sCCTpXtznMduj01Tu0A4BWlwcAcECL04ePyVwNERERERFRxWBoVyCVx0g7QzsAaP0dUvvSsVQZKyEiIiIiIqo4DO1KpFEBgrPpMNuhNxikl+x2u0xFycsQZpTapgtXZKyEiIiIiIio4jC0K5CgEiBonaPtzoXoXKHdVk1De1DNCKldeK1AxkqIiIiIiIgqDkO7QhUtRida7DAYXaPMdrujpLf4tJj6iVLbms+vNRERERER+QamG4Uquq7dYbZD7+cnPW93VM/QntC4PlRwrhpvNfvLXA0REREREVHFYGhXqJJG2h3VNLRrdVoY1ddv+2YLg6OaXiZARERERES+haFdoaTbvtlF6LVuC9GJokwVyU+nywEA2GDAhROn5S2GiIiIiIioAjC0K5RKr3G1HSrgelh3VOPQrvVz3e7ufMpJGSshIiIiIiKqGAztCiWNtMO5grwgOqfFV+PMDl2I6371mWfTZayEiIiIiIioYjC0K5SgU0tt0WKHUDTSLldBXiAoJlRqF1zJk7ESIiIiIiKiisHQrlBFq8cDgMNih3C9XY0H2hFVL15qW3NlLISIiIiIiKiCMLQrlOAW2kWzW2gXhOLfUA0k3NUARXMNrGZj6RsTEREREREpAEO7QqluCO2q61ldRPUN7X4BAfBTOW/7VmgLLWNrIiIiIiIi78fQrlDu17Q7zHaoikbYVapqfY9ync4EALCIAbiYekbmaoiIiIiIiO4MQ7tC3bgQncptWnxhQb4cJXkFnb9Vap/YdVDGSoiIiIiIiO4cQ7tCeUyPt9ihVrmH9gI5SvIK/jEBUvvaqUsyVkJERERERHTnGNoVyn0hOofZDrXK1ZXVeaQ9pnGi1C68Wn0vEyAiIiIiIt/A0K5QN64er3IL7eZqPNLesH0rqOCcIm8uCJK5GiIiIiIiojvD0K5QqhsWotOoNdJjS2GhHCV5BaO/H/y16QCAPFsN5GRmyVsQERERERHRHWBoVyjhxmva1a7H5oLqG9oBQO+XBwAQocbh7XtkroaIiIiIiOj2MbQrlMfq8WY7NBrXY4uleod2vxp6qZ1x9KyMlRAREREREd0ZhnaFcl893mGxQ6NxTY83F5rlKMlr1KhfS2oXZFTvX2AQEREREZGyMbQrlKBRAWrnbd5EiwNat9BusVjkKssrNOjQCoADAGDO85O3GCIiIiIiojvA0K5gRVPkRbMdGq1Wet5qqd4j7aGREfBXXwYA5FmjYKnmMw+IiIiIiEi5GNoVrGiKvMNsh9YttFvM1XukHQD0xmwAgB06HN21T+ZqiIiIiIiIbg9Du4K5j7RrtTrpeZvVKldJXsMQ5vpqXzh4QsZKiIiIiIiIbh9Du4IV3fZNtNqh07lCu8XKkfawxCipnXsxR8ZKiIiIiIiIbh9Du4JJK8iLgFbjPtJuk6ki71GvXXOpbcnRlbIlERERERGR92JoVzD3e7Xr1K5garVxenzNugkwqDIBAPmWSDjsdpkrIiIiIiIiunUM7Qrmfq92ncptpN3GkXYAMOqvAgAsoj9OHjwqczVERERERES3jqFdwQSdq/vcR9ptNo4qA4A+2CG1T+87LGMlREREREREt4ehXcEEvUZqu4+02zkVHAAQFBcqtXPOXpWxEiIiIiIiotvD0K5gKreRdo3guk87Q7tTfMvGUrswi191IiIiIiJSHiYZBRNKuKbd7mBoB4B6zRtDK+QBAArNYTJXQ0REREREdOsUFdpnzpwJQRAwYcIE6bnCwkKMGzcO4eHhCAgIwJAhQ5Ceni5fkVXIPbRr4WrbHY7iNq92VGo1/HQZAIACRxgupp6RuSIiIiIiIqJbo5jQvmvXLnz88cdo1qyZx/MvvPACfv75Z/zwww/4/fffcfHiRTz44IMyVVm1VO63fBPdpsc7RDnK8Ur6QIvUPrHzgIyVEBERERER3TpFhPbc3FyMHDkS//nPfxAa6lpcLDs7G5999hnmzJmD7t27o3Xr1li8eDG2bduGv/76S8aKq4bHSLvoWpTOITK0FwmICZTa106lyVgJERERERHRrdOUvYn8xo0bh/vuuw89e/bEm2++KT2/Z88eWK1W9OzZU3quYcOGqF27NrZv344OHToU+3lmsxlms1l6bDKZAABWqxVWq7WSjuLOFdVW9F+H2hXO7WY74HAAKhUcoujVx1GVohol4NRB52h74VWb7Oflxj4kZWI/Kh/70DewH5WPfegb2I/Kxz6UR3nPt9eH9u+++w579+7Frl27bnotLS0NOp0OISEhHs9HRUUhLa3kUdV33nkH06ZNu+n5devWwc/P745rrmzr168HAPjlqtEIwQCA08dOAddH2G12B9asWSNbfd7EbrVBDX/YoUNhQbDXnJeiPiRlYz8qH/vQN7AflY996BvYj8rHPqxa+fn55drOq0P7uXPn8Pzzz2P9+vUwGAwV9rmTJ0/GxIkTpccmkwlxcXHo3bs3goKCKmw/Fc1qtWL9+vXo1asXtFotbJcLcPXgfgBA7dhaUF0T4QAgqFTo37+/vMV6ke9+XwKTNQ559kjc164pQiLkW0n+xj4kZWI/Kh/70DewH5WPfegb2I/Kxz6UR9GM77J4dWjfs2cPMjIy0KpVK+k5u92OLVu2YMGCBfj1119hsViQlZXlMdqenp6O6OjoEj9Xr9dDr9ff9LxWq1XEl7SoTpW/a5V4wSJCuN4Wr29DTjr/fCALAFQ4ufsAOtzfS+aKlPNdo9KxH5WPfegb2I/Kxz70DexH5WMfVq3ynmuvDu09evTAwYMHPZ57/PHH0bBhQ0yaNAlxcXHQarXYuHEjhgwZAgBISUnB2bNn0bFjRzlKrlLuC9E5LHaoANgBiIJQ4nuqI79I/fXQDlw+dk7WWoiIiIiIiG6FV4f2wMBA3HXXXR7P+fv7Izw8XHp+zJgxmDhxIsLCwhAUFIRnn30WHTt2LHEROl8iuN3yTTTbUZTVGdo9RdavjbPHnO38y+bSNyYiIiIiIvIiirjlW2k++OADDBgwAEOGDEGXLl0QHR2N5cuXy11WlRBUAgStswtFix2qorAuqGCzceXHIg07toIAOwDAnBtYxtZERERERETew6tH2ouzefNmj8cGgwELFy7EwoUL5SlIZoJeDdHqgMPigNpthN2cXwBNEK9HAYDg8DAEai/BZK2FHFs0Mi6kIbJmyWseEBEREREReQvFj7RXd0VT5EWzDSqVqzsLCsp3+4DqwhCcd72lwt+/bZO1FiIiIiIiovJiaFc41fXF6BxmO9Qq95F2hnZ3oXXCpfa14+kyVkJERERERFR+DO0KJ60gbxOhVrmudjCbC2WqyDs1vre91C7MMspYCRERERERUfkxtCucys913brWPbQXFMhRjteKTYxHgCYNAGCyxCInM0vegoiIiIiIiMqBoV3hVEZXUHcP7RYzb212I0NANgBAhAbJG/+UuRoiIiIiIqKyMbQrnHto16hco+7mQk6Pv1FwfJDUvnzkvIyVEBERERERlQ9Du8J5jLQLHGkvTf27W0ntgqtqGSshIiIiIiIqH4Z2hVP5uYd210i7xcLQfqM6TRvBT3UVAJBTWBMFeVxhn4iIiIiIvBtDu8K5j7TrPEbaLXKU4/WM/lcAAHboceD37TJXQ0REREREVDqGdoUT3K9pF10j7TYrQ3txAmq6bvd2af8pGSshIiIiIiIqG0O7wnlc0w63kXaLVY5yvF5i+6ZSOz9DlLESIiIiIiKisjG0K5xHaBddbZuVob04jdq1gF5lAgDkFcbAbrXJXBEREREREVHJGNoVzuOWbw7XiuhWhvZiqdRq+BvTAQAW0R8H/9wlc0VEREREREQlY2hXOI+RdrtbaLcxtJfEL8p1ns7tOSJjJURERERERKVjaFc4Qa2CoHOGUI3d1Z02m12ukrxeXKv6Ujv3EhfsIyIiIiIi78XQ7gOKRts1VtcIst3Oa7VL0rRLR2gF5z3a8/Mj4bDzFxxEREREROSdGNp9gMrPGdrVbjPibQyiJdLqtAgwXAIAFDpCcGzvQZkrIiIiIiIiKh5Duw8oGmlXO9xH2hnaS2OMcN3u7eSOAzJWQkREREREVDKGdh8gFIV20dWddrtDrnIUIbppgtTOPZcnXyFERERERESlYGj3AdI17W7daXcwtJemefdOUMO5CF1BbqjM1RARERERERWPod0HSNPj3brTIYolbU4A/AICEKC/CADIs0fibMoJmSsiIiIiIiK6GUO7D5AWonMP7Q6G9rIYQ123ezvy+y4ZKyEiIiIiIioeQ7sPKBppFyBAuD7CzpH2stVoVFNqZ53Kkq8QIiIiIiKiEjC0+4Ci0A4AKlEAAPCK9rK16tNVuq493xTO+7UTEREREZHXYWj3ASqjVmqr4QztHGcvW0BIEAIN5wEA+Y4InEj+W+aKiIiIiIiIPDG0+wD3kfai69o50l4+fpGuM5WyZa+MlRAREREREd2Mod0HFBfaRUGQqxxFiWvTQGrnnreUsiUREREREVHVY2j3AYJbaNdAff1Jdm15NO/WCTohBwCQmx8Dq8Uqc0VEREREREQuTHY+QFVsaBdgszKAlkWr0yLA7xIAwCIGIPm3P2WuiIiIiIiIyIWh3QcIKgGC3hnWpdAOoCAvV66SFCWwlkFqn997TMZKiIiIiIiIPDG0+wiVn3O0XSO6Rt0LCwrkKkdR6ndtLbXz09WlbElERERERFS1GNp9RNEUea3bSLu5kKG9POq3ago/9WUAgMlcCzmZWfIWREREREREdB1Du48oCu3u0+PNHGkvN//AawAAB7TYu/Z3mashIiIiIiJyYmj3EUWhXS26utRcWChXOYoTUi9Mal8+fFHGSoiIiIiIiFwY2n2EyqgFAKg9pscztJdX056dADgAAPlZAfIWQ0REREREdB1Du48Qihaic+tSS6FZrnIUJyYhDkFa5wh7jrUmLpw8LW9BREREREREYGj3GcVNj7dYGNpvhTE0X2r/vXG7jJUQERERERE5MbT7CCm0u02Pt5gZ2m9F5F21pHbWySz5CiEiIiIiIrqOod1HuEK7q0utVotc5ShSq75doYbznOXlRMBht8tcERERERERVXcM7T7Cdcs3t+nxZob2WxEQFIhAw3kAQIEjHCl7D8hcERERERERVXcM7T6iuGvaOdJ+6/wiRal94o99MlZCRERERETE0O4zihtpt1ptcpWjWPHtGkjt3AtWGSshIiIiIiJiaPcZxS1EZ+NI+y1r3rUTdEIOACAnP5a3zSMiIiIiIlkxtPsIwaABBM+F6GxcSO2WqbUaBPpfAgBYRX9s/X61zBUREREREVF1xtDuIwSVAMGggcbtmnabjdO7b0dUi0ipnbbvqoyVEBERERFRdcfQ7kNURo3nSLuNI+234+6h98OougYAyCpIROrhFJkrIiIiIiKi6oqh3Yc4Q7vrmna73SFjNcql1WkRHO0cYRehxr7lm+UtiIiIiIiIqi2Gdh+iMmo8Vo/nNe23r+XgLgCcv/TIvhgGq4WXGhARERERUdVjaPchKqPG4z7tdgdH2m9XnaaNEGo8BQDId4Tjz2VrZK6IiIiIiIiqI4Z2H6Ly00DjNj3ewdB+RyKbh0ntS3vSZKyEiIiIiIiqK4Z2H3LjQnQOUZSxGuW755H7YFBlAQAy8xJx7vgpeQsiIiIiIqJqh6Hdh6iMGqggANezOqfH3xm90YjgGukAABEa7P5hg8wVERERERFRdcPQ7kMEowYCBGkxOgcH2u9Ys4F3S+3sC0GwW20yVkNERERERNUNQ7sPURm1ACBNkWdmv3P1WzVFiME5LT7PHoltK9fKXBEREREREVUnDO0+RGXUAIB0r3ZOjq8YNZoESO0LO87LWAkREREREVU3DO0+pCi0a8SikXZBznJ8xj0jHoBeZQIAZOYm4mLqGZkrIiIiIiKi6oKh3Ye4Rtqvh3aBob0iGP39EBJ+CQDggBa7lq6TuSIiIiIiIqouGNp9iMrPM7RDEHiv9grS+L52UvvauSBYCs0yVkNERERERNUFQ7sPEfRqQAVorl/TDkGAzWqRtygf0bhDa4QaTgIA8u018NviZTJXRERERERE1QFDuw8RBAEqgwZq0dWtBfn5MlbkWxK6xknt9EPg7d+IiIiIiKjSMbT7GJVR45oeD8DM0F5hOjzQC8G60wCAXFsMNn2zUtZ6iIiIiIjI9zG0+xjBqIHGrVsLCwpkrMa3qNRq1GwfKj2+tCcPDrtdxoqIiIiIiMjXMbT7GJWfVrpPOwCYCwplrMb3dB32AAK1FwAAJmscti5bI3NFRERERETky7w6tL/zzjto27YtAgMDERkZiUGDBiElJcVjm8LCQowbNw7h4eEICAjAkCFDkJ6eLlPF8lMZPa9pN5sZ2iuSSq1GdHOd9PjctgwZqyEiIiIiIl/n1aH9999/x7hx4/DXX39h/fr1sFqt6N27N/Ly8qRtXnjhBfz888/44Ycf8Pvvv+PixYt48MEHZaxaXqobpsdbChnaK9q9jz0If00aACDLnIi/fl4vc0VEREREROSrNHIXUJq1a9d6PP7iiy8QGRmJPXv2oEuXLsjOzsZnn32GJUuWoHv37gCAxYsXo1GjRvjrr7/QoUMHOcqW1Y0L0VksvJ94RdPqtIhsaEfq387Hp35LRYf75a2JiIiIiIh8k1eH9htlZ2cDAMLCwgAAe/bsgdVqRc+ePaVtGjZsiNq1a2P79u0lhnaz2Qyz2RVmTSYTAMBqtcJqtVZW+XesqLbSahR1AjSi65r2gvx8rz4mpery2ECkv/Ir8h3hyCyohz0btqBZ145lvq88fUjej/2ofOxD38B+VD72oW9gPyof+1Ae5T3fgiiKYiXXUiEcDgceeOABZGVlYevWrQCAJUuW4PHHH/cI4ADQrl073HvvvXj33XeL/aypU6di2rRpNz2/ZMkS+Pn5VXzxVSg8QwfT6avYqT0BAIgO8ENMUgOZq/JNpt2nYLrcHAAQbDyCwG61ZK6IiIiIiIiUIj8/HyNGjEB2djaCgoJK3E4xI+3jxo3D33//LQX2OzF58mRMnDhRemwymRAXF4fevXuXerLkZrVasX79evTq1QtarbbYbQoPX8O20xulx3G1aqJ3//5VVWK1ktvZhB+nbIHZEYzsggZoHGhA03val/qe8vQheT/2o/KxD30D+1H52Ie+gf2ofOxDeRTN+C6LIkL7+PHjsXr1amzZsgW1arlGM6Ojo2GxWJCVlYWQkBDp+fT0dERHR5f4eXq9Hnq9/qbntVqtIr6kpdVpD9BD43bLN5vNpohjUqLQiHCE17qCi2eDAaiQvCwNASFHUb9V0zLfq5TvGpWO/ah87EPfwH5UPvahb2A/Kh/7sGqV91x79erxoihi/PjxWLFiBX777TckJiZ6vN66dWtotVps3OgaWU5JScHZs2fRsWPZ1xf7ohtv+Wa12mSsxvd1f3qIdN/2AkcY/vz0CE4dPCJzVURERERE5Cu8OrSPGzcO//3vf7FkyRIEBgYiLS0NaWlpKCgoAAAEBwdjzJgxmDhxIjZt2oQ9e/bg8ccfR8eOHavlyvEAoPLTeqweb7VaZKzG9wWHh6HnxC4I0FwCAOQ7IrBl0X6cTTkhc2VEREREROQLvDq0f/TRR8jOzka3bt0QExMj/Vm6dKm0zQcffIABAwZgyJAh6NKlC6Kjo7F8+XIZq5bXjfdpt9k40l7ZYhPj0f35jvBXpwMA8uyR2DR/Jy6cPC1vYUREREREpHhefU17eRa2NxgMWLhwIRYuXFgFFXk/QaeCyj20c3p8lYhLqoOu46zYvDAZ+fYayLVFY+MHW9HnZQ2ianNVeSIiIiIiuj1ePdJOt04QBGh1rt/FcKS96iQ2boC7n7wLRtVVAECOLRbr3tsI8/XLOYiIiIiIiG4VQ7tCFNoKsWD/AqTb08vcVq11C+12e2WWRTdIatEEHR+vB4MqCwBgssZh7YIl8hZFRERERESKxdCuEJvPb8bnhz7H/Jz5GPHLCHx9+GtcKbhS7LZag+vWAXaG9irXqG1LNB0QIj1OPxWJi6ln5CuIiIiIiIgUi6FdIdYfXIkXVtjRNsWBE5ePYNauWej5Q0+M3TAWa1PXosDmmoKt9zNI7ZyCQjgcDjlKrtba9e+OiOAUAIBV9MfWT9bIXBERERERESmRVy9ERy4v5HRAztHf0fGoiBwDsK2xgC13ifjDsQV/XPgDRo0Rd9e8Gz1q90CrwDiEXwrEVVUOzCo1Nq/+Cd0fGCz3IVQ7nZ7ojTVzj8ImGnE5Mwn7ftuKu+5pL3dZRERERESkIBxpV4rte6RmYCHQZ6+It76yY+7Hdjz4pwNBGflYf2Y9XvnjFay6+DNa2+q43rprDxycJl/l4hrURVTti9cfqXDopxT2AxERERER3RKGdoWo+cEHiFn0EUwtW0AwuKa/x2YCw7Y4MP9jO2Z9ZsNDfzggFuQizhGOGo4gAIBVrcHsT9/AgcsH4BA5Vb4q9X52OPzVGQCAbHMiNn+9QuaKiIiIiIhISTg9XiEEjQb+nTsjLTsbLbp2RcFvm5C9ahXyd+wArt/PPiEDSMhwQFs3F0JTAa1tdbBWlwwAyD1rwT9Wj0SoXxg6xXZCh5gOaB/THtH+0TIele/zCwhAXDstjm53Pr64T43ArhZ5iyIiIiIiIsVgaFcglb8/Qh4cjJAHB8N68SJMv/yCnHXrUbB/PwBAtOYBAGo6whBU4IDJqIKg9cN9B+vh96TjWF24GqtPrQYAJAQloH1Me7SPaY+OMR0RoAuQ7bh81b3/GIS0fZ8jq7Au8h0R0CbvBx6UuyoiIiIiIlIChnaF08bGInzMGISPGQNrejpyNmxAzpZjAAABAlpcsmJLHT0AwE/fAB/NS8HpGBF/Jwg4mCDgWM1ULDWdxtKUpdCr9ehaqyv61+mPe2reA51aJ+eh+QyVWo3mDzXHlv9mQ4QauZkNcfrwMSQ1byJ3aURERERE5OUY2n2INioKYSNHwr9jNi5/cgAAUD8qCTtNB1AYFAK73oADdzVEm4NHUP+iiAe3ibBogKO1nAF+d1Ih1tnXYd2ZdQjUBaJ3fG/0T+yPNtFtoBK4/MGduOvudjjyy0fIuNoAduix8/PtiJ5aE4GhIXKXRkREREREXoxJzAep/Fy/i/Frfzd6D3bd7i01qQFsale362xAs9MiRm524IP/2PHu5zbc/5cDuism/Hj8R4xZNwaDfhqEpUeXIt+aX6XH4Wu6jh0MvWACAGSZ6+CnqctxNf2yzFUREREREZE3Y2j3QapA17R229VCtLqnKwJVzsXqHDo9jj3/LGLfm4XgBx+EJibG472J6cCjmxz4aKEdU/9rQ899DlzKOIU3d7yJnst6Yvbu2biYexF06yJrRqNRHwO0gvOXH9nmBKx5cw0yzl2QuTIiIiIiIvJWDO0+SO2vhTrEeR279UIORIeIXn37Sa8fOncemq5dEfv2W6j320bUXfsLov41GYZmzTw+p/E54Km1DixaaMeoDXYY00344tAX6Le8H17Y9AL2pO+BeH3leiqfdvf1gH+jC9ALOQAAkzUOa9/dhPMnUmWujIiIiIiIvBFDu4/SxQUCAESLA9b0fDRr1wFBagEA4FBrsGD2ezBlZkIQBOgSEhD22GNI/H4p6v66FhHPPQtdYqL0WX5m4L5dIuYtsuOfy+xolGrDhjPrMXrtaAz73zD8fPJnWO1WWY5TiQLio9HmkUgYVFkAgBxbLDZ88BdOHToqb2FEREREROR1GNp9VFFoBwDLOed11Pc/+CAEhx0AUCCosfCDObh2OcPzffHxqDF2LOqs+R8Sli1DyMMPQdA7R+1VANoeF/HGtw6895kd9+534HjaIfxr67/Q+8feWLR/ETILM6vmABWuSec26Px4HfiprgIA8uxR+H3hQaz96BuYCwpkro6IiIiIiLwFQ7uP0tV2C+1nnVOxk5o0xZD7B0CwO4O7WaXGovnzcPnSzdeoC4IA411NEDNjBupt3oQaL7wATVSU9Hr8ZeD/MdOo2QAAWlBJREFUrXFe+/7I73bYMy5jYfJC9PmxDz7Y8wGuFV6r5CNUvoZtW6Dr/7sL/up0AEC+Ixwn98fgmxf/h5Xv/AeXL6bLXCEREREREcmNod1HaWMDpN61nMuRnr+rbXsMe+hBqOw252sqDT7+979x8ezpEj9LExqKiKefQr0N61FzzmwYmzeXXgsqAIZsE7Hw33Y8+5MdUefz8fnfn6Pvj30Z3suhTtNG6DGhPYJ1p6XnChxhuHCmLpbP2IMfXv0Ix/bsL/NzrqRl4Oc5n+Onmf9BbpapEismIiIiIqKqxNDuo1Q6NbTR/gAAW0Y+HGab9FqD5i0xcvhwKbjb1Bp89p9PcfbE8VI/U9BqEdS/PxKWfoeE75ciaMAAQOO8vZzGAdxzWMSsxXa89KMdkRc8wzunzZcsLqkORnwwCi37WBDqdxyAAwBgEw3IuNoA6/9zFd9M+BS/fvItck05Hu89uisZP7z6b/w4bRfOHkvA+dN1sXLKMmRcSJPhSIiIiIiIqKIxtPsw6bp2EbCcz/V4rW7jJhg9ahTU14O7Xa3BF19+iQM7/yrXZxubNUPN999DvY0bEP7M01CHhEivtTsm4r3P7XhhhR0Rl5zhvc+PfTB3z1yG9xKo1Gp0GtwXI+Y8je6Ph6FG6FGoYZZezyqsgxN7o/DtpN/ww6sfYf1nS7Fk4ifY+NkVZFxtCJtolLbNtiTgl3c24syR0n8JQ0RERERE3k8jdwFUeXRxQcjb4RxxtZzLgaFuiMfrteslYcyT/4fP//MpbGoNHGoNlq9eg4yLF9Bz0JBy7UMbFYXICRMQ8cwzyFr2I65+/DFsly8DADoeFdH+qB3bGwn4sXM+PrN9hiVHl2BEwxEY1WQUQg2hFXq8vqJR+1Zo1L4V0s+ex7b//oLsCwHIszvXE7CIgci42gAZVwGghvQeDQoREnwGuTnRKHQEI9cWg9/mJ6PdoyY06dja4/OPJx/C/uVbUJhlhEptgUZvhyZABUOIHwKjQhHbIBGJTRpCpVZX4VETEREREVFxGNp9WHGL0d0otnYCnhk3Dp999G8UCGpApcLW5INIv3QJw58eC5WqfJMxVAYDwv4xEiEPDUHW99/jyif/gf3KFagAdD4iovMRO3Y0EPBjJ1d4H9loJMbcNQYBuoCKOFyfE1W7Fgb/60k47HZs/2kdzm0/jaycRNihk7Yxqq4hLC4Tnf7RH5Fx/XFsz378+flR5NtrIN8Rjm1fnUdeZjZa9+qCrT+uwYWdacjMrwOggWtHBQCyAJwH8DdwYGM6/FSH4Bd4FcGJwWjSowPikupU7cETEREREREAhnafpokwQjCoIRbaYTmXA1EUIQjCTdtFRMfg+Vf+hU/nzsEVs/N+68fTr2DBzLfw1IQXYfDzK/c+VQYDwh57DCEPP4zM75bi6n/+A/s152J07VNEtE+xY089Z3j/1PYpVhxfgQmtJ+CBug9AJfBqjeKo1Gp0frAf8CCQcSENO75bg8KrhQhODEPXfwyE3uiaGl+/dXP4BQfhtw//QI61FixiIPatKsTh//1wfbS+Xrn2me+IQH52BK4kAyeTTyNAsx2BUbnoMfZhBIeHVc6BEhERERHRTZiSfJigEqCr5Rxtd+RYYM+2lLitwWjE2EmT0SA2ChBFAMA1ix1zZ76DtPPnbnnfKqMR4Y+PRr0N6xH5yiRoarimcrc+IeLtr+z413d2+J+9gtf/fB3/WPMPHLh84Jb3U91E1ozG/S8+gYffHoveTw7zCOxFatVLxH2v9UOI4RQAwAaDNL0ecI7Ox8adwMCXEjHon3XQYYgGDTpcQ+16pxEVmYIQfSrU8Pyu5NpicOlCEn58fSPWfboUjuu3DSQiIiIiosrFkXYfp4sLhPlEFgDAcs4ETUiNErdVqVQY/tT/w28/r8SWXXsBlQqFKjU++fhj9OrWFR179Lrl/av8/BA+ejRChw9H1rJluPrpZ7BdugQAaJEqotnndmxsLmBplwMYeWUkHqj7AF5o/QIijBG3dbzkFB5VA4PfHIZVU/+Lq7n1AQAh+lRENQ9ElxEDoTPopW1r1k246f25WSbsW78F6YfOo+CaATmWmhChRoEjHMd3A5cPfoFmD9+Fpne3L7OWrCtXcfHEaWRezEDtJvUR16BuhR0nEREREZGvY2j3cdIK8nBe1+7XtOTQXqT7/YMQHVsLy1auhOP6AnW/btmKE0ePYvjTz0Cj0d5yHSq9HmEjRyL04YeR9dNPuLroY1gvXIBKBHolO695/7GTCmtsP2HTuU14ue3LGFh3YLHT+al8/AIC8PA7/4e/fl6P0NhINO4wptzvDQgJwj0PDwAedj4+smMv9n23B5kFzsCdZU7EH/814egv/4Yh3AhboRV2sx0OK+CwAnarBjabH8z2IFhF/+ufqsf+dScREbIObUZ2Q52mjW7peLKvXsOlU2cRGhWBqNq1bum9RERERERKxdDu4zwWoztX/GJ0xWncug3G1qyJLz5ehFxRAAQBJy9fxQdvzsBjTz6FqJq3F5oEnQ6hDz+M4IEDkfnVV7jy0SI48vLgZwYe3eRAr33AFz2z8brldaxNXYs3Or6BmICY29oXAWqtxnk9/B1q1L4VGrRpjk3/XYmzO23It9eACDUyrjYErpb/c0RocDmrAX5deBbhYZvR/tGeiG+UJL2emXEFh7fuwuVj52HOssFu1sBqNcJiD4JFLFqw8Br8VNthMF6DPgwIS4xG/Y4tEJsYf0fH6LDbuWI+EREREXkdhnYfpw7QQR2qhz3TDOuFXIh2BwR1+ZYyiIiOwcTXpuC7/3yMY5fSAUFAHlT4eNEi9OxyDzr06FXu1eVvpNLpEP5//4fgQYNwed58ZP3wAyCKiM4CXlnmwMbmIr7ssRWDMgbhxTYv4qH6D3GhOpmp1Gr0GDUEuQNNWLfgW2Scj/dYyf5GalhgUGdBo8mDRmeBSgNkZcXBKvrBAS0uX2uAXz48ibDQDXDYAXN+MHJtkQD0AEqfQp/vCEd+XjiQB1w6BxzachJB2t8REFOIBr3boXGbFuU+roxzF7BpwSpkmeIRHHAW3ScMQmTN6HK/n4iIiIioMjG0VwO6uEAUZJohWh2wpuVDV7P8t1hTqdUY8cxY7Pp9E37ZsFGaLr/uz+3YtOUP1K0dh259+iK69u2NcmoiIhAzfRpCRwxH+jszkb9jBwCgx34RTc7YMf/+PMywzcDa02sxrdM0xAXG3dZ+qOIEhAThwdeexqlDR3Fkww5ALUDvZ4A+0A/G4ED4hwYhLKoGatSMuWnkOuPcBWz57GdcTa8Nm2iAHTpczmxQwp6cVLDCoM6CVpMHjdYCm1WHPEsUbKLnInwmay2YzgIXP72G3V9+Bf+oAtTr3hbNOrYs9jILh92ODYuX4exeHcwOZw1Xc+pj9Vt/oGGfIHQa2OcOzxQRERER0Z1jaK8GdHFBKDhwBYBzivythPYibbvei4T6DfDlJx87p8sDsKo1OHrhEo5+9jkCVMBdTZqgXZeuCIuMKuPTbmZo2BC1v1iM7B9/RNrb70DMz0d0FjDjv3as6Chg2d07MWTVELzY+kUMbTCU17p7gTpNGqJOk4a39J7IuJp4aOozuJh6BtsWr8GVjATY4VwUT4ANgdo06APy4B/jh+hGiYipF4/o2rVuCv9WixXH9uzH2eQU5F4woTDbHyar65KNHGst5JwH0r7KQvKSb+BfIxcJnZug1b2doFKrceboCWz7ZCOu5SfhRgWOcCT/Ykda8iIMmPS4x6J9lSnj3AUc3LgdOWmZiLkrEe0H9KyS/RIRERGRd2NorwZuuq69w+1dI14jJhYTX5uC/y1dgkMpx1AoXA9SgoBcEfjr78P46+/DUNltMGrUCA0KRHR0DOo1aoz6zf5/e3ceH1d5H/r/c9bZR6slWd6NjY0xBmODY5ZLCDs0JISkaUpTkpvbvEigJeE2a5utTZutWdo0IU1ukt7fTVJSUiAJgVC2sAQwi8GY2NjGuyxLsrbZZ872/P44o7FkS7ZsjCXZ3/frdXxGM2fOPDOPLOn7nO/zfc48bCq9pmnUv/OdxM89l86Pf4LSSy+hK7j+KcVZ23y+fW2RL675Ig/teoi/O+/vZK77FNY+bw7v/LsP0fHadjY+8Tx1rU0suWA1yXTq8E8GLNvi9NUrOX31ytp9W15cz4YHniG7xybr7s/IyHvt5PdC9y9c1t/9X8STfWSyM3HV/oC9Mb6FBZefyqb7t5GpzENhsHfvqfznx3/GiveehR2x6d66m0xnL6X+Am4+wHVd7l7zAwxbx4gYGBETK2YTb0yTbm2meWYbrbNnYtkjCze6jkumf4Dsvj62Pv8Kg1t7KWWS5NzpQCPQyJ4dsPWR77P0umUsu/BNo34G2zds4g8PPI0Zt/kfN1xLPHnkg3FCCCGEEGLyk6D9JGC3J0DXIFA4u7Ov61y6YfDWP30vbwW2vbqBJx56kF3d+/CN/d9KgWFSUFDI5OnIbOH5TVtI/vIe3vHudzN/8ZLDt3f2bOb85P/R94MfsO873wXP45Qu+OqPfP7fxTr/veJprvvVdXz8nI9z3YLr5Kr7FDZzwTxmLph3TM61cPkZLFx+BgA71m9g/X1PkdljkHH2T90o+s0UM/uXE4zoGeascLjkff8L3TA46y0XcO/Xfsze3WHgnnHm8MgPB6pH20D7yBctjtWaErAdjdeI6HkMrYyvIngqiqeiw45rqm4HGygu4Imf5tl47+2c82dhtf3erh7W/OK3DL7mMFieC8wGoHPdb5k2P8vFH3zXmAMfge9TKVeIJeJjNVoIIYQQQkxCErSfBDTLwGpP4Hbk8faVCMoeevT1d/38xUuYv3gJge/zwpOP89LzzzOQy1HyFeqAdOa80vj/fnYH81qaeNf7P0A8ceirgppp0vyhD5G44EI6P/5xnO3bsT34wIMBK7dofPeP8nzuqc/x0M6H+MJ5X2Ba/PBL2YmTx9wzljD3jHCAqGPzVtb95nEyOz0Gy3NQ1R97zXWbuPiWa2mZNaP2PMu2uO5vPsia+x5mw2/2UfRf3/eVwqAc1AF1hzkyIGV1EqsrYCdN+nY1UAoaAZ3ezCL++zs7SUWfJFeehc/BKzeUgkZ2vdbIzz/5IM3zBrjkL96FFbF4+bE1dK57jWK3T6HUSkUlSZrdRBNZku1xZq9YzJJVKzAsk0qpxK5N2+jZtotsVx9OtkTD/OmsfvsVB2ULCCGEEEKI40eC9pOEPSuF25EHFabIRxc2HLNz64bBORddzDkXXQxAEAT0dO5h6x9eYdfOHby2e094JV7X2d47wNe//GX+x+pVXHT1Ww977tgZS5l313/R809fZ+CnPwXgzB2Kr/8fnx9cofMET/COX72Dz63+HJfOkTnA4mAzTz2FmaeG1eh7OrtYd/9jpKc3s+rqD435nFVXX8KCFft45Nu/oJKNYVgVzFiAXWeTnFZH3YwWNr22mfmz5uAWylTyRdySg1us4BZc/DL4joHvRXD9BJ6KYGoVDN1B1x103UM3fMx4QNPCFpZecj4tM/Z//+YHszx0+8/Zt6sNRyXwsRksj6yonzB6SLcMUs5oDFTn5peDejq21nPHpx7Fx6pe1Z8/4nl5bzr5zHR6M7BjY4Gnf/pbTK1MOahDYRBW7w8zCjp2wJbf/Zp0YzdzLziNFZddOOayeIHvs+e1HezZvI3Bjh6KfXm8QoAZ10nPaGDGGQtZcObSEQMATrnC1vUb6NywjVxnP0bEZOmV5zFvyaGLEwohhBBCnEwkaD9J2LNSFJ7eCxz7oP1Auq7TNnMWbTNncT5QyGb5zx//kJ39g6Bp+IbJo8++wPPPv8Cb3vQmVl9y2SHXx9ZjMdo+87ckL76YvZ/+NF5PD8kyfPSXAedsUfzw8gE++ruP8vYFb+eT536ShJV4w96bmNpa2tu47APvHtexTa3TeNcXRw/sXddlT3GAlVe8Gcs69lehk/Vp3v6pv6BnTxeP/9vd9PbMw8cmomWpa+hkzgWLWXnFu2r/b9Y+9ASb7t9AfyEM3ivq4BR5WysQMQbJe621bAMAV8Vx1dgp8+WgnnJvPT33BPzh13eSaBiAQOG7OoFr4Ps2XhCl4tdVlwA0gGH1JgZg7x7Y9OwAj/EQCasHwyrjOkmKXjMBFtBQ3WDX5j3U2U+Rmumz7I8ulABeCCGEECc9CdpPEvasA4rRHUeJdJr33/pRtqxfz93/dSdFwoJ0uQAefOoZHnniSeZNb+Ut17yV9jlzxzxP8oLzmf+rX9L1d39H9r77AbhgQ7g03Pev0rmHe3iu6zm+dOGXWN6y/Hi8NSHeUC0z2moF+zo2bGH5ZVcQicUOOu7sSy/k7Esv5MVHn+LVe9eTKczB1vPE470k2m1mrziNpef9DwzLJDcwyPrH19CzYRel3oBiqRmlDCJmBtMuhxkFaQsrZpPZUWKwMLcaWEPBb6HQ23LU78cnEhYJdA99XMaZQ2YbdPxLGMBbsTKaAboBmqGhmRq6oYOqPkFVb6j951AAWrhphM8xIxZG1MKKRjAjFrnuvezdvouZp8wdc+BwX2c3W59fR//OLgIvQLN0DMNANw0008CORZi//HRmLTpl1OcLIYQQQrxeErSfJMzmGFrMRJU8nN05lFLHvYDbwjPO4K9PP53/vutOnl23nqBavM43TF7r6eO1H/2YlKFxzjkrueDyq0b9I9qor2fGN75B8i2X0PV3f0eQzdJQgE/8IuCxpYp/v7SD9/32fbz/9Pdz05k3ETWjB51DiKlmvAX7ll98HssvPo/A9wFG/T+UaqgP16B/2/heu2dPF8/91wMMbvUYrMwBDlwFIsDWCthGAcvKY8Y8ImmLREsd6bZmMnv3kd0zgJPRqJTTFPxpgI6OS9zchx3JY6ch0ZKi2Jsn150k7+2/Up9x5oAzvrYeuVP59cadmNqrxIx+LLuImQjCaUR5k7LTSDloIPxVeXAtgSGv/G4nUf0lYtFeoo3QOL+NRGMdA7u7KOzL4eY83JKJ6yXQUOi6G25GgG4GGDa0nDHriOsHKKXYtWU729e+Qqkvy4Lzl7PwrNNf/8cihBBCiElFgvaThKZp2LNSVDYPEORd/IEKZuPxD2h1XefKd76bt7z1bTzx2/t5ad1L5HxA00DTyAXwyJrnefypZ1h66gIuv+564smDU33r/uga4uecQ9dnP0v+sccAuOgVxRk7fL5/peKH6oc8sOMB/vZNf8v5M84/zu9SiIl1qOkmR6plRhvX/NWNAOzcuIWdL79KNJUgPa2RxrZpNLW1Yljj/1WSGxhkX8deZi1aMGaA+spTz7H54bXkuhPkvfZRjzmWPBUj580Aj0OsCHBo5aCBcrEBirC3AyAAWqrb4XU/Aq/97pekm3tZfPlyll6wasTje/d0sXXtK/Rt7aC8r4iXtym5zVSCNJAEkmxb382z0adpWhTj/D+5hlRD/dG9GSGEEEJMKhK0n0SGgnaA3GO7abhu4WGe8Qa2JRLlkrddxyVvu46O7Vt59L772NHVXVs6zjNMXtq6g3Vf+SpzW5q58h3X0zpj5JUuq7WFmd+7nczd99D9pS8R5HI05uGTvwh4/HTF/3vLbm566CaumnsVHz/34zTHmkdrihBinOactpA5p72+nxuphvrDBpNLzzuHpeedA4QDBdm+fpxSBbdUxqk4eGUH3/XC1HdNr6bBa+zPAlBoqGrWvEIFCt9x8csunuMROD6+41MuOOheHMdNUvIba9MAhthajpjdhx2vEG2yMaIWyg0IfB/lKwI/wC/5VHIWhUrbIWsDAJhaGY1gzONKQSOlnka6f1LgxZ//XyLJAm7Rpuw2VK/46wwt8zeWwfJ8BtfBrpefpK5uN9NOb0U3DHw/IAiqm68IfA/l+ijPR3kBgR+gvAC/EhC4isDVCHydwDdRykDXPTQ9QNMDdF2hmQrdBN3U0G0DwzYxIiZmxApfwwvPHVTPDWBGLaxYBCsRJZqME0unSDfV09TeSqq+7pgONgkhhBAnEgnaTyLx5S3kH+9AuQGFNV1E5tURP+vo56ceKzPnncJ7b/5LPM/lyQfu59nnnq/Ne1eGwfa+AW7//veptwyWLVvGeZddSbQ6r1fTNOrfcR2J889j72c+Q+HxJwD4H39QrNzic+eFOr/17+PJPU9y69m38s5T34mhyx+GQkwVr3eQYCyu63Lfffdx9dVXY1kWruOy69XNdG7ZAcDcZacxY/6ccQeSvuuxee3L7HzxVXIdGQIP7LRBvDlF4+w2Zi5eQOus9jCAdj0G+wfI9w+SGxikc8M2ejdkGCzOrRUJzLqzYODQrxnRs8TtXuyUi25qZHvqKfjhz3RXxekdXETv74/6IzoOStVtLwYOll7A0kvoRnU+hALQUUpDoaFrHnayTLI9xYwzFnDaqrPRjAOna4SrEgz2DdAwrUmWKxRCCHFCkKD9JGI1x6i/9hQG/msLAAN3bcFqT2K1HPrq0PFimhZvvuZa3nzNtbzy3BoefehB+spuNXVeZ9BTPL52HU88t5bWdIJz3nQey8+/AF3XsVpbmfVv/0bmrrvo/urXCDIZ4g7c+HDAW9bBjy7P8EX3i/zolR/x7sXv5h0L3kF9tH6i37IQYpKwbItTlp3OKcuObk64YZmctupsTlt19riObWqdRlPrNIBaVsGerTt44a6Hyewyw6C9KjLsin+syaZpbisLzjmDaXPmjDhv4Ps8c+9D7H5qG4PZOdUl/6YGHxs/sMOMAu8QB/bDvn7Y/kqJp//jEZJ2F+Byx0M/wwuiuEFiWCZDQETPY+l5TLOMYXroVjUDQ4EKQCkNlIZuBphxjUg6SmJamoaZ02mZ044KFJViKdxKFdxyGcM0WLhi2SEzRlzH5dVn17L31R1oGpixCHY0QiQZJxqPYUQs/IqLU67gOS6e6+I7Lg3TW1iyeoVkHQghhBhBgvaTTHxlK5XtGYpre1BOQN9PN9Jy81no9uT6A2HpOatYes4qOnft4IG772Z3b1+tcJ0yDLoKZX798CP85r//m7hp0FCXpq1tOnNOOYU5d/0X+e/9G4O/+AUoxaxe+NzPAp46TfHzC/fwzcI3+e5L3+WqeVfxnsXvYUnTkgl+t0IIATNOmcuMj30AgI1r1pLp7mXussW0zT10SvwQ3TBqRQYHe/t4+j/vp7gvj6Zp1bHPcK/rGpqhoxs6mmmgWQa6ZWCYJnYqRqIuRaK+jnRTA/UtzdiRCMVCkfxglmI+TzlXoJQvUimWcIslnEIZr1zBr3gEjgfVc+umjm6GlfaVUnglpzpFISBwAwIXAlcn8Ex8P4IbxHCC5EHTFDR8NIKD7veJhIUKx/5EqATpcN7/oQYChmSAvcAmCDMAto5xoMfTv3iehLGPSDRDpEEjPbMJt1gmvydHJReh4LThEQWmHfBcH8iPck6ruuVZ87Nfkkh0k56TZPHF5zD/9MVAOCjTvbuTru27GdzbQyVbBF1D03V0XUczNNCNsF+1MBMt7PNwH0slSTbU0dDaTGNbq2QhCCHEFCJB+0lG0zTq374ApyOP11PE6y4y+KutNL7z1Ilu2qjaZ8/l/bd+FM91eeaRh3jxhRfoKzughymRgWGSV5AfzLF7MMdzr24GpUgYcOZff5T5DzyI+/J6AM7bqDhvo88LCzTuPbfEPd7d3PPaPSxrXsYVc6/gsjmXMT05/VDNEUKI42I8V+wPpb65ias+/GfHqDWQqkuRqju4KOixFvg+xVwB3dQxTQvTMmtXnfd1dvPqU8/Tt6WTUp9PudRA0Q+DYg0fWyti6kVMo4KmuwS+jefHqQQpfOxj3FKdgt9KodAKhaHig8dGOainnKun7xXY/koncWMdKI1yUDds4CJd3Y6ED/RXt81YWhFLK2KaRUyzghEJMGI6dsrGTkQxbQvDtjAjNlbMxjBNst19FLoHqWQqeCUNtxIlUDaRaJZIo0HzwuksOm8lLTPaRm2BCgKCSgVjlKUrhRBCjE2C9pOQbhs03bCYnn99CeUGFJ/vJjKvjsSK1olu2phMy+KCK67igiuuIjc4wBMP/JYNr75KwQtQB6YRahoFNJ7avYc1i05l7tIlnP7wo8S6ewBY8ZpixWuK7a3wm3N0fr9kHS/3vszXnv8ay5qXcdmcy7h0zqXMTI29xJMQQohjTzcMkvWjB6PT2luZ9s5rRtzX3bGHxx57nGuvezvR+OiBYOD7DPT207Org1Imh2FZmLaNYRpYEQvTssn09tG3u4vivkHKmTJeQeE7Bmig6wHoCk0H3YDAA6cYp+C2jjkYEDd6iUb7iTRq6LqO74aF/gJPoXwNFYCmj9zQwcka5MozR0xtGBqYONZcFQ+nEjiEW5HD1FHwgfrqNlI+3wZ56NwFLz+8gYTxO0yziApM/MAiUBaeiuCpCAqT7zbkcXUrLCYJEBj84yuPkYpZpKImqWi4Tw/djpgj7reG1zLQRr1ZW9bWNnRSUZO66rmTERNzlFoIQggxmUnQfpKyWhPUv2MhAz/fBMDgPa+hJyzM+giapaPZxv69fvj13IOyR3ljP9b0BFZb4g1te6q+gavf/R6uBoIgoKtjF9s3bmT3rl309vUyWCjhDVsDfmsAWy+6iCavwpwtW5mz4VVs32deN9xyb8CfPwzPnarx7Kkar3hhAP/1F75OXaSO+kg99ZF6GiIN1EfraYo2sbR5KctbltMUa3pD36cQQohDa2xtIZKMH3LZQd0wRtQQGN2RFzx0HZcta1+m45XXyHUOoJs6jfPbWHze2bTOfssRn29IqVDkpYefZO+67RT3WeScGVhaGdvIYlplDNvDimuYcQtUuDpCOEdfoVR4GwVKKaoLKKCUCqcjOEOrAoQ1BFw/TkUd6RX74QIMvIMGLwp+SxjjjyHmlymo4QPuGt25Ct25yutoy/glbIOYbWAbOrapEzGN6l4fsbdNg4ipY+oagQrrISio7hXpqEVrOkprOkJLKtw3JGz8QFF2fSpeUNv7gcLQtXDTwr1paCQjJk2JCLFJNk1RCDG5SNB+Ekssb8HZlqHwXBfKDej79z8cdIxm6STOaSN18SyM1MFXFFSgKL7YQ+b+7QR5FwB7Tprk6unEljajmW/saLau67TPnkv77Lm1+wLf56mH/ps1a54hF2hDB9Jnx+g7fSlrT1tCvJBjWuce5u3YzbSBQS5Zp7hknaJswYunaDy3UGP93EF2JjPsZOeorz03PZflLcs5u/VsFjUsImkliVtxElaCiBGpjfILIYQ48Vi2xZI3rWDJm1Yc0/PGEnFWX3s5XHtMTzumSqlE96499O7uJNPdR7Evi1d2w+X6PIXywoEB5YMR04g2xkm3NdE6byazFi/EtCxefXYtu9dtprAnRyUfJ++04WOj4WNqZQzNwdAcdN1F1zyWtLbQZ6UJVJgyP5DNgRUlX/YoOIeI9o+RguMfl9c5ElFLpykRoTFhUx+30DUNfag2AWFNXtvUSUct6mIW6epWF7NIVzMJ9n9tYZs6Jcenv+gwUHDoLzgMFB38QNGSitKSjtCaipKOma/775Vc2WXT3gz9x2fMRYiTkgTtJ7n6a+fj7M7hdhVGfVy5AfmnOik830Xy/BmkLpqJHg2/bZzOPIO/3IqzMzviOc7OLP07s+iJbSTOaSN+dguaoRFUfFTFr+3RNYyUhZG00VM2emT0UWblB6AY9wCAbhi1VPqtG/7Aw/f/hr2ZHGpoqTddp5iqY+eiOnYuWoLuOlilItFikWQhTzqb47rnMtz4SAFXL7OrJUyl39amsaNVoz8FvqGxI7uDHdkd3P3a3Qe1wdRM4lachmgDTdEmmmJNNEWbaIw0sqeyh0RHgpZkS+3+qDl1qjwLIYQ4cURiMWYvWsDsRQuO+hxnXLCKMy5YVfvadVwq5TLxRHzUSvjDqy3sX37xIizLwg8U+bJHtuySK3vkyi7Z6j5X9shXPPxAAeEV7yHVvIIR9yug4vnkyh7ZUvj8ofOWXR/HC6h4QXXvEww73/FWdgP2DJbYM1g6JuezDA3XP/wbipg6LekIlq6Hn4Uffh6OF+ArxfS6KLMb48xqjDOrIc7sxjiGDhv25ti4N8vGvVk6BobabHJX9zNcs6ydq5dOZ3bTyNWJerJl1u4a4MVdg+zLV2iM2zSnIjQlwn1zIkJT0qYpaRMxx8488PyAgaJL2fVJRy2SURNjHFmhQkxlErSf5DTLoPkDSyms2UtQ9FBuQOD6KCdAuT7OjizKDVBOQO7R3eSf2Uv6zTPxBisUntnLsN+RRBc34g2U8bqLAAQFl9zvdpP73e5xtkVHT1phKp8bhJvnQwBoYLXGseekseekicxJYzRGDzs6fMqS0zllyenkBgd45tFH2Lx5E335Yq0SPUBg2VQsm0q6ngyw58CT+D6653KK67Jom0PDQD/TenuwCp1kE4qBJAwmIRPXyMapbi6ZRIZdiQw7swdfqf/l478c8XXCStAQaSAdSZO206TsFGk7TTqSJm7GiZmx2hY348SsGCkrRdJOkrJTpOwUESMyrs9ZCCGEeCNZtnXU1ekNXaMublEXP/7V7T0/DFor7v7gteL5eIFCH3bFW9M0lILBokNPrkJ3tlzbDxZdLEMjYhpErTD1PmLqGLqGHyh8pQgChRco/ECRLbv05fdfCe8vOMdk8GA8ATtAxQvY3T/2QMHOviI7+4rjft31e7Ks35Ply/e/ytIZaS46dRq7+kus3TlwRAMSqajJtGQYxNfFbLJll/6CQ1++wmDJHTFgA5CKmKSrdQumpSLMbIgzqzHGrIY4MxtiTK+LkSm5dGXLdGfLdGfKdOfKFCp+tV6CSTpqkYpapGMmMxviLG5LEbVk2oKYHCRoFxgpm/Sloy+b4+ccso/sovBsF/gKVfLI3L9jxDFmdf336KkNKKVwtmfJP9NJ6ZU+juQ3j3ID/IExcqsUuF1F3K4ihTVdAOhJC3t2GntWEntmCntmCj02+rd0qr6By667nssI58FvWvciLz33LB2deyn6Aco4xH8FwyAwDIJIFDcBpYYmOucvBN8nlhukad8+lu/spm1fHzHHHfFU14C9DdDVqLG3EfY2aHQ3QMnWcExwTHBNcMw8nZE8Ha9jpNjSLdJ2mrpIXW0/dHtobn59dP/8/PpIPWk7LVf5hRBCCMA0dExDJ36sFxs4AkGgyDseKiCcR8/++gQVLyBTdMmWXTIll2ypuq9mEuz/2qVQ8UnHTBoTNg3x6paw0TVqAwz7hg04KAWWMWw+v6GjUHQOlslXxl4zMW4bLGpLcUpzgmc2ddBR2P93zCt7sryyJzvmcw8lzLLw2NY7eiboQcdXPHLVdr7alTuq1zyQZWgsbkuzbGYdZ86sZ0l7mnzFY2+mROdgmc7BEnszZQaK4UCLqtY9CJQa5WtVG2ioi1u0VqcotKTCeggNCRtDDweEhgaIhqZIoA3dDpftNHSNmQ0xpiUPPxXT9QPyZQ8vUHhBgOcrXD+sseD64X2uHw4glR2H17LQl6/Q1iBLQk42ErSLQzJSNg1vW0DqwplkH9pJ8cWe2tV1zdJJXTKb1AUzaqnrmqYRmV9HZH4dfs6h8FwXTkc+vIoeMdBsAz1qoEUMlK8Icg5+3q3uHYKCC5oWFsAz9bAYnqWjKj5udyG86l4V5F3KG/oob+ir3Wc2x7BnpYgtaya6uHHUH2a6rnPa8hWctnz/PMTBvl46tm2lc/cuent6GMhkqFQcHN/H8wN8TQvT64efzzAo1TfRUd9Ex8JwHV3dqRAp5knk8tQNZmjv7mZmbz+ze4cGL8YexPA16K2DnjqNnnrorg/3JRsCHQKtuukang75WHhVvxgFpWm4gUtfuY++ct+YrzGaiBEJr+pXr+zXR+ppijXRHGumOdpMc6yZplgTdZE6uaovhBBCvIF0XSMdHTtgmlF/fJfLU0oxUHTZ1V9kV3+R3f1FHC9gUVuK06anmdMYR9e16jSHnZy+6s3896v7uH99F+v3ZGrniVo6y2bWc/bsBs6eXc/c5gQDBYfevENfoUJv3qE3X6EvX6GvdtupBeIQFhBsTNo0JSI0J20illGbSpGtDl5kSi6OF4z2Vo6Y6yvW78mwfk+Gn67ZdUzOeSzVxy1ObU1xamuSRa0ppqWidAwU2dFXYGdfuN8zUDrCzA2Tb//hMZqTNovb0ixqS7G4LcX8aYlaLYRDTV3wA0VfocK2fYXqlmdbb4HtvQUMXWPZzDrOmlXPmTPrWTw9dchziZEkaBfjYjZGafzjRaQumknu8T1opkbq4tmY9WMHb0bKJv2W2cesDUHFx+nI4ezM4uzMUtmZQ5VHjv56vSW83hLFF3uILmqg/tpTMJsO/wuuvqmZ+qZmlp6zisqODNmHd2EkbaKLGogsbMBIWAS+z+5tW1n37Bp27NzBYKkyIs0eILAjlOwIpfomemfB1jPCQD7V38f0rr3M39lBqlQetQ2GgtZBaB1UhLXvxvdTNtA1igmTfEJnMAE9yYDulEdvWqM3DX1pjWIEPB18A7zqpqoDEBW/wr7SPvaV9o3r9QBs3SZlp0hYiWqaoCJQQW1OoW3YtCfaaU+G24zkDNqT7TTHmqmP1BM344ccHfYDn0AFWIaM9AohhBATRdM0GhM2jQmbs2bVH/b4OU1xPvzmBXz4zQvY1Vfk5T2DzGlMsHh6auRSfeNUdn2yJZdU1BpXhf2hQYbd/UV2DxTpGCixu79Id7ZCfdyiNR2hLR2lJR2lNR0lFTXJV6/qh/UOXAaLLpu787zcMchr+/IHpeKPRT/girh2wNe6Fq5CkCuPnblwJAaLLs9u7+fZ7f3H5HzD9eYdnnytlydf6z3osfq4RUsqwrRUBNdTtUGTXNkbMcgymtd68ty1NpyIahs6p01PkYyatToKjq9wPB8/UJiGjmXoWIZ2wP7g2y2pYbUXGsPpECdanQMJ2sURsVoTNL7r1Al5bT1iED2lnugp9UBYud7rK+HszoXF9DryOJ15qM7jKm8aoOubL5C6aBbpN89EG8e8pMIL3QzctaV2juKLPaCBPStFdFEjbQvbaL/s+rCwnqbY8domXln/Il37usmWilQU+wveVQV2hExbO5m2dl4982zMSolk4JMMAuo8nwbPY1rZIdHfh797D0HuyNK69ECRzLkkc9AGLK49cujfMoEGgaHh6xqeoXB18HRFMQq7mzV2tWjsnAa7WsLgf3iWgRM4h72qvz2zfczHTN2spewnrSQVv0LBLdS2sh8ObAzN9W+MNtIQbaAhGt6uFferFvJriDagoaEIBw+GNku3aIo1oWuyJq8QQghxPM1uih9UjO5IRS3jiOaVDx9kOHMcgwyHk694rO/I8HLHIFt68tTHLKbXx2ivi9b2TcnIEQWIFc+nN+/QU52a0JOrkKml2FenRQyl2BPug+p6g4FSVLyA7b0FNnfn6M6OXbI/FTGZ25ygKWlj6mFwaxo6lj605GD1vupjOoo/bN6KG29mU3ee/oIz6nkHi/sHNsYrZhn4gcLx92dBOH7Auo7MIZ519CxDY0Z9jDNm1vPt9yx/Q17jeJOgXUxZmq5hTYtjTYuTOLsVAOUFlP7QR+Y32/CzDniK3MO7KL7YQ/1b5xM7bfS11VWgyD64k9yjoxTNU+DsyuHsysGDI4vKxYFzWYieXEL9dfOJLmumq2MXOzZtYteOHXR0dpL3FejVoFHT8KJxBoFBoGP4+2mfTvSMpTQk4rQkk8yIxJnpe5iuB36ACnw818MPPHzHwcjkUAMDeAP9+P0D+P39KGf0H7Cj0RXonsJEMSJfIgez9ynO37g/6HdjFuWkTaACfAICFAHhbdfQ8E0Nz9BwLfAMjaIZkIn65GKQi2nkY5CLQV8qnNPvWB69pV56SweP4A43FMR35DsOedyhmLpJa7yVtkQb0xPTmZ6YTtyKV+cIVn8xEqCh1QoADq8LkLSS6Jpe2wzNIPDDQQEhhBBCnLiSEZPVpzSx+pTR/348GhHTYEZ97JhMdcgUXTb35NjcnaMv7zCzIcacpgRzm+I0JuwjWs7PdV3uc7dw9dUrMU2TffkKm7pybOrK0TFQoidXpidbobu6r1SnIURMvVYEMB21qI9bzG1KcMq0BPOnJZk/LUFbOorjB2zcm2Pd7kHW7R7kpY5Btu3bX7NA08Kr77ahYxgavh8G+Y4fjDvbofZefMWOviL1E1mg4hiToF2cUDRTJ37mNKKLG8k+sov8E3sgUPj9Zfr+7was6QniK1qJnzUNIxn+Rw4cn4E7N1Navz+ATKxqI3ZGM+VNA5Q39eP1HLriaZB36b9jE/HNA7S97RTaZ8/lvOpj5WKRF596kg2vrKe7bwBnjKJ3SjcoAaVihc5ihZfoA6XQVIBCo5ZrNSQew4xFsGZOJ2JZxKJRUrEoDdEoTYbFNN8n1j+A37OPoFREuS64Hsp1Ud6wvVe9v3qfNzAA7siCelbJxSqNvG/8Dv5Jm02bdDfqdNYFdKUDdE0nhkVMmUQDk6gy0RQUNIe8ViGnOdWCfVCMhHP5MwmNTLVav2uN/UvJCzz25PewJ3/QugCvi4bGN/7rGzREG2pZA0O3G6ONta8bIuF9MStWWwngwF+ifuBT9IoU3SIVv0JDtIGUnTqm7RVCCCHEiaUubnHO3EbOmdt4TM+raWHKeUsqyoULpx30uFKKfMXDMvRxZ0JETIOzZtWPmGZRcnwCpbBNHVPXxhxkCAvnBdVt5O2y69OVKddqLgyvvzCr8fVlekwmErSLE5IeMai/ah6JFa0M/vI1KlvD9Bt3b4HMvdvI3L+d6OJG4mdOI/d4B25HNcVHg7o/mk/yvHY0TSO6oAGumY/XX6a8uR+3s4DyApSvIFBhMb2CW1urvri2B2dnlsb3LMaeGQZd0Xic1ZdezupLL8d1XX55z90smT+f7r2d9HZ30z8wQL5YoOj6+AcG9JqG0sb4YajreOh4QMkLGMwX2ZsfuSyLFvhYMZtIKk7UtonFosTjCVKpFKm6NIZhous6um6g6Rq6YRCPxWi3o2i7dlPZtInK5s1UtmwhKJXCvK1qFdswhysIg//K2OlZo0lnPdJZWFi7JwCOfo6Xaxt4to5v6gSmjm8aBJZOxdbZl1J0JirsSTj0paE3HVbuNwIw/XAzgjDzIBuHvhSUI4cfmVYoBiuDDFYGj6itGlpt+T6FougWa9MBhktZKdqSbbQn2pmemE5jrJFABbi+ixuEmxd4xM0405NhBkF7Mjw2bafRNA3Hdyi4BfJunqJbxFc+rfFWGqOjF2kUQgghhDgcTdNIHaJg4niNp04BhBXzDX3sqRKnTU8fdJ+qTiU4UUjQLk5oVkuc5v91BqX1veSf3BOmuAP4ivIf+ij/Yf+cbM02aPzTxcQWHzxaaTZGSb6pfczXKaztZvCerSjHx+sr03P7Ououn0vywhloB8xzsuwIC5edyZIVKwlKHsWXeiiu7QkHA85Js7O0g53bt9Gzr4dsqYxS1Yvs7F8KBMJlPLyhqvZjULqBAzgB5MoOlB0YyAJ7D/3BKYUZ+CRsi4ali2m77C00NjcTTySIJZIk0mmS6TTxZApd11FKgesSOC7KqRAUi/gDg/iDA/iDg/gDg3gD/XidnTg7d+Hs2oXff+wKp1iOj+X4oz7WDpx5hOfzYjbFxjj5eptC3ECpIMx6CIJwGUM/IINDpl6nJ+bSE3MYSMJgUsM1QNUq/Yd7zwhXAVB6OO++6BUpeode9zbn5sgN5NgysOUIWw8xM4YXeLjB6NkRUSMaBvjJ6cxIzCAdSe8vJFidQwdhTYERSwjadSTtJJZuYeompm5i6RaWvv8Xd6CCcAJFdU5ewkpIQUEhhBBCHFeaph1RPYTJToJ2ccLTNI34smnEl03D7SlSeKGb4tpugtz+gMaoj9D8vtOx2hJH9RqJs1uJzE7Td8er4VV7X5G5fzuF57uIzK3Dnp3Cnp1C1VvhHPntWXIv9lJc3wvDRwF/VWD+glbOfutqrNbR26K8AD/rYNTZaIZOsZCnd+9e+rq76O3pZl9PD5lMhnyxRNkP8A9cqm48NA3PMMn4ikzfIDv6Bkc/rpq+ryuFDpi6hmkYRCyTeDxOKpmirqGexrkzaVq1kqaWNpJ1dei6jp/L4ezahdfdDbqOZlkEhsFAIU9fLkvFcYiZJnHTJmFZxHUd3fXwM4P4/QM4fb2U+vqoDA7i5HIkCkW0SgXlOCjHIXDdg9L8x8ssOaT3OKSPbUY9lYhBOapTioYV/V1bB9NAM0w000SzLLBMMrZPj1lir5knEwvIxaBiadiuIupC1KlubjhloDcN++rCgoHliEbJO/R0jrJfZltmG9sy247tGxxD3IxTF6mjPlJfWzpQ13Q0wlS4oX3UiIaDA9UBgqEBA626pOHQQIQXeAQqIGJEiBpRIma4j5pRYmaMpJ0kYSYwDjGgJYQQQggxVUjQLk4qVkuc+qvmUXf5XMpbBii91AOaRt3V8zBSr69Yhdkco+WmM8OCdo93gAJvXwlvX4nCc10AaBGDpaqOgWc2jHmeymuDdP/zWpKr20lfOgc9ZuL1lihvGaC8eYDK1gzK8cHUsNoS2DOSNM1I0TZ3OtaqOJo5slK657r093Qz0LuPgb4+soMD5LJZisUiQbD/iujQViqVyJbKOOj7C+iNpZq+7wM+4AIEhEt+VLLhVf3dB0S+SqEFPgbVIF/XcXwfT0FwuAGGwEejulydpkNTU7gBBAEWAQnboi6VpnlaM02NjZhlF6NYwMzn0QcG0fv7yHsefRoMaho5DQqajqNpRHyPVKlEfSZDY0cnTT37sP1jl1oVqfhEKj51r7NYqmOa7Gxvo6u1BdP3mPtCB629/ehAKW4ymNbxLaNaOE9H18LbStcoWAFZ06XfKJO3fEoR8HUtHHgJwoyOoX0xooX1A+KQjYe3czFwLHANxj0YNJRZsLdwmAyPYyxuxknayVoxwaHig4EKv+/zhTz/597/M2IAQdd0bN0mYSdImIlwAMBKkLAStVUIVLUizlBGgqEZIzIPDM3A0A0MzagVL9Q1HUM3SFkpGqONNMYaaYw2EjOP75rLQgghhJh6JGgXJyXN0Igtbhw1Ff51ndfUqbtqHpGF9WQf2ImzJxdO165SFZ8I+6/+aTGTxPIW4ue04feVGPzNNvyBCgSQ/30nxZf2oUUM/P5R1nb3FG5HHrcjT4FwUABDw2yOYbXGsVoTWK1xzNY4DWYD6ViCmanp+EEFXzkElhcuZbekCav54MDB81w6tm5l+5bNdHXuoVgs4roerufiej6+7+MFAb4K32KgjSPIhzDQN0w8qrPYFaCP80eRboy9kJ2u46Iz6CkGBzLsHMgAWw8+LjZ2UZIykEmk6WhuhVNOBaXQfQ8dQAOd6jQFwPc8bNPAUAo9CND9AN33wq+VwggUhgo3ggCv+nl5SoWDHLpGUA2klaajNK22Wa5LpFwiVioTLxVJFkr4hs7etlYGGpupJFMjPutdp56GUSlT39vDrN17OGV3B46h6J7WRF9jA4N1dRRTKTzTIlYokM5kaO3v56yeXlKFInr1U/U1jWI0QjEapRyNYPo+0YpDtFIhWnHC9zKMZ+l4lo5r6aBVl2HUTRzLxjUsHMsiMMAzAiqGS9F0KdkeJcvHN218wybQbQLDQukWnq4oWUUykTwD8TKFOBSi4beIGYSDCUawvwaBp4dTD4Zvjgm+EQ4mDA0W9NAzZp/3ZcdetvB1CyDhREiXY6QqceJuFF/zKVkOJatCwS7jJTTqkvUjMhGG9hEjQtkrU/ErlL0yZb9MxauE++p9Fb9CxS1j5yBFCqs9SUOkoXaelJ0iZsaImtFaVkLMjGFoBhrh5zQ828HQjBGDDoZuoKOPWgPB0i2iZpSIEcE8xP/hoZUWZOlFIYQQ4uhI0C7EGyC6oIHoggYCx8fdk68uGZelsiuLn3OIzK8juWo6sSXNaFb1D9npCaKLGsg9vofc73aj3ICg4EJhZIq3nrCwZ6Xwekt4vQekQfsKr7uI112kxKGXUwMore8lc992zGkxoqc1EVvSiD0rDUGA7sLMtnnMaJqDcgM0PRyU0Cy9tscc+ce8UymTHRhgYF8vffu6GejrJTM4SD6fo1As43gurh8G+r6mwVD6slLogY+lQcQyiUWiWJaF6zo4rofreeEAQRAGjZoGenV+/9Ac/7Ln4x3NVIBD0TQC02LUa+2GxdHW0z+cQye3j86PROmbMZu+GbN56dxgzAEUN5Ei29JGB7AO0DwX3fcIDBNlmIf+/HwfLRildoAWDjxgvL50dBNIVbeZnkLvdzEcB1C17Aql7x/g0KBWHHHotqYCtCAI90pB4IMKUweGVmFQWnXoZei2ph38WLggYLVl1Vva0D9DKzns/6wO/NSUpoNphRkhBzpwZcbAJ/CKlLUcGWsnr8VforOun4JdIepZpCoxEpUICSdKzIuScOPU+XGmqXoMLYpmRMLClM4gv1QPv44eOHqmZhIxI0SMCF7g7d+UVwvaDc3ANmws3cI2bGzdDgcFhpZURA8LYzL6965t2DTHmpkWm8a0+DRa4i00WA1sdbfy+87fg05YpNHfX6hxqA3Dp1YMFXIcflzEiFAfrQ9Xe6ju6yJ1+IFfGySpeBUqflh0sy5SF2ZMRBupj9aPqOsAYTaGF3g4gVPLzBhO13SiZlQGMoQQQoyLBO1CvIF02yAyr47IvDqgugbmb+7j6mtWY1kHF+fSLIP0JbOJr2ghc992Si/3gqERmZMmcmoD0YUNWNMTteJ2QdnD7czj7Ak3d28hDOT9I1vQ0ttXIr+vg/zjR7geuh5OObBmprBnpbBnpmhqa6W5bToLOeOwT3cqZfKZDOnGRkzz0MXKgoqH05HHSFpjzvf3XJeObVvp2LGdrs495LI5PM/D88PMAD/w8f0A0zRIp1I0NDQwrbWN6bNn09zaRueunezetpXuvXvpGxggX67gqaFgb3/Qd0wHBo6C6Xs0JOPMnTOXQiHPzt0dFNSwdo0n46FKmRb+YT77GsNAvc7AfNw0jcCyCayxp60c4bKtb6jX1RbdQLdTxEkRp53pRVhWpDrYMEpf6tXtADZRbrvLpxCFfAzyQ7UTzHA6g2NWb5sQHOJ7WFMKTVWHJ1RYULFialRsKFtQqW6eAb5OWANDeXiuR8EtjHleX/mUvBKloxqWOrQf/+7Hx/ycRyJlp7B0C8d3wi04cGTmYEMrScStOAkrQdyMEzEiWIZVK/Y4NN3CCZww26KacVH2yriBS8SI7N+qtR00NFwVDkb4gV8bvBia/jE0HWRo6oapm7XMClM3MTUTTdNGFKZUqNpAhKe8EQMeCkXKTtVqUaQjaersOqJmdETdCtif1XHVvKuwjRNn/WQhhHijSdAuxPE2jnjPrI/S9KenEVzvh2nZYyyJoUdNIvPricyvr92n/ACvt4TbXcTtLuL1FNFMHaM+glFnY6QjGHUR0DUqmwcobewLl6w7mqgjALeriNtVpPh8d7Xx4Vx7szmG1RzDbI5hNoV7PTbyR44didLYEh311MpXOHtyVLYMUt4yEFb+r15pt6YniK9oJX7WNIzk/j/8TMti7qLFzF20+CjeDCyqb2DRsrMOe1ypVOS+e3/DhRecT+C6lEslKpUylXIZt+KEGQJOBc9xcF2XwA+wIjaRSJRIJIIdjRKJRrEjEQzDxLQtTMPEtMP3UstU6O0lkwkzFXxfMXvObM489020z5l7UJsyA/08+7tH2fTqqwwWS5i6RjoeZ1pzMzPnzGHe4tOob2xi28YNbH9tM3s79zKQzVDyApQWzmk3NbAMg4hlYdsWvh/geiMzHcaa4a8BhqZhGjqWoWOZFpZlohThVArfI/B8PN9HBQGGpmHpGqamY+kalq7jez5Fp0I5CHAIiyEGQ8sgKhVeOVfB/ivsUL1KTm3Aolbv4EgHVqpX7KtRyrCr6uwfqDngGA21vx0jPglABRiug+U6WJUKkUqFSLlMoOs4dgTXtvAsG8+y8OwIvh05uM3juQrr+5hOBatcIl4o8KZNw9tzfIY1fF3DNyDQNXw9DPSNIAz89QD06v/bwNBqx/rVqQ1Uj9X9cG8E4fPLdlhToRiFYlSjGNEo2EFt4MExwTHD5RthaPlGhTlsKccRW/V+XYUrOiht2CoPWlinIR+FfEwL91EoRsP2mUPTMqrngf2DF2Vbo2xDxcoSaBBXkFTh6+jV96KrcD/8ttLANRWOWcA1C/Sb++gyw89kqG3Dv68hHEzRh00T0dg/LUSN8v2uKYXlgeWB7YWDL54efvb+IZ53PFw8+2IJ2oUQ4ghI0C7EJKZHjvyqpmbo1fnsh6+Eb09PkLpoJn7Bpbypn/Kr/fgDlTAFflgavGYZqECFy9K5QW0fFF28fcUR8/aHz7U/6HqaoR2UYq+Z+v6BjGExhjdQRpVHX8bN3Vsgc+82MvdtJ7q4kcTZLZjNsfBcw9P3AeX4KCdAOT6B46McH03X0KImesQI91EDzRj/1WnTtDBtm4bmaaNmTLxe481UGK6uoZHLrrueyw5z3JIVK1myYuXRN+44C4IA/QgyB4Z4rku5VMQplyllc2godE3D0HV0wyDwA554/HEuveQSotFoOM0iCFBBgCqX8TOZcLnCTAZ/MIOfyYSp9jX7A3rNCiv/a6YJQ3tAeR64Lqq6BY6DKpYIigWCQgG/UED1Z/BzOXID/XSaBvuaGhlsqKeQSuMbJqbrYLouluOEgb/jkCgUqMvmaMhkSZTK4UV3XYdgYtajHQq2DzdIoHsKc5wDCVYJUqX90xNG90YMSkym/I3wR6vSwDhMs4bXd9BUGKRbo//4HMGvDlg45rCt+iPN8sKBDsvff3u0ED/QwkGWshUubVmOaJStcHAFhg1AsP9r3lYBidmFEGLcTpig/Tvf+Q5f+9rX6Orq4swzz+Tb3/4255577kQ3S4gpwUhYJM5uJXF26xE/N3D8MEW/I4/TkcPtyOP1lUb/29dXKN9HVcbx1+QBzKYokVPqcfcWcHbnqi+uKG/oo7zh9RcT0yw9DOBj1S1qoMWqgf3wAQZTR+mKph6byuYBVF0MPWljJKz99QlGoarruyu/OvjhVefn62GFu3Afbpqp16ZAHHQe18fPOvg5Bz/nhvP7kxZ6wsJI2mhR46CiYbXXVtX3OcHp/UfiaAJ2CLMuklYdpOug5eDva9d1MerribW2viEDL0djmevi9fXh7evF27ePoFREj8XR4zH0WAwtGkOPRdEiUTTbQrNsdNuCavtVsYifzYbbYAY/myHIF8JlECsVlFMhqFRQ5UqYen8ApVT4vTFszj+aFs65L5UJSkVUqVS9XQrP63kozwXXC2/7Ppoe1jfQDGP/Hg4+1vPCQQ/TDI+xTLRqXYWgUCDI5fBzOfCP/OfFiUKHcY0jmEE1C+AIC20YCmJOuL0eicrwrw7fYJvjNM1GCCFOECdE0P7zn/+c2267je9973usWrWKb33rW1xxxRVs2rSJlpaWiW6eECc03TaIzK0jMreudp9yA7z+Uq1YntdbxusrEZS98Er9sKv1yjsgeBialh01icyvI7qggciCeszG/Wn0bneBwtoeimt7CHKv86/NYW1WrjPu880lyeDWTSObbhvhX9mKaip1NXs6ULXU/vHSLB3NDrMcNFsHBX7ORZW9Qz/R0NDjZvjaXvXz9dTIx2PDBifiVjhff6g/3ADl+ihPhRkJB2Vd6FgzUqQvnnVE70ccnmZZWG1tWG1tR/f8RAI9kcCaPv0Yt2ziKKXCwYhcjqBQQFUqBOVwAEJVKriFAmtffJEVq1ZhRmNhxoNlhdkPth0OCAzdZ5phscTqig4qCMK9H4TZD9ns/syKbJYglwVND89lmmCaaNX6D0GpSFAsVjMnimHbVICmG2Dow/Z6WHBT19CGVtjQNQhU+F4qZVR5/175XvXnRQDVZTgJwsKS4SCIjmaY+wdChjI5HKe2R9fRolF020aLRtEiNnokEmZLuQ54Hsr19meAHNiGcrhaiWbb4RaJhHvLqg0mjiiu5/nhZ1D9HA6eMnIwy5TL7EIIcSROiKD9G9/4Bn/xF3/B+9//fgC+973v8Zvf/IYf/ehHfPKTn5zg1glx8tGs8afoHw2rNUH9VfOou3wu5dcGwivezlCwGV7FVq4fzju2w6BXsw1020Czq6n+ZY+g4of7sh8OKJQ9glKYQn80jvZ5o56rGkBXF8YbP18R5A5xuc1XBHmXIH/0te/DdknQLt54mqbVBiNG47ouedclcdFFkyZj4mSmlEKVy+EghusOqwMxtAtv6/Gxl94UQghxsCkftDuOwwsvvMCnPvWp2n26rnPppZfy9NNPj/qcSqVCpbI/lyubzQLhL3/XfaMWcXr9hto2mdsoDk368Ngz56cw56eO6TmVr1CVMJhXZR/lB1AbDAjwKi4bX3yFU2ctQCsHYRBcCDegmlZMNdWY8CqbodXm9GPoaGY1pSBQYaZyEP5xq3wFw694O9XgXakwDT5loyctjHS4BwgKXu31g7xLUPTClHtz5PJ8AKrkhQMUpTGmKVSvrmNq4XSGA6/UA8rQpvz3sPxfPDFIP05Cpgl1dQfdPXxSjuf7tWkP0ocnBunHqU/6cGKM9/PW1GgLiE4hnZ2dzJgxg6eeeorVq1fX7v/4xz/OY489xpo1aw56zuc//3m+8IUvHHT/z372M+Iy+iuEOFkEYPphlahAVwRjLCWGAi0APdDQg7CQlGdP6V8dQgghhBATrlgs8qd/+qdkMhnS6fSYx035K+1H41Of+hS33XZb7etsNsusWbO4/PLLD/lhTTTXdXnwwQe57LLLJA1wipI+PDFIP0590ocnBunHqU/68MQg/Tj1SR9OjKGM78OZ8kF7c3MzhmHQ3d094v7u7m7axijmE4lEiEQiB91vWdaU+CadKu0UY5M+PDFIP0590ocnBunHqU/68MQg/Tj1SR8eX+P9rI9uLZ1JxLZtVqxYwcMPP1y7LwgCHn744RHp8kIIIYQQQgghxFQz5a+0A9x2223ceOONrFy5knPPPZdvfetbFAqFWjV5IYQQQgghhBBiKjohgvZ3v/vd7Nu3j89+9rN0dXVx1lln8dvf/pbW1taJbpoQQgghhBBCCHHUToigHeCWW27hlltumehmCCGEEEIIIYQQx8yUn9MuhBBCCCGEEEKcqCRoF0IIIYQQQgghJikJ2oUQQgghhBBCiElKgnYhhBBCCCGEEGKSkqBdCCGEEEIIIYSYpCRoF0IIIYQQQgghJikJ2oUQQgghhBBCiElKgnYhhBBCCCGEEGKSkqBdCCGEEEIIIYSYpCRoF0IIIYQQQgghJikJ2oUQQgghhBBCiElKgnYhhBBCCCGEEGKSkqBdCCGEEEIIIYSYpCRoF0IIIYQQQgghJikJ2oUQQgghhBBCiElKgnYhhBBCCCGEEGKSkqBdCCGEEEIIIYSYpCRoF0IIIYQQQgghJikJ2oUQQgghhBBCiEnKnOgGTAZKKQCy2ewEt+TQXNelWCySzWaxLGuimyOOgvThiUH6ceqTPjwxSD9OfdKHJwbpx6lP+nBiDMWfQ/HoWCRoB3K5HACzZs2a4JYIIYQQQgghhDiZ5HI56urqxnxcU4cL608CQRDQ2dlJKpVC07SJbs6Ystkss2bNYvfu3aTT6YlujjgK0ocnBunHqU/68MQg/Tj1SR+eGKQfpz7pw4mhlCKXy9He3o6ujz1zXa60A7quM3PmzIluxril02n5zzTFSR+eGKQfpz7pwxOD9OPUJ314YpB+nPqkD4+/Q11hHyKF6IQQQgghhBBCiElKgnYhhBBCCCGEEGKSkqB9ColEInzuc58jEolMdFPEUZI+PDFIP0590ocnBunHqU/68MQg/Tj1SR9OblKITgghhBBCCCGEmKTkSrsQQgghhBBCCDFJSdAuhBBCCCGEEEJMUhK0CyGEEEIIIYQQk5QE7UIIIYQQQgghxCQlQfsU8Z3vfIe5c+cSjUZZtWoVzz777EQ3SRzCl770Jc455xxSqRQtLS28/e1vZ9OmTSOOKZfL3HzzzTQ1NZFMJrn++uvp7u6eoBaLw/nyl7+Mpml85CMfqd0nfTg17Nmzhz/7sz+jqamJWCzGGWecwfPPP197XCnFZz/7WaZPn04sFuPSSy9ly5YtE9hiMZzv+3zmM59h3rx5xGIxTjnlFP7+7/+e4XV0pQ8nn8cff5y3vvWttLe3o2ka99xzz4jHx9Nn/f393HDDDaTTaerr6/nABz5APp8/ju/i5HaoPnRdl0984hOcccYZJBIJ2tvb+fM//3M6OztHnEP6cOId7v/icDfddBOapvGtb31rxP3SjxNPgvYp4Oc//zm33XYbn/vc51i7di1nnnkmV1xxBT09PRPdNDGGxx57jJtvvplnnnmGBx98ENd1ufzyyykUCrVjPvrRj/LrX/+aO++8k8cee4zOzk7e8Y53TGCrxViee+45/u3f/o1ly5aNuF/6cPIbGBjg/PPPx7Is7r//fjZs2MDXv/51Ghoaasd89atf5V/+5V/43ve+x5o1a0gkElxxxRWUy+UJbLkY8pWvfIXbb7+df/3Xf2Xjxo185Stf4atf/Srf/va3a8dIH04+hUKBM888k+985zujPj6ePrvhhhv4wx/+wIMPPsi9997L448/zgc/+MHj9RZOeofqw2KxyNq1a/nMZz7D2rVrueuuu9i0aRPXXnvtiOOkDyfe4f4vDrn77rt55plnaG9vP+gx6cdJQIlJ79xzz1U333xz7Wvf91V7e7v60pe+NIGtEkeip6dHAeqxxx5TSik1ODioLMtSd955Z+2YjRs3KkA9/fTTE9VMMYpcLqcWLlyoHnzwQXXRRRepW2+9VSklfThVfOITn1AXXHDBmI8HQaDa2trU1772tdp9g4ODKhKJqP/4j/84Hk0Uh3HNNdeo//k//+eI+97xjneoG264QSklfTgVAOruu++ufT2ePtuwYYMC1HPPPVc75v7771eapqk9e/Yct7aL0IF9OJpnn31WAWrnzp1KKenDyWisfuzo6FAzZsxQr7zyipozZ4765je/WXtM+nFykCvtk5zjOLzwwgtceumltft0XefSSy/l6aefnsCWiSORyWQAaGxsBOCFF17Add0R/bp48WJmz54t/TrJ3HzzzVxzzTUj+gqkD6eKX/3qV6xcuZJ3vetdtLS0sHz5cn7wgx/UHt++fTtdXV0j+rGuro5Vq1ZJP04S5513Hg8//DCbN28GYN26dTz55JNcddVVgPThVDSePnv66aepr69n5cqVtWMuvfRSdF1nzZo1x73N4vAymQyaplFfXw9IH04VQRDw3ve+l4997GOcfvrpBz0u/Tg5mBPdAHFovb29+L5Pa2vriPtbW1t59dVXJ6hV4kgEQcBHPvIRzj//fJYuXQpAV1cXtm3XfrENaW1tpaurawJaKUZzxx13sHbtWp577rmDHpM+nBq2bdvG7bffzm233canP/1pnnvuOf7qr/4K27a58cYba3012s9Y6cfJ4ZOf/CTZbJbFixdjGAa+7/MP//AP3HDDDQDSh1PQePqsq6uLlpaWEY+bpkljY6P06yRULpf5xCc+wXve8x7S6TQgfThVfOUrX8E0Tf7qr/5q1MelHycHCdqFeIPdfPPNvPLKKzz55JMT3RRxBHbv3s2tt97Kgw8+SDQanejmiKMUBAErV67kH//xHwFYvnw5r7zyCt/73ve48cYbJ7h1Yjz+8z//k5/+9Kf87Gc/4/TTT+ell17iIx/5CO3t7dKHQkwCruvyx3/8xyiluP322ye6OeIIvPDCC/zzP/8za9euRdO0iW6OOARJj5/kmpubMQzjoIrU3d3dtLW1TVCrxHjdcsst3HvvvTz66KPMnDmzdn9bWxuO4zA4ODjieOnXyeOFF16gp6eHs88+G9M0MU2Txx57jH/5l3/BNE1aW1ulD6eA6dOns2TJkhH3nXbaaezatQug1lfyM3by+tjHPsYnP/lJ/uRP/oQzzjiD9773vXz0ox/lS1/6EiB9OBWNp8/a2toOKrjreR79/f3Sr5PIUMC+c+dOHnzwwdpVdpA+nAqeeOIJenp6mD17du1vnZ07d/K///f/Zu7cuYD042QhQfskZ9s2K1as4OGHH67dFwQBDz/8MKtXr57AlolDUUpxyy23cPfdd/PII48wb968EY+vWLECy7JG9OumTZvYtWuX9Oskcckll7B+/Xpeeuml2rZy5UpuuOGG2m3pw8nv/PPPP2i5xc2bNzNnzhwA5s2bR1tb24h+zGazrFmzRvpxkigWi+j6yD9XDMMgCAJA+nAqGk+frV69msHBQV544YXaMY888ghBELBq1arj3mZxsKGAfcuWLTz00EM0NTWNeFz6cPJ773vfy8svvzzib5329nY+9rGP8cADDwDSj5PGRFfCE4d3xx13qEgkov793/9dbdiwQX3wgx9U9fX1qqura6KbJsbwoQ99SNXV1anf/e53au/evbWtWCzWjrnpppvU7Nmz1SOPPKKef/55tXr1arV69eoJbLU4nOHV45WSPpwKnn32WWWapvqHf/gHtWXLFvXTn/5UxeNx9ZOf/KR2zJe//GVVX1+vfvnLX6qXX35Zve1tb1Pz5s1TpVJpAlsuhtx4441qxowZ6t5771Xbt29Xd911l2publYf//jHa8dIH04+uVxOvfjii+rFF19UgPrGN76hXnzxxVpl8fH02ZVXXqmWL1+u1qxZo5588km1cOFC9Z73vGei3tJJ51B96DiOuvbaa9XMmTPVSy+9NOJvnUqlUjuH9OHEO9z/xQMdWD1eKenHyUCC9ini29/+tpo9e7aybVude+656plnnpnoJolDAEbdfvzjH9eOKZVK6sMf/rBqaGhQ8XhcXXfddWrv3r0T12hxWAcG7dKHU8Ovf/1rtXTpUhWJRNTixYvV97///RGPB0GgPvOZz6jW1lYViUTUJZdcojZt2jRBrRUHymaz6tZbb1WzZ89W0WhUzZ8/X/3N3/zNiMBA+nDyefTRR0f9PXjjjTcqpcbXZ319feo973mPSiaTKp1Oq/e///0ql8tNwLs5OR2qD7dv3z7m3zqPPvpo7RzShxPvcP8XDzRa0C79OPE0pZQ6Hlf0hRBCCCGEEEIIcWRkTrsQQgghhBBCCDFJSdAuhBBCCCGEEEJMUhK0CyGEEEIIIYQQk5QE7UIIIYQQQgghxCQlQbsQQgghhBBCCDFJSdAuhBBCCCGEEEJMUhK0CyGEEEIIIYQQk5QE7UIIIYQQQgghxCQlQbsQQggh3nCapnHPPfdMdDOEEEKIKUeCdiGEEOIE9773vQ9N0w7arrzyyolumhBCCCEOw5zoBgghhBDijXfllVfy4x//eMR9kUhkglojhBBCiPGSK+1CCCHESSASidDW1jZia2hoAMLU9dtvv52rrrqKWCzG/Pnz+cUvfjHi+evXr+ctb3kLsViMpqYmPvjBD5LP50cc86Mf/YjTTz+dSCTC9OnTueWWW0Y83tvby3XXXUc8HmfhwoX86le/qj02MDDADTfcwLRp04jFYixcuPCgQQYhhBDiZCRBuxBCCCH4zGc+w/XXX8+6deu44YYb+JM/+RM2btwIQKFQ4IorrqChoYHnnnuOO++8k4ceemhEUH777bdz880388EPfpD169fzq1/9igULFox4jS984Qv88R//MS+//DJXX301N9xwA/39/bXX37BhA/fffz8bN27k9ttvp7m5+fh9AEIIIcQkpSml1EQ3QgghhBBvnPe973385Cc/IRqNjrj/05/+NJ/+9KfRNI2bbrqJ22+/vfbYm970Js4++2y++93v8oMf/IBPfOIT7N69m0QiAcB9993HW9/6Vjo7O2ltbWXGjBm8//3v54tf/OKobdA0jb/927/l7//+74FwICCZTHL//fdz5ZVXcu2119Lc3MyPfvSjN+hTEEIIIaYmmdMuhBBCnAQuvvjiEUE5QGNjY+326tWrRzy2evVqXnrpJQA2btzImWeeWQvYAc4//3yCIGDTpk1omkZnZyeXXHLJIduwbNmy2u1EIkE6naanpweAD33oQ1x//fWsXbuWyy+/nLe//e2cd955R/VehRBCiBOJBO1CCCHESSCRSByUrn6sxGKxcR1nWdaIrzVNIwgCAK666ip27tzJfffdx4MPPsgll1zCzTffzD/90z8d8/YKIYQQU4nMaRdCCCEEzzzzzEFfn3baaQCcdtpprFu3jkKhUHv897//Pbqus2jRIlKpFHPnzuXhhx9+XW2YNm0aN954Iz/5yU/41re+xfe///3XdT4hhBDiRCBX2oUQQoiTQKVSoaura8R9pmnWir3deeedrFy5kgsuuICf/vSnPPvss/zwhz8E4IYbbuBzn/scN954I5///OfZt28ff/mXf8l73/teWltbAfj85z/PTTfdREtLC1dddRW5XI7f//73/OVf/uW42vfZz36WFStWcPrpp1OpVLj33ntrgwZCCCHEyUyCdiGEEOIk8Nvf/pbp06ePuG/RokW8+uqrQFjZ/Y477uDDH/4w06dP5z/+4z9YsmQJAPF4nAceeIBbb72Vc845h3g8zvXXX883vvGN2rluvPFGyuUy3/zmN/nrv/5rmpubeec73znu9tm2zac+9Sl27NhBLBbjwgsv5I477jgG71wIIYSY2qR6vBBCCHGS0zSNu+++m7e//e0T3RQhhBBCHEDmtAshhBBCCCGEEJOUBO1CCCGEEEIIIcQkJXPahRBCiJOczJQTQgghJi+50i6EEEIIIYQQQkxSErQLIYQQQgghhBCTlATtQgghhBBCCCHEJCVBuxBCCCGEEEIIMUlJ0C6EEEIIIYQQQkxSErQLIYQQQgghhBCTlATtQgghhBBCCCHEJCVBuxBCCCGEEEIIMUn9/5ykImZuY8aZAAAAAElFTkSuQmCC","text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA+0AAAK9CAYAAABRvo1QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIgElEQVR4nOzdfXzN9f/H8ee52tmFzUxsluuryFVFSkhlrirJRSh9I0r1JUmpfMtlF8q3C0Wl66sfla7UV5KlIiWJlApJQsk1m222nZ3z+f0x5zjHNrYZZ5/PHvfbzW07n8/nnPM+ex3jed5XNsMwDAEAAAAAgHLHHu4GAAAAAACAwhHaAQAAAAAopwjtAAAAAACUU4R2AAAAAADKKUI7AAAAAADlFKEdAAAAAIByitAOAAAAAEA5RWgHAAAAAKCcIrQDAAAAAFBOEdoBAKiA3njjDTVp0kQul0vx8fHhbs5JMWnSJNlstjJ9zCFDhqhu3bpl+pgAABwLoR0ASmHTpk266aabVL9+fUVGRiouLk7t27fXk08+qUOHDoW7eSiBL7/8Un369FFSUpIiIiJUvXp19ezZU++//364m3bSrF+/XkOGDFGDBg30wgsv6Pnnnz8lz/v111+rd+/eSkxMlNvtVt26dXXTTTdp69atpX7MrKwsTZo0SV9++WXZNdTkXn31VdlsNn3//fchx9PS0tS2bVtFRkZq4cKFYWpd6QwZMkQ2m+24f4YMGRLupgJAmXOGuwEAYDYff/yxrrrqKrndbl133XVq3ry5cnNztWzZMo0dO1a//PLLKQtBODETJ07UlClT1KhRI910002qU6eO9u7dqwULFqhv376aPXu2rrnmmnA3s8x9+eWX8vl8evLJJ9WwYcNT8pwzZszQbbfdpvr16+vWW29VjRo1tG7dOr344ot6++23tWDBAl1wwQUlftysrCxNnjxZknTRRReFnLvvvvt0zz33lEXzA1544QX5fL4yfcxTIT09XV27dtVPP/2kDz74QN27dw93k0rkpptuUkpKSuD25s2bNWHCBA0fPlwdO3YMHG/QoEE4mgcAJxWhHQBKYPPmzRo4cKDq1Kmjzz//XDVq1AicGzFihH7//Xd9/PHHYWzhicvOzlZERITsdmsPxnr33Xc1ZcoU9evXT3PmzJHL5QqcGzt2rD799FN5PJ4yea6srCxFR0eXyWOVhV27dklSmQ6LP9Zr/PrrrzV69Gh16NBBCxcuDLnulltuUfv27dWvXz/98ssvqlKlSpm1yel0yuks2//qBL9PToW8vDz5fD5FRESU+jEOHjyobt26ac2aNXr//ffVo0ePE27Xqf490a5dO7Vr1y5w+/vvv9eECRPUrl07XXvttaekDSVVFrUDAInh8QBQItOmTVNGRoZeeumlkMDu17BhQ912222B23l5ebr//vvVoEGDwHDg//znP8rJyQm5X926dXX55Zdr2bJlgeGr9evX1+uvvx645vvvv5fNZtNrr71W4Hk//fRT2Ww2zZ8/P3Ds77//1tChQwNDkZs1a6aXX3455H5ffvmlbDab3nrrLd133306/fTTFR0drfT0dEnSO++8ozPPPFORkZFq3ry5Pvjgg0Ln9Pp8Pk2fPl3NmjVTZGSkEhMTddNNN2n//v0lfp1+Bw4c0O233666devK7XarZs2auu6667Rnz57ANTk5OZo4caIaNmwot9utWrVq6a677irw8y3M+PHjlZCQoJdffrnQINatWzddfvnlko4MN/7zzz8L/fkFD82+6KKL1Lx5c61atUoXXnihoqOj9Z///EeXX3656tevX2hb2rVrpzZt2oQc+7//+z+1bt1aUVFRSkhI0MCBA7Vt27aQazZu3Ki+ffsqKSlJkZGRqlmzpgYOHKi0tLQiX3fdunU1ceJESVK1atVks9k0adKkwPlnnnlGzZo1k9vtVnJyskaMGKEDBw6EPEZRr7Eo999/f+C9e3Swb9CggaZNm6Z//vlHzz33XOD4kCFDVKlSJf3xxx/q1q2bYmJilJycrClTpsgwDEnSn3/+qWrVqkmSJk+eHBgi7X89hc1pt9lsGjlyZOC9HRUVpXbt2mnt2rWSpOeee04NGzZUZGSkLrroogI1P/r9f9FFFxU5VPvVV18NXHfgwAGNHj1atWrVktvtVsOGDfXII4+E9Nr/+eefstlsevTRRzV9+vTA741ff/1VUv60hpJOJcjIyFD37t21evVqvffee7rssstCzp/o74l9+/bpzjvvVIsWLVSpUiXFxcWpR48e+vHHHwu0ZcaMGWrWrJmio6NVpUoVtWnTRnPmzCnR6zmWotYwKOzvb0l/F51o7QDgRNDTDgAl8L///U/169cv9jDeG264Qa+99pr69eunO+64QytWrNDUqVO1bt06ffDBByHX/v777+rXr5+GDRumwYMH6+WXX9aQIUPUunVrNWvWTG3atFH9+vU1d+5cDR48OOS+b7/9tqpUqaJu3bpJknbu3Knzzz8/EFCqVaumTz75RMOGDVN6erpGjx4dcv/7779fERERuvPOO5WTk6OIiAh9/PHHGjBggFq0aKGpU6dq//79GjZsmE4//fQCr/Omm27Sq6++quuvv16jRo3S5s2bNXPmTP3www/6+uuvQ0Lx8V6nlB80OnbsqHXr1mno0KE655xztGfPHn300Uf666+/dNppp8nn8+mKK67QsmXLNHz4cDVt2lRr167VE088od9++03z5s0rsi4bN27U+vXrNXToUMXGxharliWxd+9e9ejRQwMHDtS1116rxMREtW7dWtddd51Wrlypc889N3Dtli1b9O233+q///1v4NiDDz6o8ePHq3///rrhhhu0e/duzZgxQxdeeKF++OEHxcfHKzc3V926dVNOTo5uvfVWJSUl6e+//9b8+fN14MABVa5cudC2TZ8+Xa+//ro++OADPfvss6pUqZJatmwpKT/0TJ48WSkpKbrlllu0YcMGPfvss1q5cmWBOhb2GguTlZWlxYsXq2PHjqpXr16h1wwYMEDDhw/X/PnzQ4aze71ede/eXeeff76mTZumhQsXauLEicrLy9OUKVNUrVo1Pfvss7rlllvUu3dv9enTR5ICr6coX331lT766CONGDFCkjR16lRdfvnluuuuu/TMM8/o3//+t/bv369p06Zp6NCh+vzzz4t8rHvvvVc33HBDyLH/+7//06effqrq1asHfgadOnXS33//rZtuukm1a9fWN998o3Hjxumff/7R9OnTQ+7/yiuvKDs7W8OHD5fb7VZCQoIkqWnTpurUqVOx5+9nZmaqR48eWrlypd59993Ah1B+ZfF74tdff9W8efN01VVXqV69etq5c6eee+45derUSb/++quSk5Ml5U8rGDVqlPr166fbbrtN2dnZ+umnn7RixYqwTUEpzu+isqodAJwQAwBQLGlpaYYko1evXsW6fs2aNYYk44Ybbgg5fueddxqSjM8//zxwrE6dOoYkY+nSpYFju3btMtxut3HHHXcEjo0bN85wuVzGvn37AsdycnKM+Ph4Y+jQoYFjw4YNM2rUqGHs2bMn5LkHDhxoVK5c2cjKyjIMwzC++OILQ5JRv379wDG/Fi1aGDVr1jQOHjwYOPbll18akow6deoEjn311VeGJGP27Nkh91+4cGGB48V9nRMmTDAkGe+//75xNJ/PZxiGYbzxxhuG3W43vvrqq5Dzs2bNMiQZX3/9dYH7+n344YeGJOOJJ54o8ppgr7zyiiHJ2Lx5c8hx/8/viy++CBzr1KmTIcmYNWtWyLVpaWkFXqdhGMa0adMMm81mbNmyxTAMw/jzzz8Nh8NhPPjggyHXrV271nA6nYHjP/zwgyHJeOedd4r1GoJNnDjRkGTs3r07cGzXrl1GRESE0bVrV8Pr9QaOz5w505BkvPzyy8d9jYXx/z247bbbjnldy5YtjYSEhMDtwYMHG5KMW2+9NXDM5/MZl112mRERERFo++7duw1JxsSJE4t8ncEkGW63O6SWzz33nCHJSEpKMtLT0wPHx40bV6DugwcPDnn/H+3rr782XC5XyN/H+++/34iJiTF+++23kGvvuecew+FwGFu3bjUMwzA2b95sSDLi4uKMXbt2FXhsSUanTp2KfG4///u1Tp06hsvlMubNm1fodWXxeyI7Ozvk/eJ/HW6325gyZUrgWK9evYxmzZodt+3FtXLlSkOS8corrwSOFVZvwyj8729xfxeVVe0A4EQwPB4Aisk/ZLy4PbMLFiyQJI0ZMybk+B133CFJBea+n3nmmSELKlWrVk1nnHGG/vjjj8CxAQMGyOPxhKxsvmjRIh04cEADBgyQJBmGoffee089e/aUYRjas2dP4E+3bt2Ulpam1atXhzz34MGDFRUVFbi9fft2rV27Vtddd50qVaoUON6pUye1aNEi5L7vvPOOKleurC5duoQ8V+vWrVWpUiV98cUXJX6d7733nlq1aqXevXsX+Ln6h7++8847atq0qZo0aRLyvJdccokkFXjeYCWtZUm53W5df/31Icf8w4bnzp0bGN4t5Y+SOP/881W7dm1J0vvvvy+fz6f+/fuHvK6kpCQ1atQo8Lr8PemffvqpsrKyTrjNn332mXJzczV69OiQeco33nij4uLiCrxfC3uNhTl48KCk4/+sY2NjA3UJNnLkyMD3/h7h3NxcffbZZ8d97qJ07tw5ZIj7eeedJ0nq27dvSDv9x4Pfm8eyY8cO9evXT2eddZaeeeaZwPF33nlHHTt2VJUqVUJqmpKSIq/Xq6VLl4Y8Tt++fQPD/oMZhlGiVfJ37typyMhI1apVq9DHOtHfE1L++8D/fvF6vdq7d68qVaqkM844I+T+8fHx+uuvv7Ry5cpit/9kK87vorKqHQCcCIbHA0AxxcXFSToSQo5ny5YtstvtBVbnTkpKUnx8vLZs2RJy3B/aglWpUiVkXnirVq3UpEkTvf322xo2bJik/NB32mmnBcLq7t27deDAAT3//PNFrmLvX4jM7+hhy/62FbayeMOGDUP+M75x40alpaUFhgIf77mK8zo3bdqkvn37Fvp4wc+7bt26Iv+DfPTzBitpLUvq9NNPL3TxqQEDBmjevHlavny5LrjgAm3atEmrVq0KGWK7ceNGGYahRo0aFfrY/iHq9erV05gxY/T4449r9uzZ6tixo6644gpde+21RQ6NPxZ/zc8444yQ4xEREapfv36B92tRr/Fo/hB8vJ/1wYMHCwR7u91eYB2Axo0bS1KBueYlcfR70P/zOjrc+o8fvTZDYfLy8tS/f395vV69//77crvdgXMbN27UTz/9VOz3alHTCErqueee05gxY9S9e3d99dVXIbUti98TkgK7EDzzzDPavHmzvF5v4FzVqlUD399999367LPP1LZtWzVs2FBdu3bVNddco/bt25/oyyy14vwuClftACAYoR0AiikuLk7Jycn6+eefS3S/whZGKozD4Sj0eHCvrJQf/B588EHt2bNHsbGx+uijj3T11VcHVsn2L4507bXXFpj77nf0nN+je89KwufzqXr16po9e3ah54/+z25xX2dxnrdFixZ6/PHHCz1fWO+iX5MmTSQpsPjY8RRVw+CAEqyon2fPnj0VHR2tuXPn6oILLtDcuXNlt9t11VVXBa7x+Xyy2Wz65JNPCv1ZBY98eOyxxzRkyBB9+OGHWrRokUaNGqWpU6fq22+/Vc2aNYv12kqruO+Zhg0byul06qeffirympycHG3YsKHAYnwnS1HvwRN5b44dO1bLly/XZ599VuBn7/P51KVLF911112F3tf/QYTfifx9DHbmmWdqwYIF6ty5s7p06aKvv/468PeirH5PPPTQQxo/fryGDh2q+++/XwkJCbLb7Ro9enTIQm1NmzbVhg0bNH/+fC1cuFDvvfeennnmGU2YMCGwZd+JKunf0+LUO1y1A4BghHYAKIHLL79czz//vJYvXx6y/VBh6tSpI5/Pp40bN6pp06aB4zt37tSBAwdUp06dUrVhwIABmjx5st577z0lJiYqPT1dAwcODJyvVq2aYmNj5fV6Q/Y1Lgl/237//fcC544+1qBBA3322Wdq3759mf2HtUGDBsf9cKRBgwb68ccf1blz52J/MOLXuHFjnXHGGfrwww/15JNPhgThwvi3ITt6FfWje5+PJyYmRpdffrneeecdPf7443r77bfVsWPHwGJdUv7rMgxD9erVKxAICtOiRQu1aNFC9913n7755hu1b99es2bN0gMPPFCitvlrvmHDhpDe7dzcXG3evLnU76WYmBhdfPHF+vzzz7Vly5ZC3/dz585VTk5OgYXSfD6f/vjjj5Cfw2+//SZJgeHtJa39yfDWW29p+vTpmj59ujp16lTgfIMGDZSRkVHqn+GJaNu2rebNm6fLLrtMXbp00VdffaVq1aqVye8JKX/rxIsvvlgvvfRSyPEDBw7otNNOCzkWExOjAQMGaMCAAcrNzVWfPn304IMPaty4cYqMjCx1G/yC/54Gb2dY0r+nwcJZOwDwY047AJTAXXfdpZiYGN1www3auXNngfObNm3Sk08+KUm69NJLJanA6sL+nuGjt14qrqZNm6pFixZ6++239fbbb6tGjRq68MILA+cdDof69u2r9957r9Dgu3v37uM+R3Jyspo3b67XX39dGRkZgeNLliwp0DvtHxJ8//33F3icvLy8AkG3OPr27asff/yxwAr70pFesP79++vvv//WCy+8UOCaQ4cOKTMz85jPMXnyZO3du1c33HCD8vLyCpxftGhRYAu9Bg0aSFLI/FWv11vksOJjGTBggLZv364XX3xRP/74Y2AtAr8+ffrI4XBo8uTJBXp4DcPQ3r17JeXPyz+63S1atJDdbi/WlndHS0lJUUREhJ566qmQ533ppZeUlpZW6verJN13330yDENDhgzRoUOHQs5t3rxZd911l2rUqKGbbrqpwH1nzpwZ+N4wDM2cOVMul0udO3eWpMAWcqV5n5WFn3/+WTfccIOuvfbakO0eg/Xv31/Lly/Xp59+WuDcgQMHCn3/FaY0W75J+XP433zzTf3+++/q3r270tPTy+T3hJT/++bo9+k777yjv//+O+SY/33rFxERoTPPPFOGYcjj8ZTwFRWusL+nmZmZhW6TWVxlVTsAOBH0tANACTRo0EBz5szRgAED1LRpU1133XVq3ry5cnNz9c033+idd97RkCFDJOXPPx88eLCef/55HThwQJ06ddJ3332n1157TVdeeaUuvvjiUrdjwIABmjBhgiIjIzVs2LCQhcMk6eGHH9YXX3yh8847TzfeeKPOPPNM7du3T6tXr9Znn32mffv2Hfc5HnroIfXq1Uvt27fX9ddfr/3792vmzJlq3rx5SJDv1KmTbrrpJk2dOlVr1qxR165d5XK5tHHjRr3zzjt68skn1a9fvxK9vrFjx+rdd9/VVVddpaFDh6p169bat2+fPvroI82aNUutWrXSv/71L82dO1c333yzvvjiC7Vv315er1fr16/X3Llz9emnnx5zuPWAAQO0du1aPfjgg/rhhx909dVXq06dOtq7d68WLlyoxYsXB/aQbtasmc4//3yNGzdO+/btU0JCgt56661S/Yf90ksvVWxsrO68885AcArWoEEDPfDAAxo3bpz+/PNPXXnllYqNjdXmzZv1wQcfaPjw4brzzjv1+eefa+TIkbrqqqvUuHFj5eXl6Y033ij0MYujWrVqGjdunCZPnqzu3bvriiuu0IYNG/TMM8/o3HPP1bXXXlvix/S78MIL9eijj2rMmDFq2bKlhgwZoho1amj9+vV64YUX5PP5tGDBgkBPqV9kZKQWLlyowYMH67zzztMnn3yijz/+WP/5z38C0y6ioqJ05pln6u2331bjxo2VkJCg5s2bq3nz5qVub0n4F+O78MIL9X//938h5y644ALVr19fY8eO1UcffaTLL788sKVYZmam1q5dq3fffVd//vlngV7pwpR0y7dgvXv31gsvvKChQ4fqiiuu0MKFC8vk98Tll1+uKVOm6Prrr9cFF1ygtWvXavbs2QXWIujatauSkpLUvn17JSYmat26dZo5c6Yuu+yyMlsQsmvXrqpdu7aGDRumsWPHyuFw6OWXX1a1atVK9WGHpDKrHQCckFO7WD0AWMNvv/1m3HjjjUbdunWNiIgIIzY21mjfvr0xY8YMIzs7O3Cdx+MxJk+ebNSrV89wuVxGrVq1jHHjxoVcYxj52w9ddtllBZ6nU6dOhW7xtHHjRkOSIclYtmxZoW3cuXOnMWLECKNWrVqGy+UykpKSjM6dOxvPP/984Br/Vk5FbRv21ltvGU2aNDHcbrfRvHlz46OPPjL69u1rNGnSpMC1zz//vNG6dWsjKirKiI2NNVq0aGHcddddxvbt20v1Ovfu3WuMHDnSOP30042IiAijZs2axuDBg0O2p8rNzTUeeeQRo1mzZobb7TaqVKlitG7d2pg8ebKRlpZW6Gs62uLFi41evXoZ1atXN5xOp1GtWjWjZ8+exocffhhy3aZNm4yUlBTD7XYbiYmJxn/+8x8jNTW10C3fjre11aBBgwxJRkpKSpHXvPfee0aHDh2MmJgYIyYmxmjSpIkxYsQIY8OGDYZhGMYff/xhDB061GjQoIERGRlpJCQkGBdffLHx2WefHfc1F7blm9/MmTONJk2aGC6Xy0hMTDRuueUWY//+/SHXFOc1Fmbp0qVGr169jNNOO81wuVxG7dq1jRtvvNH4888/C1w7ePBgIyYmxti0aZPRtWtXIzo62khMTDQmTpxYYIuxb775xmjdurURERERsv1bUVu+jRgxIuSYf7uu//73vyHHC/v7cfSWb/6twwr7E7wd2cGDB41x48YZDRs2NCIiIozTTjvNuOCCC4xHH33UyM3NPWY7gtteki3fVq5cWeDco48+akgyLr/8csPj8Zzw74ns7GzjjjvuMGrUqGFERUUZ7du3N5YvX17g7/Rzzz1nXHjhhUbVqlUNt9ttNGjQwBg7dmyx/54erbAt3wzDMFatWmWcd955RkREhFG7dm3j8ccfL3LLt+L+LiqL2gHAibAZRglX/gEAVGhnnXWWqlWrptTU1HA3BRY2ZMgQvfvuuyGjOgAAqIiY0w4AKJTH4ykw/PvLL7/Ujz/+qIsuuig8jQIAAKhgmNMOACjU33//rZSUFF177bVKTk7W+vXrNWvWLCUlJenmm28Od/MAWITX6z3uwneVKlU67i4PAGBVhHYAQKGqVKmi1q1b68UXX9Tu3bsVExOjyy67TA8//LCqVq0a7uYBsIht27apXr16x7xm4sSJmjRp0qlpEACUM8xpBwAAQNhkZ2dr2bJlx7ymfv36BVakB4CKgtAOAAAAAEA5xUJ0AAAAAACUU8xpl+Tz+bR9+3bFxsbKZrOFuzkAAAAAAIszDEMHDx5UcnKy7Pai+9MJ7ZK2b9+uWrVqhbsZAAAAAIAKZtu2bapZs2aR5wntkmJjYyXl/7Di4uLC3JqieTweLVq0SF27dpXL5Qp3c1DGqK+1UV9ro77WRn2tjfpaF7W1NivUNz09XbVq1Qrk0aIQ2qXAkPi4uLhyH9qjo6MVFxdn2jcmikZ9rY36Whv1tTbqa23U17qorbVZqb7Hm6LNQnQAAAAAAJRThHYAAAAAAMopQjsAAAAAAOUUc9oBAECFYxiG8vLy5PV6j3utx+OR0+lUdnZ2sa6HuVBf66K21maG+jocDjmdzhPeVpzQDgAAKpTc3Fz9888/ysrKKtb1hmEoKSlJ27ZtO+H/eKH8ob7WRW2tzSz1jY6OVo0aNRQREVHqxyC0AwCACsPn82nz5s1yOBxKTk5WRETEcf+z5/P5lJGRoUqVKsluZ2ah1VBf66K21lbe62sYhnJzc7V7925t3rxZjRo1KnU7Ce0AAKDCyM3Nlc/nU61atRQdHV2s+/h8PuXm5ioyMrJc/scQJ4b6Whe1tTYz1DcqKkoul0tbtmwJtLU0yuerAwAAOInK63/wAADWUhb/3vAvFgAAAAAA5RShHQAAAACAcorQDgAAUEHVrVtX06dPD3czLOHPP/+UzWbTmjVrLPE8JfXll1/KZrPpwIEDxb5PSd5/NptN8+bNK1XbgJIoj3/HCO0AAADlnM1mO+afSZMmlepxV65cqeHDh59Q2y666CKNHj36hB7DbIYMGaIrr7wy5FitWrX0zz//qHnz5uFpFMqVV199VfHx8eFuBiyC1eMBAADKuX/++Sfw/dtvv60JEyZow4YNgWOVKlUKfG8Yhrxer5zO4/83r1q1amXb0ArM4XAoKSkp3M1AMeXm5p7QvtlmU9Fer9XQ0w4AAFDOJSUlBf5UrlxZNpstcHv9+vWKjY3VJ598otatW8vtdmvZsmXatGmTevXqpcTERFWqVEnnnnuuPvvss5DHPXp4ss1m04svvqjevXsrOjpajRo10kcffXRCbX/vvffUrFkzud1u1a1bV4899ljI+WeeeUaNGjVSZGSkEhMT1a9fv8C5d999Vy1atFBUVJSqVq2qlJQUZWZmnlB7tm7dql69eqlSpUqKi4vTgAEDtGvXrsD5SZMm6ayzztJzzz0X2Bqwf//+SktLC5x/7bXX9OGHHwZGOnz55ZcFhtT6h4t/+umnOvvssxUVFaVLLrlEu3bt0ieffKKmTZsqLi5O11xzjbKysgLPv3DhQnXo0EHx8fGqWrWqLr/8cm3atKlUr7W0bcjJydGoUaNUvXp1RUZGqkOHDlq5cmXIYy9YsECNGzdWVFSULr74Yv35558Fnn/ZsmXq2LGjoqKiVKtWLY0aNeqE6+c3ceJE1ahRQz/99FOxnqtu3bq6//77dd111ykuLi4wwuTuu+9W48aNFR0drfr162v8+PHyeDyB+/3444+6+OKLFRsbq7i4OLVu3Vrff//9Mdv25Zdf6vrrr1daWlqB0TCFDfOPj4/Xq6++KunI0Oz3339fF198saKjo9WqVSstX7485D6lfb1F8T/v3LlzA4977rnn6rffftPKlSvVpk0bVapUST169NDu3btDXmvbtm0VExOj+Ph4tW/fXlu2bAmc//DDD3XOOecoMjJS9evX1+TJk5WXl3fMtrz44otq2rSpIiMj1aRJEz3zzDMF2vnWW2+pQ4cOSkpKUsuWLbVkyZKQx1iyZInatm0rt9utGjVq6J577gl5Xp/Pp2nTpqlhw4Zyu92qXbu2HnzwwZDH+OOPP4qswZYtW9SzZ09VqVJFMTExatasmRYsWHDM13Ui6GkHAAAVXs8Zy7T7YE4RZw35DEN2m02SrUyft1qsW/+7tUOZPNY999yjRx99VPXr11eVKlW0bds2XXrppXrwwQfldrv1+uuvq2fPntqwYYNq165d5ONMnjxZ06ZN03//+1/NmDFDgwYN0pYtW5SQkFDiNq1atUr9+/fXpEmTNGDAAH3zzTf697//rapVq2rIkCH6/vvvNWrUKL3xxhu64IILtG/fPn311VeS8kcXXH311Zo2bZp69+6tgwcP6quvvpJhGKX+Gfl8vkBgX7JkifLy8jRixAgNHTpUS5cuDVz3+++/a+7cufrf//6n9PR0DRs2TP/+9781e/Zs3XnnnVq3bp3S09P1yiuvSJISEhK0ffv2Qp9z0qRJmjlzZiD89+/fX263W3PmzFFGRoZ69+6tGTNm6O6775YkZWZmasyYMWrZsqUyMjI0YcIE9e7dW2vWrCn11lElbcNdd92l9957T6+99prq1KmjadOmqVu3bvr999+VkJCgbdu2qU+fPhoxYoSGDx+u77//XnfccUfIc27atEndu3fXAw88oJdfflm7d+/WyJEjNXLkyMDPrTQMw9CoUaM0f/58ffXVV2rYsGGRz3XrrbeGfCj16KOPasKECZo4cWLgWGxsrF599VUlJydr7dq1uvHGGxUbG6u77rpLkjRo0CCdffbZevbZZ+VwOLRmzRq5XK5jtvGCCy7Q9OnTQ0bEBI+GKY57771Xjz76qBo1aqR7771XV199tX7//Xc5nc5i/2wLe73HM3HiRE2fPl21a9fW0KFDdc011yg2NlZPPvlk4P0zYcIEPfvss8rLy9OVV16pG2+8UW+++aZyc3P13XffyWbL/z351Vdf6brrrtNTTz2ljh07atOmTYEPD4pq0+zZszVhwgTNnDlTZ599tn744QfdeOONiomJ0eDBgwPXjR07Vo8//rhq166tF154QT179tTmzZtVtWpV/f3337r00ks1ZMgQvf7661q/fr1uvPFGRUZGBj48GTdunF544QU98cQT6tChg/755x+tX7++2DUYMWKEcnNztXTpUsXExOjXX38tcY1LxICRlpZmSDLS0tLC3ZRjys3NNebNm2fk5uaGuyk4CaivtVFfa6O+5nHo0CHj119/NQ4dOhRy/LwHPzPq3D3/lP8578HPSvwaXnnlFaNy5cqB21988YUhyZg3b95x79usWTNjxowZgdt16tQxnnjiicBtScZ9990XuJ2RkWFIMj755JMiH7NTp07GbbfdVui5a665xujSpUvIsbFjxxpnnnmmYRiG8d577xlxcXFGenp6gfuuWrXKkGT8+eefx31dxbVo0SLD4XAYW7duDRxbu3atIcn49ttvDcMwjIkTJxoOh8P466+/Atd88sknht1uN/755x/DMAxj8ODBRq9evUIee/PmzYYk44cffjAM40hdPvvsSI2nTp1qSDI2bdoUOHbTTTcZ3bp1K7LNu3fvNiQZa9euLfR5jqU0bcjIyDBcLpcxe/bswPnc3FwjOTnZmDZtmmEYhjFu3LhADf3uvvtuQ5Kxf/9+wzAMY9iwYcbw4cNDrvnqq68Mu90e+Pt39PvvWCQZ77zzjnHNNdcYTZs2DanPsZ7rn3/+Mbxer1GnTh3jyiuvPO7z/Pe//zVat24duB0bG2u8+uqrxWpjsKP/nga/jg8++CDkWOXKlY1XXnnFMIwj9X3xxRcD53/55RdDkrFu3TrDMIr/sy3O6/Ur7HnffPNNQ5KxePHiwLGpU6caZ5xxhmEYhrF3715DkvHll18W+pidO3c2HnrooZBjb7zxhlGjRo0i29GgQQNjzpw5Icfuv/9+o127diHtfPjhhw2v12vs37/fyMnJMWrWrGk88sgjhmEYxn/+8x/jjDPOMHw+X+Axnn76aaNSpUqG1+s10tPTDbfbbbzwwgvF/lkcXYMWLVoYkyZNKvJ1BCvq3x3DKH4OpacdAABUeNVi3cc4e3J72stKmzZtQm5nZGRo0qRJ+vjjj/XPP/8oLy9Phw4d0tatW4/5OC1btgx8HxMTo7i4uJDh4yWxbt069erVK+RY+/btNX36dHm9XnXp0kV16tRR/fr11b17d3Xv3j0wNL9Vq1bq3LmzWrRooW7duqlr167q16+fqlSpUuhzNWvWLDAst2PHjvrkk08KbU+tWrVUq1atwLEzzzxTlStX1rp163TeeedJkmrXrq3TTz89cE27du3k8/m0YcOGEs9bD/55JiYmBoZhBx/77rvvArc3btyoCRMmaMWKFdqzZ498Pp+k/GH9pV3kriRt2LRpkzwej9q3bx8473K51LZtW61bt06SQn5Wfu3atQu5/eOPP+qnn37S7NmzA8cMw5DP59PmzZvVtGnTEr+O22+/XW63W99++61OO+20Yj3Xli1bVL16dUkF/45I+WtEPPXUU9q0aZMyMjKUl5enuLi4wPkxY8bohhtu0BtvvKGUlBRdddVVatCgQYnbXlLBNatRo4YkadeuXWrSpEmxf7aFvd6SPG9iYqIkqUWLFiHH/L8PEhISNGTIEHXr1k1dunRRSkqK+vfvH2jvjz/+qK+//jpk2LnX61V2draysrIUHR0d8tyZmZnatGmThg0bphtvvDFwPC8vT5UrVw65Nvj95nQ61aZNm5D3Z7t27QI9/lL+752MjAz99ddf2rFjh3JyctS5c+di/yyOrsGoUaN0yy23aNGiRUpJSVHfvn1Dri9rhHYAAFDhHWuIus/nU3p6uuLi4ko9PPlUiImJCbl95513KjU1VY8++qgaNmyoqKgo9evXT7m5ucd8nKOH/tpstkBwLGuxsbFavXq1vvzySy1atEgTJkzQpEmTtHLlSsXHxys1NVXffPONFi1apBkzZujee+/VihUrVK9evQKPtWDBgsBc5KioqJPS3tII/nnabLbj/nx79uypOnXq6IUXXlBycrJ8Pp+aN29+3LqVZRvKQkZGhm666SaNGjWqwLljTc84li5duujNN9/Up59+qkGDBh33uXw+X8gK7kf/HVm+fLkGDRqkyZMnq1u3bqpcubLeeuutkHUXJk2apGuuuUYff/yxPvnkE02cOFFvvfWWevfuXarXYLPZCkzxCJ5D73d0zfyv51ivVwr92R79eoujsOc9+ljwe+WVV17RqFGjtHDhQr399tu67777lJqaqvPPP18ZGRmaPHmy+vTpU+B5IiMjCxzLyMiQJL3wwgsFPhRyOBwlfi1FKe7vh2PV4IYbblC3bt308ccfa9GiRZo6daoee+wx3XrrrWXWzmCEdgAAAAv6+uuvNWTIkEC4yMjIKHSxsJOpadOm+vrrrwu0q3HjxoH/hDudTqWkpCglJUUTJ05UfHy8Pv/8c/Xp00c2m03t27dX+/btNWHCBNWpU0cffPCBxowZU+C56tSpU6z2bNu2Tdu2bQv0tv/6669KS0vTmWeeGbhu69at2r59u5KTkyVJ3377rex2u8444wxJUkREhLxeb+l+KMewd+9ebdiwQS+88II6duwoKX/BsVOpQYMGioiI0Ndffx34mXo8Hq1cuTKwtV/Tpk0LLFD47bffhtw+55xz9Ouvv6phw4Zl1rYrrrhCPXv21DXXXCOHw6GBAwce87n8H7gV5ZtvvlGdOnV07733Bo4FL6Lm17hxYzVu3Fi33367rr76ar3yyivHDe1FvUeqVasWshvExo0bQxYBLI6T8bM9EWeffbbOPvtsjRs3Tu3atdOcOXN0/vnn65xzztGGDRuK3c7ExEQlJyfrjz/+CPlQpjDffvutOnTI/7A1Ly9Pq1at0siRIyXlvz/fe+89GYYRCNtff/21YmNjVbNmTVWvXl1RUVFavHixbrjhhlK/7lq1aunmm2/WzTffHJgjT2gHAABAsTVq1Ejvv/++evbsKZvNpvHjx5+0HvPdu3cHVk33q1Gjhu644w6de+65uv/++zVgwAAtX75cM2fODKwGPX/+fP3xxx+68MILVaVKFS1YsEA+n09nnHGGVqxYocWLF6tr166qXr26VqxYod27d5dqWLVfSkqKWrRooUGDBmn69OnKy8vTv//9b7Vv3z5kKHFkZKQGDx6sRx99VOnp6Ro1apT69+8fGBpft25dffrpp9qwYYOqVq1aYOhuaVWpUkVVq1bV888/rxo1amjr1q265557yuSxiysmJka33HKLxo4dq4SEBNWuXVvTpk1TVlaWhg0bJkm6+eab9dhjj2ns2LG64YYbtGrVqsDq53533323zj//fI0cOVI33HBDYLGu1NRUzZw5s9Tt6927t9544w3961//ktPpVL9+/Yp8rkWLFhVYETxYo0aNtHXrVr311ls699xz9fHHH+uDDz4InD906JDGjh2rfv36qV69evrrr7+0cuVK9e3b97jtrFu3rjIyMrR48WK1atVK0dHRio6O1iWXXKKZM2eqXbt28nq9uvvuu4+7sN3RTtbPtqQ2b96s559/XldccYWSk5O1YcMGbdy4Udddd50kacKECbr88stVu3Zt9evXT3a7XT/++KN+/vlnPfDAA4U+5uTJkzVq1ChVrlxZ3bt3V05Ojr7//nvt378/5MO6p59+Wg0aNFCtWrX04osvav/+/Ro6dKgk6d///remT5+uW2+9VSNHjtSGDRs0ceJEjRkzRna7XZGRkbr77rt11113KSIiQu3bt9fu3bv1yy+/BN7jxzN69Gj16NFDjRs31v79+/XFF1+c0O+m4ym/Y7wAAABQao8//riqVKmiCy64QD179lS3bt10zjnnnJTnmjNnTqC3zf/nhRde0DnnnKO5c+fqrbfeUvPmzTVhwgRNmTJFQ4YMkZS/1dX777+vSy65RE2bNtWsWbP05ptvqlmzZoqLi9PSpUt16aWXqnHjxrrvvvv02GOPqUePHqVup81m04cffqgqVarowgsvVEpKiurVq6eXX3455LqGDRuqT58+uvTSS9W1a1e1bNkyZNupG2+8UWeccYbatGmjatWqFRhNUFp2u11vvfWWVq1apebNm+v222/Xf//73zJ57JJ4+OGH1bdvX/3rX//SOeeco99//12ffvppYD2B2rVr67333tO8efPUqlUrzZo1Sw899FDIY/i34frtt9/UsWNHnX322ZowYUJg9MKJ6Nevn1577TX961//0vvvv1/q57riiit0++23a+TIkTrrrLP0zTffaPz48YHzDodDe/fu1XXXXafGjRurf//+6tGjhyZPnnzcNl5wwQW6+eabNWDAAFWrVk3Tpk2TJD322GOqVauWOnbsqGuuuUZ33nlngbndx3Myf7YlER0drfXr16tv375q3Lixhg8frhEjRuimm26SJHXr1k3z58/XokWLdO655+r888/XE088ccxRMTfccINefPFFvfLKK2rRooU6deqkV199tcCUmIcffljTpk1Tx44d9fXXX+ujjz4KrHNw+umna8GCBfruu+/UqlUr3XzzzRo2bJjuu+++wP3Hjx+vO+64QxMmTFDTpk0LbP14PF6vVyNGjFDTpk3VvXt3NW7cOOR3RFmzGUdPqqiA0tPTVblyZaWlpYUsPFHeeDweLViwQJdeemmJP5FD+Ud9rY36Whv1NY/s7Gxt3rxZ9erVK3ROZWHMMqcdpXN0fSdNmqR58+YVGDkA8+HvrvX8+eefqlevnn744Qe1bNnSFPU91r87xc2h5ffVAQAAAABQwRHaTeLTX3botrd/1Esb7Prpr7RwNwcAAABhdvPNN6tSpUqF/rn55pvD3bximz17dpGvo1mzZuFuXqF69OhRZJuPnipQHjz00ENFtvdEppzg1GAhOpPYtDtDC37eKcmunek54W4OAACAJU2aNEmTJk0KdzOKZcqUKbrzzjsLPVeep3we7YorriiwxZdfeZ1y9OKLL+rQoUOFnktISDjFrTm+m2++Wf379y/0XHnaIvF46tatG9gy72QtrFkeEdpNwmm3Bb7Pq0BvUAAAABSuevXqql69eribccJiY2MVGxsb7maUyOmnnx7uJpRIQkJCufwwAcXD8HiTcAYtruD1Vfi1AwEAOCGswwsAOBXK4t8bQrtJOB1HetoJ7QAAlI5/qG1WVlaYWwIAqAj8/96cyFQPhsebhCNkeDyhHQCA0nA4HIqPjw/sxxsdHS2bzXbM+/h8PuXm5io7O7tcbyuE0qG+1kVtra2819cwDGVlZWnXrl2Kj4+Xw+Eo9WMR2k0ieE47Pe0AAJReUlKSJAWC+/EYhqFDhw4pKirquAEf5kN9rYvaWptZ6hsfHx/4d6e0CO0m4Qj69MhDaAcAoNRsNptq1Kih6tWry+PxHPd6j8ejpUuX6sILLyy3K1mj9KivdVFbazNDfV0u1wn1sPsR2k2CnnYAAMqWw+Eo1n+mHA6H8vLyFBkZWW7/Y4jSo77WRW2trSLVt/wN/kehWIgOAAAAACoeQrtJsE87AAAAAFQ8hHaTCJ7T7vXS0w4AAAAAFQGh3SScbPkGAAAAABUOod0k2KcdAAAAACoeQrtJsHo8AAAAAFQ8hHaTcDqOlIqedgAAAACoGAjtJuGgpx0AAAAAKhxCu0mwEB0AAAAAVDyEdpMI7Wlnn3YAAAAAqAgI7SbhdAT1tLNPOwAAAABUCIR2k2B4PAAAAABUPIR2k3Daj5SKhegAAAAAoGIgtJuEg552AAAAAKhwwhraly5dqp49eyo5OVk2m03z5s0LnPN4PLr77rvVokULxcTEKDk5Wdddd522b98e8hj79u3ToEGDFBcXp/j4eA0bNkwZGRmn+JWcfMFz2ulpBwAAAICKIayhPTMzU61atdLTTz9d4FxWVpZWr16t8ePHa/Xq1Xr//fe1YcMGXXHFFSHXDRo0SL/88otSU1M1f/58LV26VMOHDz9VL+GUYZ92AAAAAKh4nOF88h49eqhHjx6FnqtcubJSU1NDjs2cOVNt27bV1q1bVbt2ba1bt04LFy7UypUr1aZNG0nSjBkzdOmll+rRRx9VcnLySX8Np0rwnHaPly3fAAAAAKAiCGtoL6m0tDTZbDbFx8dLkpYvX674+PhAYJeklJQU2e12rVixQr179y70cXJycpSTkxO4nZ6eLil/SL7H4zl5L+AEGN68wPd5Xl+5bSdKz19TamtN1NfaqK+1UV9ro77WRW2tzQr1LW7bTRPas7Ozdffdd+vqq69WXFycJGnHjh2qXr16yHVOp1MJCQnasWNHkY81depUTZ48ucDxRYsWKTo6umwbXkZyvJK/XLv27NGCBQvC2h6cPEePMIG1UF9ro77WRn2tjfpaF7W1NjPXNysrq1jXmSK0ezwe9e/fX4Zh6Nlnnz3hxxs3bpzGjBkTuJ2enq5atWqpa9eugQ8EypucPJ/u+u4zSVLl+ARdemnbMLcIZc3j8Sg1NVVdunSRy+UKd3NQxqivtVFfa6O+1kZ9rYvaWpsV6usf8X085T60+wP7li1b9Pnnn4eE6qSkJO3atSvk+ry8PO3bt09JSUlFPqbb7Zbb7S5w3OVylduC2x1HFp/zGSq37cSJK8/vQ5w46mtt1NfaqK+1UV/rorbWZub6Frfd5Xqfdn9g37hxoz777DNVrVo15Hy7du104MABrVq1KnDs888/l8/n03nnnXeqm3tSBS0ezz7tAAAAAFBBhLWnPSMjQ7///nvg9ubNm7VmzRolJCSoRo0a6tevn1avXq358+fL6/UG5qknJCQoIiJCTZs2Vffu3XXjjTdq1qxZ8ng8GjlypAYOHGipleMlyWazyWm3Kc9nKM/H6vEAAAAAUBGENbR///33uvjiiwO3/fPMBw8erEmTJumjjz6SJJ111lkh9/viiy900UUXSZJmz56tkSNHqnPnzrLb7erbt6+eeuqpU9L+U81xOLR7vfS0AwAAAEBFENbQftFFF8kwig6gxzrnl5CQoDlz5pRls8otp8OmnDyGxwMAAABARVGu57QjlPPwxHYvoR0AAAAAKgRCu4k4Dod2etoBAAAAoGIgtJuI055fLnraAQAAAKBiILSbiIPh8QAAAABQoRDaTcQf2j1s+QYAAAAAFQKh3URc9LQDAAAAQIVCaDcRFqIDAAAAgIqF0G4ibPkGAAAAABULod1EHA5COwAAAABUJIR2E2F4PAAAAABULIR2E/Hv024Y9LYDAAAAQEVAaDcR/5x2Scpj2zcAAAAAsDxCu4kEh3Z62gEAAADA+gjtJuII6WkntAMAAACA1RHaTSQ4tHu9hHYAAAAAsDpCu4k46WkHAAAAgAqF0G4iDhaiAwAAAIAKhdBuIk7HkXLlMTweAAAAACyP0G4irB4PAAAAABULod1EWD0eAAAAACoWQruJ0NMOAAAAABULod1EWIgOAAAAACoWQruJhGz5xkJ0AAAAAGB5hHYTCVk9nuHxAAAAAGB5hHYTcTCnHQAAAAAqFEK7iTiZ0w4AAAAAFQqh3UToaQcAAACAioXQbiLs0w4AAAAAFQuh3URYPR4AAAAAKhZCu4k4HcHD45nTDgAAAABWR2g3EaedLd8AAAAAoCIhtJsIC9EBAAAAQMVCaDcRB3PaAQAAAKBCIbSbiJOedgAAAACoUAjtJhLc0+5hIToAAAAAsDxCu4m46GkHAAAAgAqF0G4izGkHAAAAgIqF0G4irB4PAAAAABULod1E2KcdAAAAACoWQruJOBzBPe0sRAcAAAAAVkdoN5HgLd88zGkHAAAAAMsjtJsI+7QDAAAAQMVCaDeRkNXjCe0AAAAAYHmEdhMJ7WlnTjsAAAAAWB2h3UToaQcAAACAioXQbiLBW74xpx0AAAAArI/QbiIOVo8HAAAAgAqF0G4iTvZpBwAAAIAKhdBuIk7mtAMAAABAhUJoNxEH+7QDAAAAQIVCaDcRetoBAAAAoGIhtJtISE87C9EBAAAAgOUR2k0ktKedhegAAAAAwOoI7SbidBwpF8PjAQAAAMD6CO0mwkJ0AAAAAFCxENpNJGR4PHPaAQAAAMDyCO0mQk87AAAAAFQshHYTcdhYiA4AAAAAKhJCu4nY7TbZlN/DzkJ0AAAAAGB9hHaTcRzubGdOOwAAAABYH6HdZPzT2pnTDgAAAADWR2g3mUBPO3PaAQAAAMDyCO0mQ087AAAAAFQchHaTsQd62gntAAAAAGB1hHaTsbMQHQAAAABUGIR2k3HQ0w4AAAAAFQah3WSOzGlnIToAAAAAsDpCu8nQ0w4AAAAAFQeh3WT8BWP1eAAAAACwPkK7ybB6PAAAAABUHIR2kzmyejxz2gEAAADA6gjtJuOf0+4zJB+97QAAAABgaYR2k/GHdknyGoR2AAAAALAyQrvJ2G1HgjqL0QEAAACAtRHaTcYe1NPOYnQAAAAAYG2EdpMJDu1eL6EdAAAAAKyM0G4ywXPaPT5WkAcAAAAAKyO0m0xITzvD4wEAAADA0sIa2pcuXaqePXsqOTlZNptN8+bNCzlvGIYmTJigGjVqKCoqSikpKdq4cWPINfv27dOgQYMUFxen+Ph4DRs2TBkZGafwVZxaDua0AwAAAECFEdbQnpmZqVatWunpp58u9Py0adP01FNPadasWVqxYoViYmLUrVs3ZWdnB64ZNGiQfvnlF6Wmpmr+/PlaunSphg8ffqpewinHnHYAAAAAqDic4XzyHj16qEePHoWeMwxD06dP13333adevXpJkl5//XUlJiZq3rx5GjhwoNatW6eFCxdq5cqVatOmjSRpxowZuvTSS/Xoo48qOTn5lL2WUyV09XjmtAMAAACAlYU1tB/L5s2btWPHDqWkpASOVa5cWeedd56WL1+ugQMHavny5YqPjw8EdklKSUmR3W7XihUr1Lt370IfOycnRzk5OYHb6enpkiSPxyOPx3OSXtGJ83g8IUMjsnPLd3tRMv5aUlNror7WRn2tjfpaG/W1LmprbVaob3HbXm5D+44dOyRJiYmJIccTExMD53bs2KHq1auHnHc6nUpISAhcU5ipU6dq8uTJBY4vWrRI0dHRJ9r0k8phPxLbv1yyVBtjwtgYnBSpqanhbgJOIuprbdTX2qivtVFf66K21mbm+mZlZRXrunIb2k+mcePGacyYMYHb6enpqlWrlrp27aq4uLgwtuzYPB6P3n1uceB2uws6qPnp5be9KBmPx6PU1FR16dJFLpcr3M1BGaO+1kZ9rY36Whv1tS5qa21WqK9/xPfxlNvQnpSUJEnauXOnatSoETi+c+dOnXXWWYFrdu3aFXK/vLw87du3L3D/wrjdbrnd7gLHXS5XuS948Jx22e3lvr0oOTO8D1F61NfaqK+1UV9ro77WRW2tzcz1LW67y+0+7fXq1VNSUpIWLz7Ss5yenq4VK1aoXbt2kqR27drpwIEDWrVqVeCazz//XD6fT+edd94pb/OpwD7tAAAAAFBxhLWnPSMjQ7///nvg9ubNm7VmzRolJCSodu3aGj16tB544AE1atRI9erV0/jx45WcnKwrr7xSktS0aVN1795dN954o2bNmiWPx6ORI0dq4MCBllw5Xjp69XhCOwAAAABYWVhD+/fff6+LL744cNs/z3zw4MF69dVXdddddykzM1PDhw/XgQMH1KFDBy1cuFCRkZGB+8yePVsjR45U586dZbfb1bdvXz311FOn/LWcKg562gEAAACgwghraL/oootkGEUHT5vNpilTpmjKlClFXpOQkKA5c+acjOaVS3bbkZ+Xx8s+7QAAAABgZeV2TjsKR087AAAAAFQchHaTYU47AAAAAFQchHaTYfV4AAAAAKg4CO0mQ087AAAAAFQchHaTCZ3TzkJ0AAAAAGBlhHaTCQ7tHi897QAAAABgZYR2k2FOOwAAAABUHIR2k2FOOwAAAABUHIR2kwkumNfLnHYAAAAAsDJCu8k46GkHAAAAgAqD0G4yzGkHAAAAgIqD0G4y9LQDAAAAQMVBaDeZkIXo2PINAAAAACyN0G4yocPjWYgOAAAAAKyM0G4yDI8HAAAAgIqD0G4ydtuRoM5CdAAAAABgbYR2k7HT0w4AAAAAFQah3WRChsd7mdMOAAAAAFZGaDcZetoBAAAAoOIgtJtM6OrxhHYAAAAAsDJCu8mwejwAAAAAVByEdpMJLhg97QAAAABgbYR2k2FOOwAAAABUHIR2k2H1eAAAAACoOAjtJkNPOwAAAABUHIR2k3GwejwAAAAAVBiEdpOhpx0AAAAAKg5Cu8mE7tPOnHYAAAAAsDJCu8mELkRHTzsAAAAAWBmh3WQYHg8AAAAAFQeh3WTsNsl2OLgT2gEAAADA2gjtJuQ83N3OnHYAAAAAsDZCuwk5Dod25rQDAAAAgLUR2k3IEehpJ7QDAAAAgJUR2k3ISWgHAAAAgAqB0G5CTnt+2TzMaQcAAAAASyO0m1Cgp5057QAAAABgaYR2EwosRMfweAAAAACwNEK7CbEQHQAAAABUDIR2E3LS0w4AAAAAFQKh3YToaQcAAACAioHQbkJOx+HV472sHg8AAAAAVkZoNyH2aQcAAACAioHQbkLBq8cbBsEdAAAAAKyK0G5C/p52SaKzHQAAAACsi9BuQo6g0J7nY147AAAAAFgVod2EgnvamdcOAAAAANZFaDchp+NIaPd4Ce0AAAAAYFWEdhNy0NMOAAAAABUCod2EnPYjZWNOOwAAAABYF6HdhOhpBwAAAICKgdBuQiGrxzOnHQAAAAAsi9BuQqweDwAAAAAVA6HdhIJXj2dOOwAAAABYF6HdhIJ72vPoaQcAAAAAyyK0mxBz2gEAAACgYiC0m5AjaMs35rQDAAAAgHUR2k2I4fEAAAAAUDEQ2k2IfdoBAAAAoGIgtJuQK2ROO6vHAwAAAIBVEdpNyMHweAAAAACoEAjtJsTweAAAAACoGAjtJsRCdAAAAABQMRDaTcjhCO5pZ047AAAAAFgVod2EnEH7tNPTDgAAAADWRWg3oZDh8V5COwAAAABYFaHdhFg9HgAAAAAqBkK7CTntzGkHAAAAgIqA0G5C9LQDAAAAQMVAaDchJ/u0AwAAAECFQGg3IQcL0QEAAABAhUBoNyGnI3jLN+a0AwAAAIBVEdpNyMmcdgAAAACoEAjtJhQ8PN7L8HgAAAAAsCxCuwnR0w4AAAAAFQOh3YQcrB4PAAAAABUCod2E2KcdAAAAACoGQrsJuYJXj/eyejwAAAAAWFW5Du1er1fjx49XvXr1FBUVpQYNGuj++++XYRzpXTYMQxMmTFCNGjUUFRWllJQUbdy4MYytPvnoaQcAAACAiqFch/ZHHnlEzz77rGbOnKl169bpkUce0bRp0zRjxozANdOmTdNTTz2lWbNmacWKFYqJiVG3bt2UnZ0dxpafXE7mtAMAAABAheAMdwOO5ZtvvlGvXr102WWXSZLq1q2rN998U999952k/F726dOn67777lOvXr0kSa+//roSExM1b948DRw4MGxtP5noaQcAAACAiqFch/YLLrhAzz//vH777Tc1btxYP/74o5YtW6bHH39ckrR582bt2LFDKSkpgftUrlxZ5513npYvX15kaM/JyVFOTk7gdnp6uiTJ4/HI4/GcxFd0YvxtM3zeI8fyvOW6zSg+fx2ppzVRX2ujvtZGfa2N+loXtbU2K9S3uG0v16H9nnvuUXp6upo0aSKHwyGv16sHH3xQgwYNkiTt2LFDkpSYmBhyv8TExMC5wkydOlWTJ08ucHzRokWKjo4uw1dwcqxY/o38pduybZsWLNgS3gahTKWmpoa7CTiJqK+1UV9ro77WRn2ti9pam5nrm5WVVazrynVonzt3rmbPnq05c+aoWbNmWrNmjUaPHq3k5GQNHjy41I87btw4jRkzJnA7PT1dtWrVUteuXRUXF1cWTT8pPB6PUlNT1aljBz245ltJUmJSsi69tGWYW4ay4K9vly5d5HK5wt0clDHqa23U19qor7VRX+uittZmhfr6R3wfT7kO7WPHjtU999wTGObeokULbdmyRVOnTtXgwYOVlJQkSdq5c6dq1KgRuN/OnTt11llnFfm4brdbbre7wHGXy2WKgke6IwLfG7KZos0oPrO8D1E61NfaqK+1UV9ro77WRW2tzcz1LW67y/Xq8VlZWbLbQ5vocDjk8+XvTV6vXj0lJSVp8eLFgfPp6elasWKF2rVrd0rbeiqFLkTHPu0AAAAAYFXluqe9Z8+eevDBB1W7dm01a9ZMP/zwgx5//HENHTpUkmSz2TR69Gg98MADatSokerVq6fx48crOTlZV155ZXgbfxI52PINAAAAACqEch3aZ8yYofHjx+vf//63du3apeTkZN10002aMGFC4Jq77rpLmZmZGj58uA4cOKAOHTpo4cKFioyMDGPLTy4nW74BAAAAQIVQrkN7bGyspk+frunTpxd5jc1m05QpUzRlypRT17Awo6cdAAAAACqGcj2nHYVzBfe0ewntAAAAAGBVhHYTYiE6AAAAAKgYCO0m5GBOOwAAAABUCIR2E7LZbIHgzpx2AAAAALAuQrtJ+UM7c9oBAAAAwLoI7SblpKcdAAAAACyP0G5S/tDOQnQAAAAAYF2EdpNyOvJLx0J0AAAAAGBdhHaTYk47AAAAAFgfod2kmNMOAAAAANZHaDepQE87oR0AAAAALIvQblJHetpZiA4AAAAArIrQblIsRAcAAAAA1kdoNyknC9EBAAAAgOUR2k3KwUJ0AAAAAGB5hHaTCvS0M6cdAAAAACyL0G5S/p52nyH56G0HAAAAAEsitJuU036kdF6D0A4AAAAAVkRoNymnwxb4nnntAAAAAGBNhHaT8g+PlySPl3ntAAAAAGBFhHaTctrpaQcAAAAAqyO0m5QjaE57HqEdAAAAACyJ0G5S9LQDAAAAgPUR2k3KEbQQHT3tAAAAAGBNhHaTcgX3tHsJ7QAAAABgRYR2kwqe0+7xsXo8AAAAAFgRod2kmNMOAAAAANZHaDepkDntDI8HAAAAAEsitJsUPe0AAAAAYH2EdpNy2INXj2dOOwAAAABYEaHdpFyOI6Wjpx0AAAAArInQblLBPe0e5rQDAAAAgCUR2k2KOe0AAAAAYH2EdpNiTjsAAAAAWB+h3aToaQcAAAAA6yO0m5QzaCG6PEI7AAAAAFgSod2k6GkHAAAAAOsjtJtU6OrxzGkHAAAAACsitJsUPe0AAAAAYH2EdpNy2JnTDgAAAABWR2g3KXraAQAAAMD6CO0m5XQE79NOaAcAAAAAKyK0m1TwQnReFqIDAAAAAEsitJuUkzntAAAAAGB5hHaTCu5pJ7QDAAAAgDUR2k2KhegAAAAAwPoI7SblCF6IzktoBwAAAAArIrSblCtoTrvXx0J0AAAAAGBFhHaTYk47AAAAAFgfod2k2KcdAAAAAKyP0G5SIT3tzGkHAAAAAEsitJtU6OrxzGkHAAAAACsitJsUc9oBAAAAwPoI7SblcgSvHk9oBwAAAAArIrSbFD3tAAAAAGB9hHaTcoYsRMecdgAAAACwIkK7SdHTDgAAAADWR2g3KaedOe0AAAAAYHWlCu3btm3TX3/9Fbj93XffafTo0Xr++efLrGE4NnraAQAAAMD6ShXar7nmGn3xxReSpB07dqhLly767rvvdO+992rKlCll2kAUzuUI3qed0A4AAAAAVlSq0P7zzz+rbdu2kqS5c+eqefPm+uabbzR79my9+uqrZdk+FIGedgAAAACwvlKFdo/HI7fbLUn67LPPdMUVV0iSmjRpon/++afsWociBc9pZ/V4AAAAALCmUoX2Zs2aadasWfrqq6+Umpqq7t27S5K2b9+uqlWrlmkDUTh62gEAAADA+koV2h955BE999xzuuiii3T11VerVatWkqSPPvooMGweJ1fwPu3MaQcAAAAAa3KW5k4XXXSR9uzZo/T0dFWpUiVwfPjw4YqOji6zxqFodrtNNptkGPS0AwAAAIBVlaqn/dChQ8rJyQkE9i1btmj69OnasGGDqlevXqYNRNFch+e1e33MaQcAAAAAKypVaO/Vq5def/11SdKBAwd03nnn6bHHHtOVV16pZ599tkwbiKL557XneelpBwAAAAArKlVoX716tTp27ChJevfdd5WYmKgtW7bo9ddf11NPPVWmDUTR/PPaGR4PAAAAANZUqtCelZWl2NhYSdKiRYvUp08f2e12nX/++dqyZUuZNhBFczjyQzsL0QEAAACANZUqtDds2FDz5s3Ttm3b9Omnn6pr166SpF27dikuLq5MG4iiHelpZ047AAAAAFhRqUL7hAkTdOedd6pu3bpq27at2rVrJym/1/3ss88u0waiaP457V7mtAMAAACAJZVqy7d+/fqpQ4cO+ueffwJ7tEtS586d1bt37zJrHI7NeXj1eOa0AwAAAIA1lSq0S1JSUpKSkpL0119/SZJq1qyptm3bllnDcHxO5rQDAAAAgKWVani8z+fTlClTVLlyZdWpU0d16tRRfHy87r//fvmYX33K+IfHe7z8zAEAAADAikrV037vvffqpZde0sMPP6z27dtLkpYtW6ZJkyYpOztbDz74YJk2EoXzL0RHTzsAAAAAWFOpQvtrr72mF198UVdccUXgWMuWLXX66afr3//+N6H9FHEwpx0AAAAALK1Uw+P37dunJk2aFDjepEkT7du374QbheKhpx0AAAAArK1Uob1Vq1aaOXNmgeMzZ85Uy5YtT7hRKB7/QnR5PkOGQXAHAAAAAKsp1fD4adOm6bLLLtNnn30W2KN9+fLl2rZtmxYsWFCmDUTR/D3tkuQzJIftGBcDAAAAAEynVD3tnTp10m+//abevXvrwIEDOnDggPr06aNffvlFb7zxRpk28O+//9a1116rqlWrKioqSi1atND3338fOG8YhiZMmKAaNWooKipKKSkp2rhxY5m2obxyBIV2VpAHAAAAAOsp9T7tycnJBRac+/HHH/XSSy/p+eefP+GGSdL+/fvVvn17XXzxxfrkk09UrVo1bdy4UVWqVAlcM23aND311FN67bXXVK9ePY0fP17dunXTr7/+qsjIyDJpR3nltB/5zIV57QAAAABgPaUO7afCI488olq1aumVV14JHKtXr17ge8MwNH36dN13333q1auXJOn1119XYmKi5s2bp4EDBxb6uDk5OcrJyQncTk9PlyR5PB55PJ6T8VLKhL9t/q9225Ggnp2Tqwg7wd3Mjq4vrIX6Whv1tTbqa23U17qorbVZob7FbbvNKMMVzH788Uedc8458nq9ZfJ4Z555prp166a//vpLS5YsCWwpd+ONN0qS/vjjDzVo0EA//PCDzjrrrMD9OnXqpLPOOktPPvlkoY87adIkTZ48ucDxOXPmKDo6ukzafio8v96uX/bn97Y/2CZPlVxhbhAAAAAAoFiysrJ0zTXXKC0tTXFxcUVeV6572v/44w89++yzGjNmjP7zn/9o5cqVGjVqlCIiIjR48GDt2LFDkpSYmBhyv8TExMC5wowbN05jxowJ3E5PT1etWrXUtWvXY/6wws3j8Sg1NVVdunSRy+XS/ANr9Mv+XZKkiy/prGqx7jC3ECfi6PrCWqivtVFfa6O+1kZ9rYvaWpsV6usf8X08JQrtffr0Oeb5AwcOlOThjsvn86lNmzZ66KGHJElnn322fv75Z82aNUuDBw8u9eO63W653QUDrsvlMkXB/e2McDoCx2wOhynajuMzy/sQpUN9rY36Whv1tTbqa13U1trMXN/itrtEob1y5crHPX/dddeV5CGPqUaNGjrzzDNDjjVt2lTvvfeeJCkpKUmStHPnTtWoUSNwzc6dO0OGy1tV8OrxeV7mswMAAACA1ZQotAcvCHcqtG/fXhs2bAg59ttvv6lOnTqS8helS0pK0uLFiwMhPT09XStWrNAtt9xyStsaDsH7tOexejwAAAAAWE65ntN+++2364ILLtBDDz2k/v3767vvvtPzzz8f2FLOZrNp9OjReuCBB9SoUaPAlm/Jycm68sorw9v4UyC4p93rY592AAAAALCach3azz33XH3wwQcaN26cpkyZonr16mn69OkaNGhQ4Jq77rpLmZmZGj58uA4cOKAOHTpo4cKFlt+jXZKcDnraAQAAAMDKynVol6TLL79cl19+eZHnbTabpkyZoilTppzCVpUPTrs98D1z2gEAAADAeuzHvwTlVejweEI7AAAAAFgNod3EQheiY047AAAAAFgNod3EHA62fAMAAAAAKyO0m5iT4fEAAAAAYGmEdhNzBC9ER2gHAAAAAMshtJuYi552AAAAALA0QruJOdinHQAAAAAsjdBuYiGrx3tZPR4AAAAArIbQbmLMaQcAAAAAayO0mxirxwMAAACAtRHaTcxhZ047AAAAAFgZod3EXI7gnnbmtAMAAACA1RDaTYw57QAAAABgbYR2EwtdPZ7QDgAAAABWQ2g3Mea0AwAAAIC1EdpNLHT1eOa0AwAAAIDVENpNjJ52AAAAALA2QruJuRxHyudlTjsAAAAAWA6h3cToaQcAAAAAayO0m1jI6vHMaQcAAAAAyyG0mxg97QAAAABgbYR2E3M6glaPZ047AAAAAFgOod3EHPYj5aOnHQAAAACsh9BuYqH7tBPaAQAAAMBqCO0mFjw8np52AAAAALAeQruJhawe72X1eAAAAACwGkK7iQXPaWd4PAAAAABYD6HdxJxs+QYAAAAAlkZoNzEHC9EBAAAAgKUR2k0sdCE65rQDAAAAgNUQ2k3MyZx2AAAAALA0QruJBQ+P93gJ7QAAAABgNYR2E3Mypx0AAAAALI3QbmIOO3PaAQAAAMDKCO0mRk87AAAAAFgbod3EHOzTDgAAAACWRmg3MZvNFuhtp6cdAAAAAKyH0G5y/t52Vo8HAAAAAOshtJvckZ52FqIDAAAAAKshtJucv6edOe0AAAAAYD2EdpNzOvJLyJx2AAAAALAeQrvJ+YfH5zGnHQAAAAAsh9BucqweDwAAAADWRWg3OYfDP6edhegAAAAAwGoI7SbntOeXkIXoAAAAAMB6CO0m51893sucdgAAAACwHEK7yTnZ8g0AAAAALIvQbnJOBwvRAQAAAIBVEdpNzhGY085CdAAAAABgNYR2k/MPj/cZko/edgAAAACwFEK7yfkXopOY1w4AAAAAVkNoNzlnUGhnXjsAAAAAWAuh3eRCe9qZ1w4AAAAAVkJoNzmX40gJ6WkHAAAAAGshtJscc9oBAAAAwLoI7SYXPKc9z0toBwAAAAArIbSbHHPaAQAAAMC6CO0mx+rxAAAAAGBdhHaTc9iPlJA57QAAAABgLYR2k3M56GkHAAAAAKsitJucg4XoAAAAAMCyCO0m52QhOgAAAACwLEK7yTGnHQAAAACsi9Buck7mtAMAAACAZRHaTY457QAAAABgXYR2k3OxTzsAAAAAWBah3eRC57SzEB0AAAAAWAmh3eSC57QzPB4AAAAArIXQbnIRjiMlzMmjpx0AAAAArITQbnLRbkfg+6zcvDC2BAAAAABQ1gjtJhcT4Qx8n5XrDWNLAAAAAABljdBuctERR3raM+lpBwAAAABLIbSbXIw7qKc9h552AAAAALASQrvJ0dMOAAAAANZFaDc5etoBAAAAwLoI7SZHTzsAAAAAWBeh3eRYPR4AAAAArIvQbnJRwT3tOfS0AwAAAICVmCq0P/zww7LZbBo9enTgWHZ2tkaMGKGqVauqUqVK6tu3r3bu3Bm+Rp5ibqddDrtNEj3tAAAAAGA1pgntK1eu1HPPPaeWLVuGHL/99tv1v//9T++8846WLFmi7du3q0+fPmFq5alns9kC89qzmNMOAAAAAJZiitCekZGhQYMG6YUXXlCVKlUCx9PS0vTSSy/p8ccf1yWXXKLWrVvrlVde0TfffKNvv/02jC0+tfzz2ulpBwAAAABrcR7/kvAbMWKELrvsMqWkpOiBBx4IHF+1apU8Ho9SUlICx5o0aaLatWtr+fLlOv/88wt9vJycHOXk5ARup6enS5I8Ho88Hs9JehUnzt+2o9sYHZH/2UtmTl65bj+Oraj6whqor7VRX2ujvtZGfa2L2lqbFepb3LaX+9D+1ltvafXq1Vq5cmWBczt27FBERITi4+NDjicmJmrHjh1FPubUqVM1efLkAscXLVqk6OjoE27zyZaamhpy23PIIcmmjByPPv54gWy28LQLZePo+sJaqK+1UV9ro77WRn2ti9pam5nrm5WVVazrynVo37Ztm2677TalpqYqMjKyzB533LhxGjNmTOB2enq6atWqpa5duyouLq7MnqeseTwepaamqkuXLnK5XIHjs/9ZqW2Z++UzbErp1l1upylmPeAoRdUX1kB9rY36Whv1tTbqa13U1tqsUF//iO/jKdehfdWqVdq1a5fOOeecwDGv16ulS5dq5syZ+vTTT5Wbm6sDBw6E9Lbv3LlTSUlJRT6u2+2W2+0ucNzlcpmi4Ee3s1Lkke89PpsqmeA1oGhmeR+idKivtVFfa6O+1kZ9rYvaWpuZ61vcdpfr0N65c2etXbs25Nj111+vJk2a6O6771atWrXkcrm0ePFi9e3bV5K0YcMGbd26Ve3atQtHk8MiOniv9tw8VYmJCGNrAAAAAABlpVyH9tjYWDVv3jzkWExMjKpWrRo4PmzYMI0ZM0YJCQmKi4vTrbfeqnbt2hW5CJ0VBYd2VpAHAAAAAOso16G9OJ544gnZ7Xb17dtXOTk56tatm5555plwN+uUio44UsbMHPZqBwAAAACrMF1o//LLL0NuR0ZG6umnn9bTTz8dngaVAzHuIz3th+hpBwAAAADLYJlxCwjpaSe0AwAAAIBlENotICZkTjvD4wEAAADAKgjtFhDtDp7TTk87AAAAAFgFod0CYoKGx9PTDgAAAADWQWi3gOighejoaQcAAAAA6yC0WwA97QAAAABgTYR2C4gOWoguk9AOAAAAAJZBaLeA4NCexfB4AAAAALAMQrsFxASvHk9POwAAAABYBqHdAkJ62nPpaQcAAAAAqyC0W0B0yEJ0hHYAAAAAsApCuwU47DZFuvJLmZnD8HgAAAAAsApCu0X4t32jpx0AAAAArIPQbhHR7vx57ezTDgAAAADWQWi3CH9PeyZbvgEAAACAZRDaLSLq8AryhzxeeX1GmFsDAAAAACgLhHaLiAlaQf6Qh952AAAAALACQrtFhOzVzgryAAAAAGAJhHaLiHEf6WnPZAV5AAAAALAEQrtFhPS0s4I8AAAAAFgCod0ignva2asdAAAAAKyB0G4RwT3tmcxpBwAAAABLILRbRPDq8fS0AwAAAIA1ENotItpNTzsAAAAAWA2h3SLoaQcAAAAA6yG0W0RU8Jx2Vo8HAAAAAEsgtFtESE97Dj3tAAAAAGAFhHaLCJnTTk87AAAAAFgCod0i6GkHAAAAAOshtFtE8D7tWR5COwAAAABYAaHdImLcwT3tDI8HAAAAACsgtFtENKvHAwAAAIDlENotwu20y2G3SWKfdgAAAACwCkK7RdhstkBveybD4wEAAADAEgjtFuIP7fS0AwAAAIA1ENotxL/tGz3tAAAAAGANhHYLiXYf6Wk3DCPMrQEAAAAAnChCu4VEH+5pz/MZyvX6wtwaAAAAAMCJIrRbSEzQtm+HmNcOAAAAAKZHaLeQaLcz8H0moR0AAAAATI/QbiHBPe1ZLEYHAAAAAKZHaLcQ/5x2iZ52AAAAALACQruFxLjpaQcAAAAAKyG0Wwg97QAAAABgLYR2C4kOntOeS087AAAAAJgdod1CYoJ72nPoaQcAAAAAsyO0W0i0m552AAAAALASQruF0NMOAAAAANZCaLeQkDntHnraAQAAAMDsCO0WEuM+0tOeRU87AAAAAJgeod1CgnvaM5nTDgAAAACmR2i3EHraAQAAAMBaCO0WEkVPOwAAAABYCqHdQqJdwVu+0dMOAAAAAGZHaLcQp8MutzO/pJk59LQDAAAAgNkR2i3GP6+dnnYAAAAAMD9Cu8X4V5DPYk47AAAAAJgeod1iYiLye9ozWT0eAAAAAEyP0G4x0e78nvZDHq98PiPMrQEAAAAAnAhCu8X4e9ql/OAOAAAAADAvQrvFRLNXOwAAAABYBqHdYoJDexbz2gEAAADA1AjtFhPtPjI8np52AAAAADA3QrvFxAT3tLNXOwAAAACYGqHdYqKDFqLLzKGnHQAAAADMjNBuMTFuetoBAAAAwCoI7RZDTzsAAAAAWAeh3WLoaQcAAAAA6yC0W0xwTzuhHQAAAADMjdBuMTEhoZ3h8QAAAABgZoR2i4kK2vItM4eedgAAAAAwM0K7xYTOaaenHQAAAADMjNBuMcHD4zOZ0w4AAAAApkZot5jooOHxWWz5BgAAAACmRmi3mBh3cE87oR0AAAAAzIzQbjFup112W/73bPkGAAAAAOZGaLcYm80WmNeeyfB4AAAAADA1QrsFRR9eQf4QPe0AAAAAYGrlOrRPnTpV5557rmJjY1W9enVdeeWV2rBhQ8g12dnZGjFihKpWrapKlSqpb9++2rlzZ5haXD4EetoJ7QAAAABgauU6tC9ZskQjRozQt99+q9TUVHk8HnXt2lWZmZmBa26//Xb973//0zvvvKMlS5Zo+/bt6tOnTxhbHX7+nnb2aQcAAAAAc3Me/5LwWbhwYcjtV199VdWrV9eqVat04YUXKi0tTS+99JLmzJmjSy65RJL0yiuvqGnTpvr22291/vnnh6PZYRftyi+rx2soN8+nCGe5/mwGAAAAAFCEch3aj5aWliZJSkhIkCStWrVKHo9HKSkpgWuaNGmi2rVra/ny5UWG9pycHOXk5ARup6enS5I8Ho88Hs/Jav4J87fteG2Mch0J6WmZ2YqPdp3UdqFsFLe+MCfqa23U19qor7VRX+uittZmhfoWt+02wzCMk9yWMuHz+XTFFVfowIEDWrZsmSRpzpw5uv7660MCuCS1bdtWF198sR555JFCH2vSpEmaPHlygeNz5sxRdHR02Tf+FHvlN7vW7M0P7hPPyVOCO8wNAgAAAACEyMrK0jXXXKO0tDTFxcUVeZ1petpHjBihn3/+ORDYT8S4ceM0ZsyYwO309HTVqlVLXbt2PeYPK9w8Ho9SU1PVpUsXuVxF954vzflZa/ZulySd1/5CNape6VQ1ESeguPWFOVFfa6O+1kZ9rY36Whe1tTYr1Nc/4vt4TBHaR44cqfnz52vp0qWqWbNm4HhSUpJyc3N14MABxcfHB47v3LlTSUlJRT6e2+2W212w+9nlcpmi4MdrZ2xkROD7XJ/NFK8JR5jlfYjSob7WRn2tjfpaG/W1LmprbWaub3HbXa5XKDMMQyNHjtQHH3ygzz//XPXq1Qs537p1a7lcLi1evDhwbMOGDdq6davatWt3qptbbkRHOALfZ+WwgjwAAAAAmFW57mkfMWKE5syZow8//FCxsbHasWOHJKly5cqKiopS5cqVNWzYMI0ZM0YJCQmKi4vTrbfeqnbt2lXYleMlKcZ9pKzs1Q4AAAAA5lWuQ/uzzz4rSbroootCjr/yyisaMmSIJOmJJ56Q3W5X3759lZOTo27duumZZ545xS0tX0J62tmrHQAAAABMq1yH9uIsbB8ZGamnn35aTz/99ClokTnERBwpaxY97QAAAABgWuV6TjtKJyqopz2TOe0AAAAAYFqEdguKcQcPj6enHQAAAADMitBuQdERwQvR0dMOAAAAAGZFaLegkDntOfS0AwAAAIBZEdotKDpoeDw97QAAAABgXoR2C6KnHQAAAACsgdBuQfS0AwAAAIA1ENotKNrF6vEAAAAAYAWEdgtyOuxyO/NLS2gHAAAAAPMitFtUdER+b3sWw+MBAAAAwLQI7Rbl36s9k4XoAAAAAMC0CO0WFeOmpx0AAAAAzI7QblH+nvasXK98PiPMrQEAAAAAlAah3aJigrZ9O+RhiDwAAAAAmBGh3aL8Pe0Se7UDAAAAgFkR2i0qJiJor3YWowMAAAAAUyK0W1S0m552AAAAADA7QrtFhfS059LTDgAAAABmRGi3qKigOe37MnPD2BIAAAAAQGkR2i2qbtXowPczP/9dXrZ9AwAAAADTIbRb1OUtk9U4sZIkae3faXr1mz/D2yAAAAAAQIkR2i0qwmnX1D4tArcfW7RBf+3PCmOLAAAAAAAlRWi3sNZ1EnTt+bUl5S9GN+HDX2QYDJMHAAAAALMgtFvcXd2bqHqsW5L0+fpdWrB2R5hbBAAAAAAoLkK7xcVFujT5imaB2xM/+kVpWZ4wtggAAAAAUFyE9gqge/MkpTRNlCTtycjRwwvXh7lFAAAAAIDiILRXADabTVN6NVNMhEOS9OZ3W/Xd5n1hbhUAAAAA4HgI7RVEcnyU7ux2RuD2uPd/UrbHG8YWAQAAAACOh9BegVzXrq5a1awsSdq0O1Nj5q6Rz8dq8gAAAABQXhHaKxCH3aZp/Vop+vAw+QVrd+jBBevC3CoAAAAAQFEI7RXMGUmxenrQOXLYbZKkl5Zt1svLNoe5VQAAAACAwhDaK6CLz6iuB69sHrh9/8e/auHP/4SxRQAAAACAwhDaK6iBbWvr1ksaSpIMQ7rtrTVatYUV5QEAAACgPCG0V2BjujRWn3NOlyTl5Pl0w2vf64/dGWFuFQAAAADAj9BegdlsNj3cp6U6NDxNkrQ/y6Mhr6xU2iFPmFsGAAAAAJAI7RVehNOuZ649R02SYiVJW/dladrC9WFuFQAAAABAIrRDUlykSy8NOVcxh7eCm71iq1Zt2R/mVgEAAAAACO2QJJ0eH6UxXc8I3L73g7XyeH1hbBEAAAAAgNCOgMHt6qj56XGSpPU7Duol9m8HAAAAgLAitCPA6bDrod4tZLfl357+2W/ati8rvI0CAAAAgAqM0I4QLWvG67p2dSVJ2R6fJnz4swzDCG+jAAAAAKCCIrSjgDu6NlZinFuS9MWG3frk5x1hbhEAAAAAVEyEdhQQG+nSpJ7NArcnffSL0rPZux0AAAAATjVCOwrVvXmSOjepLknadTBHj326IcwtAgAAAICKh9COQtlsNk3u1UxRrvy921//dou+2bQnzK0CAAAAgIqF0I4i1awSrdu7NJIkGYY06s0ftDM9O8ytAgAAAICKg9COYxrWob46NjpNkrQnI1cj56yWx+sLc6sAAAAAoGIgtOOYHHabnhx4tpIrR0qSVv65X498sj7MrQIAAACAioHQjuNKiInQM9e2lsthkyS9uGyzFqz9J8ytAgAAAADrI7SjWM6qFa8Jl58ZuH3Xuz/pj90ZYWwRAAAAAFgfoR3Fdu35ddTrrGRJUkZOnm75v9XKys0Lc6sAAAAAwLoI7Sg2m82mqX1aqHFiJUnShp0Hde8HP8swjDC3DAAAAACsidCOEomOcOrZa1srJiJ///YPfvhb989fR3AHAAAAgJOA0I4Sa1Ctkh69qlXg9stfb9aED3+Rz0dwBwAAAICyRGhHqfRoUUPT+rWULX9Beb3x7RbdO28twR0AAAAAyhChHaXWv00tPd6/leyHg/ub323TXe/9JC/BHQAAAADKBKEdJ6T32TU1feDZchxO7u+u+ktj5q5RntcX5pYBAAAAgPkR2nHCrmiVrJlXny3n4eD+4ZrtumX2am3blxXmlgEAAACAuRHaUSZ6tKihZ69tLZcjP7in/rpTFz36pUa9+YN+/jstzK0DAAAAAHMitKPMdDkzUc//q40quZ2SJK/P0Ec/btflM5bpXy+t0Ne/72FrOAAAAAAoAUI7ytTFTarrq7su1pgujZUQExE4/tXGPRr04gpdMfNrzf9pO3PeAQAAAKAYCO0oc1ViIjSqcyN9ffclur9XM9VKiAqcW/t3mkbO+UGXPLZEbyz/U9kebxhbCgAAAADlG6EdJ01UhEP/aldXX9xxkWZcfbaanx4XOLd1X5bGf/iL2j/8uZ5avFH7M3PD2FIAAAAAKJ+c4W4ArM/psKtnq2Rd3rKGvtm0V7OWbNJXG/dIkvZm5urx1N8084vf1aN5kgacW0vn16squ3/zdwAAAACowAjtOGVsNpvaNzxN7Ruepp//TtPzS//Q/J+2y2dIuXk+fbhmuz5cs121E6LVv01N9WtdS0mVI8PdbAAAAAAIG4bHIyyan15ZT119tpaMvVhD29dTfLQrcG7rviw9uug3XfDwYg19daUW/rxDHhauAwAAAFAB0dOOsKqVEK0JPc/U3T3O0KJfdmru99u07Pc9MgzJZ0ifr9+lz9fv0mmVItT3nJrqf24tNahWKdzNBgAAAIBTgtCOcsHtdKhnq2T1bJWsbfuy9M6qv/Tu99u0PS1bkrQnI1fPLf1Dzy39Q23qVFHvc07XRWdU1+nxUcd5ZAAAAAAwL0I7yp1aCdEa06WxbuvcSF9t3K25329T6q875fEakqTvt+zX91v2S5IaVa+kTo2r6cLG1dS2XoIiXY6QxzIMQzl5PrmddtlsLG4HAAAAwFwI7Si3HHabLjqjui46o7r2ZuTogx/+1tsrt2njrozANRt3ZWjjrgy9uGyzIl12JcVFKtvjU3aeV4dyvcrJy58Lf1oltzo3qa6UMxPVoeFpiopwFPW0AAAAAFBuENphClUruXVDx/oa1qGe1v6dpi/W79aS33ZpzbYD8uV3wCvb49Ofe7MKvf+ejBy9/f02vf39NrmddnVoeJpSzkxUg2qVlJPnVY7Hp5w8X/73eT4lxUXqnDpVVDnKVejjAQAAAMCpQGiHqdhsNrWsGa+WNeN1W0ojpWV5tOz3PVry2y59/fteZebmKdLpUKTLrkiXQ26XQy67Tb9sT9chj1eSlJPn0+L1u7R4/a7jPJfUNClObesl6Lx6CTq3XoJOq+Q+bhv9Q/Kzcr2q5HYqwskmDQAAAABKh9AOU6sc7dJlLWvospY1jnldtserbzbtUeqvu7R43U7tOphz3Mc2DOnXf9L16z/pevWbPyVJsW6nXE67Ihx2uZw2uRz53+d6fcrK8SozN09ZuV55D3f/221S7YRo1a9WSQ2qxRz+WkkJMRFyOWxy2PMfw2m3ST6v8nz5oR8AAAAAJEI7KohIl0OXNEnUJU0S5fM118/b07Rkw25l5OTJ7bQrwmmX2+mQ22WXw27Txp0Z+m7zPq3bka7gDH0wJ086ft4P8BnSn3uz9OfeLH2+vjj3cOqulZ8p2uVQZIRD0REORR1eXC8nz6ccj/fwMP78ofyRToeqxESoSkyEEqJdh79GKC7KpUpupypFOhV7+Gslt1OxkU5VcrtUKdKpaJdDdnvxFuc7lOvV/qxc7c/K1YEsjyJdDlWPdatarLvA4n8AAAAAyg6hHRWO3X5kiP3xpB3yaNWWfVqxeZ9W/blf+7Ny5fEa8nh98nh9ys3zKdfrk8thV0yEU9FuR/7XCIeiIhzak5GjP3ZnKivXW+z2eX2GDubk5X9AcBweb/51W/cVPpf/WGw2KSYiP8y7nDbZbfl/bIfP2Ww2ZebkaX9WrrI9viIfJ9btVLVYt06LdR/+wMCpuEiX4qJciot0Ki7KJZfDLkOhowgMQzrk8epgtkcZ2XlKz85TRk6eMrLzFOmyKyHGraqVIpQQk/+nakyE3M4jHxAEbwZQLdat6rFudggAAACA5RDagWOoHOUK9NCXlmEY2pGerU27MvXHngz9sTtTB7PzlOfzKe/wBwB5PkO5eV79s3OPImPjlO3x6VCuV1ker7JyvbLb8veydzvtcrvsinQ6FOG065DHq/2ZuTpwyKOSjqo3DOWH5GJ8OHAs/g8Y/tiTeUKPc6IquZ1BUxDyvzrtNh3MzlN6tkfph/K/Hsz2yGG3Hf6QxalKboeiI5yKcTvktB9ZfyA4/8e4naoSnT+KIT7GpVi3s8w+IPD6DNkPf0gCAAAAHI3QDpxkNptNNSpHqUblKHVodFqR13k8Hi1YsECXXtpOLlfJVq3P8/qUdsij/Vm52peZH0wzcvJ0MKj32h/Q/d8fzMlTxuHr8ryGDEk+w5DPZxzuFZeiIhz5QTXapSrR+cPw46NdyvZ4tetgjnYfzNGew1+LMzLgZMrIydOPf6Xpx7/STvpzOe02xUe7Cu35tx3+gCXm8GiL6MMjLyKddm3aYtfcXauUnp0/giEty6ODOXlyOWyqHOX/ObsUHx2h+CiXbDYFRnPkHp4W4fH65HY68qc/FJj6UHAqRIzbIbvNJq/PkNdnKM9nyGcYgdvBx/K8hgzDUIzbGRjhEB3h4AMFAACAMCK0AxbgdNhVtZJbVYuxuv3JcijXq7RDHqUd8hzu2fYEerjzDi/M5x967/8+KsIRmGMfe3j+fYzbqUMer/Zl5ob82ZuRqzxf/jD94FEFXsPQ9gOHtGl3hv7af6jEIw5KI89naE9GbinuaZf27i1w1OM1tCcjR3sySrBgwinidtpVNSZCCZUiZJNNOXnewAcIuYf/RDjtIVNDYtz5X/2LLNrtNjntNjnsdjnsktOev3aE/4/Tnj81w2m3yeE4cq3z8Hm30374AxCHIl35H4REuYJv5391HLVGg89nKNfry9/S0etVhMOu6Ah2dAAAAOZimdD+9NNP67///a927NihVq1aacaMGWrbtm24mwVUGFGHe5aTKkeWyeM1qFby+2R7vNq8J1N/7M7U5j0ZstlsgXn1sZH5c+0rRTrl80lZufkjDrJyvcrMyVNmTp68hwP/0XPvD+bk6UBWrvZnebQ/88iCfB6vLzAq4fDVMoz8RQMzc/OK/ADBblN+b3q0S5WjXMrxHBkpUZL1D06FnDyftqdla3ta9jEukvaGd3aEpPwPGCJdDvl8+dsu5noLX4vBabcpKuLIhwz5C1HmL0jpchz5mufN/3Ai+/ACkP6vkS5HyOiG2Einolx2bdti17rUjXI580c3BH8w4bDlf3jhsEmOoA8ygq/zf28Yh0e9GP5REfm3I10ORbscBdbOiHDa5bLb5XDY5LLb5Tz8wUf+tJv8D1Y83iMjNew2m1xOu1x2m5yO/OtddnvgA7Wj37f5u1zYGHEBAECYWCK0v/322xozZoxmzZql8847T9OnT1e3bt20YcMGVa9ePdzNA3CKRLocalojTk1rxIW7KTKM/OCYefiDgYNZOVr+9VJd2aOLqlSKKnLl/pw8r9KyPDpwyCNJijgcIgN/HHbl5PmCpjp4CkyD8N/O9C9oaCgkJPp7v/293nabTU6HfzFC6WB2Xv7ohsycwyMd8j9QsEmBdgSHXE/QlofHWrTwZPPvrHA8eT5DB7Pzf05ly67Pt28u48csP5z2/PeJy2E//Mcmpz3/feC02wLHbIffR4Gvsh1e3FL5i10e/ioFX3PknO3wgpgh1/rvKwU+ZDjSlvwPH1yOIx9EuA6fczps8vkMebyG8ny+/K9eQ16fTzab7fDWm/bAFpzOw6ND/K/VeXjEh83w6ed9NrnX75LL6Tzc9vyfS36rpKO+BD7kOHI7/2vw68r/sMb/c8j/Pvi8/aifZWDBUP+HP0Wdt6vA4/kV9WFi8PH8SVKFXx9cR5tCn4MPdgDg5LBEaH/88cd144036vrrr5ckzZo1Sx9//LFefvll3XPPPWFuHYCKyGazKdKVP2y7qiRPrEsbI6W4KNcxt9pzOx2qHudQ9biiRyxEuhyqHFWydQ9OlGEYxfoPuddnKCs3T5k5Xnm8vvy58r6Cc+iPfO+Tzyd5fD75CrnG30N8KNerQ4cXZsz2eJWVm6dDHp8O5eYFjh86fC5/SH3+Fo7uw9s5+j9cOJSb/+GC/2tWrjewbsCxplZEOI58UOH/0KSiyTtcl3B+MBNeDr2wYU24G1HuHf3hgu3o28r/EDH4t8nRv1tCzx39DLYizx19v8AHRkc9h79d/vMypKwsh6b/tkw2+5EPW/wfIIVcG3S8sMcq8rfkMX5/FnWmqLsc6zdxUb+nS/oc+fcpWQOO3a6SPccx21WCx/IZPu3dk7+eTHG3uc1/jqLfk0W1o/BrSv44hV1VvOc6+prjP05ZKsvpicEfHB6Lz2do50675h9YU2h9G1SrpLu6Nym7hoWR6UN7bm6uVq1apXHjxgWO2e12paSkaPny5YXeJycnRzk5R+aOpqenS8pfCMzj8ZzcBp8Af9vKcxtRetTX2ipSfSMdUmS0Q5LjuNeWF8bhDxfyt3LM/+p02AI7NRw9X97/4URGjlcZ2Xk6kJmt5Su+09mt28hmdwQW+/Mdvfifkf+fDG/QYoCBa438xz0Scg5/PfzcuXk+ZeUe/sAi16vMwx9U+HegyPMZygv63mm3KcJhl+vwCI2Iwz3QPkOBXufAfY+aSuD/j6Zh5LfZ4z2y20Xw/fzbX+Z589cPyPMZp2RdCZRfvsPTO1TM/3SXHzbtzi759qkwA7s2pBVcTwZWYZf27Sr0zDm1c8r9/7uK2z7Th/Y9e/bI6/UqMTF0S67ExEStX7++0PtMnTpVkydPLnB80aJFio6OPintLEupqanhbgJOIuprbdTXuhpWlg7+/n2R5+2H/5QJu6TIw3/KIcPIj2zBaz74/GtGBB0PPn/08cIex5DkNfL/5H/44P/eprzDx72+I9d4jfyeX0fQH/9tw5C8yr/ed5yv3qPy59FxNPg15LOF3j7qvM//eozD3x/18wj+6ivkZ+A73rVH3cdn2I7qYTu6pQUdq0cu//FthdanwHMffroCxwo2pbCbxT8XMrw/6KsRej+jkOOlve5Y9y8u45j90wBOxP79+7VgwYJwN+OYsrKK92Gh6UN7aYwbN05jxowJ3E5PT1etWrXUtWtXxcWFfy5sUTwej1JTU9WlS5cSbwmG8o/6Whv1tTbqa23U19rMVF/jGENZilyvoDSPVUbPcaw7lfQ58u9TxGMVcR+Px6PPv/hCl1x8sZxH1bbo5wk9Udh1RX1wd6y2FudxCruoeM919DXHf5yToSw/girOlLw8j0dLli5VpwsvLFBfKX8tloSYiDJsVdnzj/g+HtOH9tNOO00Oh0M7d+4MOb5z504lJSUVeh+32y23u+DWWC6Xq9z/spbM006UDvW1NuprbdTX2qivtVFf6/F4nIp0SPGVoqitBXk8HlWOkJITKpm2vsVtt+k3q42IiFDr1q21ePHiwDGfz6fFixerXbt2YWwZAAAAAAAnxvQ97ZI0ZswYDR48WG3atFHbtm01ffp0ZWZmBlaTBwAAAADAjCwR2gcMGKDdu3drwoQJ2rFjh8466ywtXLiwwOJ0AAAAAACYiSVCuySNHDlSI0eODHczAAAAAAAoM6af0w4AAAAAgFUR2gEAAAAAKKcI7QAAAAAAlFOEdgAAAAAAyilCOwAAAAAA5RShHQAAAACAcorQDgAAAABAOUVoBwAAAACgnCK0AwAAAABQThHaAQAAAAAopwjtAAAAAACUU4R2AAAAAADKKUI7AAAAAADlFKEdAAAAAIByitAOAAAAAEA5RWgHAAAAAKCcIrQDAAAAAFBOEdoBAAAAACinCO0AAAAAAJRThHYAAAAAAMopZ7gbUB4YhiFJSk9PD3NLjs3j8SgrK0vp6elyuVzhbg7KGPW1NuprbdTX2qivtVFf66K21maF+vrzpz+PFoXQLungwYOSpFq1aoW5JQAAAACAiuTgwYOqXLlykedtxvFifQXg8/m0fft2xcbGymazhbs5RUpPT1etWrW0bds2xcXFhbs5KGPU19qor7VRX2ujvtZGfa2L2lqbFeprGIYOHjyo5ORk2e1Fz1ynp12S3W5XzZo1w92MYouLizPtGxPHR32tjfpaG/W1NuprbdTXuqittZm9vsfqYfdjIToAAAAAAMopQjsAAAAAAOUUod1E3G63Jk6cKLfbHe6m4CSgvtZGfa2N+lob9bU26mtd1NbaKlJ9WYgOAAAAAIByip52AAAAAADKKUI7AAAAAADlFKEdAAAAAIByitAOAAAAAEA5RWg3iaefflp169ZVZGSkzjvvPH333XfhbhJKYerUqTr33HMVGxur6tWr68orr9SGDRtCrsnOztaIESNUtWpVVapUSX379tXOnTvD1GKciIcfflg2m02jR48OHKO+5vb333/r2muvVdWqVRUVFaUWLVro+++/D5w3DEMTJkxQjRo1FBUVpZSUFG3cuDGMLUZxeb1ejR8/XvXq1VNUVJQaNGig+++/X8Hr9VJf81i6dKl69uyp5ORk2Ww2zZs3L+R8cWq5b98+DRo0SHFxcYqPj9ewYcOUkZFxCl8FinKs+no8Ht19991q0aKFYmJilJycrOuuu07bt28PeQzqW34d7+9vsJtvvlk2m03Tp08POW61+hLaTeDtt9/WmDFjNHHiRK1evVqtWrVSt27dtGvXrnA3DSW0ZMkSjRgxQt9++61SU1Pl8XjUtWtXZWZmBq65/fbb9b///U/vvPOOlixZou3bt6tPnz5hbDVKY+XKlXruuefUsmXLkOPU17z279+v9u3by+Vy6ZNPPtGvv/6qxx57TFWqVAlcM23aND311FOaNWuWVqxYoZiYGHXr1k3Z2dlhbDmK45FHHtGzzz6rmTNnat26dXrkkUc0bdo0zZgxI3AN9TWPzMxMtWrVSk8//XSh54tTy0GDBumXX35Ramqq5s+fr6VLl2r48OGn6iXgGI5V36ysLK1evVrjx4/X6tWr9f7772vDhg264oorQq6jvuXX8f7++n3wwQf69ttvlZycXOCc5eproNxr27atMWLEiMBtr9drJCcnG1OnTg1jq1AWdu3aZUgylixZYhiGYRw4cMBwuVzGO++8E7hm3bp1hiRj+fLl4WomSujgwYNGo0aNjNTUVKNTp07GbbfdZhgG9TW7u+++2+jQoUOR530+n5GUlGT897//DRw7cOCA4Xa7jTfffPNUNBEn4LLLLjOGDh0acqxPnz7GoEGDDMOgvmYmyfjggw8Ct4tTy19//dWQZKxcuTJwzSeffGLYbDbj77//PmVtx/EdXd/CfPfdd4YkY8uWLYZhUF8zKaq+f/31l3H66acbP//8s1GnTh3jiSeeCJyzYn3paS/ncnNztWrVKqWkpASO2e12paSkaPny5WFsGcpCWlqaJCkhIUGStGrVKnk8npB6N2nSRLVr16beJjJixAhddtllIXWUqK/ZffTRR2rTpo2uuuoqVa9eXWeffbZeeOGFwPnNmzdrx44dIfWtXLmyzjvvPOprAhdccIEWL16s3377TZL0448/atmyZerRo4ck6mslxanl8uXLFR8frzZt2gSuSUlJkd1u14oVK055m3Fi0tLSZLPZFB8fL4n6mp3P59O//vUvjR07Vs2aNStw3or1dYa7ATi2PXv2yOv1KjExMeR4YmKi1q9fH6ZWoSz4fD6NHj1a7du3V/PmzSVJO3bsUEREROAfFb/ExETt2LEjDK1ESb311ltavXq1Vq5cWeAc9TW3P/74Q88++6zGjBmj//znP1q5cqVGjRqliIgIDR48OFDDwn5fU9/y75577lF6erqaNGkih8Mhr9erBx98UIMGDZIk6mshxanljh07VL169ZDzTqdTCQkJ1NtksrOzdffdd+vqq69WXFycJOprdo888oicTqdGjRpV6Hkr1pfQDoTJiBEj9PPPP2vZsmXhbgrKyLZt23TbbbcpNTVVkZGR4W4OypjP51ObNm300EMPSZLOPvts/fzzz5o1a5YGDx4c5tbhRM2dO1ezZ8/WnDlz1KxZM61Zs0ajR49WcnIy9QVMyuPxqH///jIMQ88++2y4m4MysGrVKj355JNavXq1bDZbuJtzyjA8vpw77bTT5HA4CqwuvXPnTiUlJYWpVThRI0eO1Pz58/XFF1+oZs2ageNJSUnKzc3VgQMHQq6n3uawatUq7dq1S+ecc46cTqecTqeWLFmip556Sk6nU4mJidTXxGrUqKEzzzwz5FjTpk21detWSQrUkN/X5jR27Fjdc889GjhwoFq0aKF//etfuv322zV16lRJ1NdKilPLpKSkAgv+5uXlad++fdTbJPyBfcuWLUpNTQ30skvU18y++uor7dq1S7Vr1w78X2vLli264447VLduXUnWrC+hvZyLiIhQ69attXjx4sAxn8+nxYsXq127dmFsGUrDMAyNHDlSH3zwgT7//HPVq1cv5Hzr1q3lcrlC6r1hwwZt3bqVeptA586dtXbtWq1Zsybwp02bNho0aFDge+prXu3bty+wReNvv/2mOnXqSNL/t3NnIVH1fxzHP1PWOGOLpmZmSEVSZhTtTctFCaVBG0YUIqM34pJ400J7UUEXUUGQUKRdKAlGi0klmXWRoLa4BJp4YRGotKOp2cX8nov4D89U/7KnZUZ9v+DAzPkdz3zPfBk8nzlzfpo0aZLGjRvn0d+Ojg5VVVXR336gu7tbQ4Z4nhYNHTpULpdLEv0dSPrSS4fDoffv3+vRo0fubcrLy+VyubRw4cK/XjN+zv8Ce3Nzs8rKyhQcHOwxTn/7r6SkJNXX13uca40fP17bt29XaWmppAHaX2/PhIcfKywsNFar1Vy4cME0NDSY1NRUExgYaNrb271dGn5Senq6GT16tLl3755pa2tzL93d3e5t0tLSTGRkpCkvLzcPHz40DofDOBwOL1aNX/Hv2eONob/9WXV1tfHz8zNHjx41zc3NpqCgwNjtdpOfn+/e5tixYyYwMNBcu3bN1NfXm3Xr1plJkyaZnp4eL1aOvnA6nSYiIsKUlJSYlpYWc/nyZRMSEmJ27Njh3ob+9h+dnZ2mpqbG1NTUGEnmxIkTpqamxj17eF96GRcXZ2bPnm2qqqrM/fv3TVRUlNmyZYu3Dgn/8r3+fvr0yaxdu9ZMmDDB1NbWepxv9fb2uvdBf33Xjz6/X/py9nhjBl5/Ce39xOnTp01kZKQZPny4WbBggamsrPR2SfgPJH1zycvLc2/T09NjMjIyTFBQkLHb7WbDhg2mra3Ne0Xjl3wZ2ulv/3b9+nUzY8YMY7VazbRp08zZs2c9xl0ul9m3b58JCwszVqvVxMbGmqamJi9Vi5/R0dFhsrOzTWRkpPH39zeTJ082e/bs8TjJp7/9x927d7/5/9bpdBpj+tbLN2/emC1btpgRI0aYUaNGmZSUFNPZ2emFo8GXvtfflpaW/3u+dffuXfc+6K/v+tHn90vfCu0Drb8WY4z5G1f0AQAAAADAz+GedgAAAAAAfBShHQAAAAAAH0VoBwAAAADARxHaAQAAAADwUYR2AAAAAAB8FKEdAAAAAAAfRWgHAAAAAMBHEdoBAAAAAPBRhHYAAPDHWSwWXb161dtlAADQ7xDaAQAY4JKTk2WxWL5a4uLivF0aAAD4AT9vFwAAAP68uLg45eXleayzWq1eqgYAAPQVV9oBABgErFarxo0b57EEBQVJ+vzT9ZycHMXHx8tms2ny5Mm6dOmSx98/efJEK1askM1mU3BwsFJTU/XhwwePbXJzcxUTEyOr1arw8HBt3brVY/z169fasGGD7Ha7oqKiVFxc7B579+6dEhMTFRoaKpvNpqioqK++ZAAAYDAitAMAAO3bt08JCQmqq6tTYmKiNm/erMbGRklSV1eXVq1apaCgID148EBFRUUqKyvzCOU5OTnKzMxUamqqnjx5ouLiYk2ZMsXjNQ4dOqRNmzapvr5eq1evVmJiot6+fet+/YaGBt28eVONjY3KyclRSEjI33sDAADwURZjjPF2EQAA4M9JTk5Wfn6+/P39Pdbv3r1bu3fvlsViUVpamnJyctxjixYt0pw5c3TmzBmdO3dOO3fu1IsXLxQQECBJunHjhtasWaPW1laFhYUpIiJCKSkpOnLkyDdrsFgs2rt3rw4fPizp8xcBI0aM0M2bNxUXF6e1a9cqJCREubm5f+hdAACgf+KedgAABoHly5d7hHJJGjNmjPuxw+HwGHM4HKqtrZUkNTY2atasWe7ALklLliyRy+VSU1OTLBaLWltbFRsb+90aZs6c6X4cEBCgUaNG6eXLl5Kk9PR0JSQk6PHjx1q5cqXWr1+vxYsX/6djBQBgICG0AwAwCAQEBHz1c/XfxWaz9Wm7YcOGeTy3WCxyuVySpPj4eD1//lw3btzQ7du3FRsbq8zMTB0/fvy31wsAQH/CPe0AAECVlZVfPY+OjpYkRUdHq66uTl1dXe7xiooKDRkyRFOnTtXIkSM1ceJE3blz55dqCA0NldPpVH5+vk6dOqWzZ8/+0v4AABgIuNIOAMAg0Nvbq/b2do91fn5+7sneioqKNG/ePC1dulQFBQWqrq7W+fPnJUmJiYk6cOCAnE6nDh48qFevXikrK0tJSUkKCwuTJB08eFBpaWkaO3as4uPj1dnZqYqKCmVlZfWpvv3792vu3LmKiYlRb2+vSkpK3F8aAAAwmBHaAQAYBG7duqXw8HCPdVOnTtXTp08lfZ7ZvbCwUBkZGQoPD9fFixc1ffp0SZLdbldpaamys7M1f/582e12JSQk6MSJE+59OZ1Offz4USdPntS2bdsUEhKijRs39rm+4cOHa9euXXr27JlsNpuWLVumwsLC33DkAAD0b8weDwDAIGexWHTlyhWtX7/e26UAAIAvcE87AAAAAAA+itAOAAAAAICP4p52AAAGOe6UAwDAd3GlHQAAAAAAH0VoBwAAAADARxHaAQAAAADwUYR2AAAAAAB8FKEdAAAAAAAfRWgHAAAAAMBHEdoBAAAAAPBRhHYAAAAAAHzUP1wVUdxS31rnAAAAAElFTkSuQmCC","text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Plotting convergence curves\n","for opt in results:\n","    plt.figure(figsize=(12, 8))\n","    for epochs in results[opt]:\n","        history = results[opt][epochs]['history']\n","        plt.plot(history['loss'], label=f'Train Loss - {epochs} epochs', linewidth=2)\n","        #plt.plot(history['val_loss'], label=f'Val Loss - {epochs} epochs', linestyle='--', linewidth=2)\n","    plt.title(f'Convergence Curves for Optimizer: {opt}')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can see that no matter the number of epochs and the initialized weight, we see that the convergence is not impacted."]},{"cell_type":"markdown","metadata":{},"source":["# Submission "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Construire le modèle final avec les meilleurs hyperparamètres\n","final_hypermodel = MyHyperModel()\n","final_model = final_hypermodel.build(best_hps)\n","\n","# Entraîner le modèle final sur l'ensemble des données d'entraînement\n","final_model.fit(X_train_scaled, y_train_log, epochs=150, batch_size=32, verbose=1)\n","\n","# Faire des prédictions sur les données de test\n","final_predictions_log = final_model.predict(X_test_scaled)\n","\n","# Convertir les prédictions en échelle originale\n","final_predictions = np.exp(final_predictions_log.flatten())\n","\n","# Créer le fichier de soumission\n","submission = pd.DataFrame({\n","    'Id': df_test_ids,\n","    'SalePrice': final_predictions\n","})\n","\n","# Sauvegarder le fichier de soumission\n","submission.to_csv('submission.csv', index=False)\n","print('Submission file ready!')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":868283,"sourceId":5407,"sourceType":"competition"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
